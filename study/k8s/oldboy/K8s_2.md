* ==每天的前情回顾都值得看几遍==
* ==[课件]([https://blog.stanley.wang/2019/01/18/%E5%AE%9E%E9%AA%8C%E6%96%87%E6%A1%A31%EF%BC%9A%E8%B7%9F%E6%88%91%E4%B8%80%E6%AD%A5%E6%AD%A5%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2kubernetes%E9%9B%86%E7%BE%A4/](https://blog.stanley.wang/2019/01/18/实验文档1：跟我一步步安装部署kubernetes集群/))==
* nginx-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```



==注意证书过期==

* 使用二进制安装部署K8S的要点
  * 基础设施环境准备好
    * CentOS7.6系统（内核在3.8x以上）
    * 关闭SELinux，关闭firewalld服务
    * 时间同步（chronyd）
    * 调整Base源，Epel源
    * 内核优化（文件描述符大小，内核转发，等等）
  * 安装部署bind9内网DNS系统
  * 安装部署docker的私有仓库-harbor
  * 准备证书签发环境——cfssl
  * 安装部署主控节点服务（4个）
    * Etcd
    * Apiserver
    * Controller-manager
    * Scheduler
  * 安装部署运算节点服务（2个）
    * Kubelet
    * Kube-proxy

* Apiserver、Controller-manager、Scheduler三个安装到一台
* Etcd可以单独安装，奇数台
* 关于cfssl工具
  * cfssl：证书签发的主要工具
  * cfssl-jsc：将cfssl生成的证书（json格式）变为文件承载式证书
  * cfssl-certinfo：验证证书的信息
* 关于kubeconfig文件：
  * 这是一个K8S用户的配置文件
  * 它里面含有证书信息
  * ==证书过期或更换，需要同步替换该文件==

```shell
[root@hdss7-22 conf]# cd /opt/kubernetes/server/bin/conf
[root@hdss7-22 conf]# md5sum kubelet.kubeconfig
b3596d403858c064af82f66025537a29  kubelet.kubeconfig
# cat kubelet.kubeconfig反解得到证书内容
"xxx=="|base64 -d
# 反解得到证书
"xxx=="|base64 -d > 123.pem
cfssl-certinfo -cert 123.pem
```

![image-20200812202215561](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200812202215561.png)

# Kubectl命令行工具使用详解

* 管理K8S核心资源的三种基本方法
  * 陈述式管理方法—主要依赖命令行CLI工具进行管理
  * 声明式管理方法—主要依赖统一资源配置清单（manifest）进行管理
  * GUI式管理方法—主要依赖图形化操作界面（web页面）进行管理
* 陈述式资源管理方法：
  * 查看名称空间：kubectl get namespaces
  * 查看名称空间内的资源：kubectl get all -n default
  * 创建名称空间：kubect create namespace app
  * 删除名称空间：kubectl delete namespace app
  * 创建deployment：kubectl create deployment nginx-dp -- image=harbor.od.com/public/nginx:v1.7.9 -n kube-public
  * 查看deployment
    * 简单查看
    * 扩展查看
    * 详细查看

```shell
[root@hdss7-21 ~]# kubectl get namespaces
[root@hdss7-21 ~]# kubectl get ns
[root@hdss7-21 ~]# kubectl get all -n default
[root@hdss7-21 ~]# kubectl create namespace app
namespace/app created
[root@hdss7-21 ~]# kubectl delete namespace app
namespace "app" deleted
```

```shell
# 管理Deployment
[root@hdss7-21 ~]# kubectl create deployment nginx-dp --image=harbor.od.com/public/nginx:v1.7.9 -n kube-public
deployment.apps/nginx-dp created
[root@hdss7-21 ~]# kubectl get all -n kube-public
[root@hdss7-21 ~]# kubectl get deployment -n kube-public
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
nginx-dp   1/1     1            1           21s
[root@hdss7-21 ~]# kubectl get pods -n kube-public
NAME                        READY   STATUS    RESTARTS   AGE
nginx-dp-5dfc689474-4525f   1/1     Running   0          42s
[root@hdss7-21 ~]# kubectl get pods -n kube-public -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES
nginx-dp-5dfc689474-4525f   1/1     Running   0          65s   172.7.21.2   hdss7-21.host.com   <none>           <none>
[root@hdss7-21 ~]# kubectl describe deployment nginx-dp -n kube-public
[root@hdss7-21 ~]# kubectl get pods -n kube-public
NAME                        READY   STATUS    RESTARTS   AGE
nginx-dp-5dfc689474-4525f   1/1     Running   0          142m
[root@hdss7-21 ~]# kubectl exec -it nginx-dp-5dfc689474-4525f /bin/bash -n kube-public
root@nginx-dp-5dfc689474-4525f:/# ip add

[root@hdss7-21 ~]# watch -n 1 'kubectl describe deployment nginx-dp -n kube-public|grep -C 5 Event'
[root@hdss7-22 ~]# kubectl delete pod nginx-dp-5dfc689474-zvfhr -n kube-public
# 强制删除
[root@hdss7-22 ~]# kubectl delete pod nginx-dp-3sf3s11121-fswe -n kube-public --force --grace-period=0
[root@hdss7-22 ~]# kubectl delete deploy nginx-dp -n kube-public
```

```shell
# 管理Service资源
[root@hdss7-21 ~]# kubectl expose deployment nginx-dp --port=80 -n kube-public
service/nginx-dp exposed
[root@hdss7-21 ~]#  kubectl get all -n kube-public
NAME                            READY   STATUS    RESTARTS   AGE
pod/nginx-dp-5dfc689474-kl77v   1/1     Running   0          11m
NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
service/nginx-dp   ClusterIP   10.254.124.177   <none>        80/TCP    79s
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-dp   1/1     1            1           160m
NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/nginx-dp-5dfc689474   1         1         1       160m
[root@hdss7-21 ~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 nq
  -> 192.168.0.21:6443            Masq    1      0          0
  -> 192.168.0.22:6443            Masq    1      0          0
TCP  10.254.124.177:80 nq         # service
  -> 172.7.21.2:80                Masq    1      0          1
[root@hdss7-21 ~]# curl 10.254.124.177
[root@hdss7-21 ~]# kubectl scale deployment nginx-dp --replicas=2 -n kube-public
deployment.extensions/nginx-dp scaled
[root@hdss7-21 ~]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 nq
  -> 192.168.0.21:6443            Masq    1      0          0
  -> 192.168.0.22:6443            Masq    1      0          0
TCP  10.254.124.177:80 nq      # service
  -> 172.7.21.2:80                Masq    1      0          0
  -> 172.7.22.2:80                Masq    1      0          0
[root@hdss7-21 ~]# kubectl describe svc nginx-dp -n kube-public
Name:              nginx-dp
Namespace:         kube-public
Labels:            app=nginx-dp
Annotations:       <none>
Selector:          app=nginx-dp
Type:              ClusterIP
IP:                10.254.124.177
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         172.7.21.2:80,172.7.22.2:80
Session Affinity:  None
Events:            <none>
```

* 陈述式资源管理方法小结
  * Kubernetes集群管理集群资源的唯一入口是通过相应的方法调用apiserver的接口
  * kubectl是官方的CLI命令行工具，用于与apiserver进行通信，将用户在命令行输入的命令，组织并转化为apiserver能识别的信息，进而实现管理K8S各种资源的一种有效途径
  * kubectl的命令大全
    * kubectl --help
    * http://docs.kubernetes.org.cn
  * 陈述式资源管理方法可以满足90%以上的资源管理需求，但它的缺点也很明显
    * 命令冗长、复杂、难以记忆
    * 特定场景下，无法实现管理需求
    * 对资源的增、删、查操作比较容易，改就比较麻烦
* 声明式资源管理方法：
  * 声明式资源管理方法依赖于——资源配置清单（yaml/json）
  * 查看资源配置清单的方法：kubectl get svc nginx-dp -o yaml -n kube-public
  * 解释资源配置清单：kubectl explain service
  * 创建资源配置清单：vi /root/nginx-ds-svc.yaml
  * 应用资源配置清单：kubectl apply -f nginx-ds-svc.yaml
  * 修改资源配置清单并应用
    * 在线修改
    * 离线修改
  * 删除资源配置清单
    * 陈述式删除
    * 声明式删除

```shell
[root@hdss7-21 ~]# kubectl get pods nginx-dp-5dfc689474-rhp84 -o yaml -n kube-public
[root@hdss7-21 ~]# kubectl get svc nginx-dp -o yaml -n kube-public
[root@hdss7-21 ~]# kubectl explain service.metadata
```

![image-20200812233816314](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200812233816314.png)

```yaml
apiVersion: v1
kind: Service
metadata:
	labels:
		app: nginx-ds
	name: nginx-ds
  namespace: default
spec:
	ports:
	- port: 80
		protocol: TCP
		targetPort: 80
	selector:
  	app: nginx-ds
	type: ClusterIP  	
```



```shell
kubectl create -f nginx-ds-svc.yaml
kubectl get svc -n default
kubectl get svc nginx-ds -o yaml
kubectl get svc nginx-ds -o json
# 在线修改（没有记录）
kubectl edit svc nginx-ds
kubectl delete -f nginx-ds-svc.yaml
kubectl get svc nginx-dp -o yaml -n kube-public > nginx-dp-svc.yaml
```

![image-20200812234506905](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200812234506905.png)

```yaml
apiVersion: v1
kind: Service
metadata:
	labels:
		app: nginx-dp
	name: nginx-dp
	namespace: kube-public
spec:
	ports:
	- port: 81
		protocol: TCP
		targetPort: 80
	selector:
  	app: nginx-dp
```

* 声明式资源管理方法小结：
  * 声明式资源管理方法，依赖于统一资源配置清单文件对资源进行管理
  * 对资源的管理，是通过事先定义在统一资源配置清单内，再通过陈述式命令应用到K8S集群里
  * 语法格式：kubectl create /apply/delete -f /path/to/yaml
  * 资源配置清单的学习方法
    * tip1：多看别人（官方）写的，能读懂
    * tip2：能照着现成的文件改着用
    * tip3：遇到不懂的，善用kubectl explain...查
    * tip4：初学切忌上来就无中生有，自己憋着写

# Kubernetes的CNI网络插件--flannel

* Kubernetes设计了网络模型，但却将它的实现交给了网络插件，CNI网络插件最主要的功能就是实现POD资源能够跨宿主机进行通信
* 常见的CNI网络插件
  * Flannel
  * Calico
  * Canal
  * Contiv
  * OpenContrail
  * NSX-T
  * Kube-router

```shell
[root@hdss7-21 ~]# kubectl get pods -n kube-public -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES
nginx-dp-5dfc689474-kl77v   1/1     Running   1          9h    172.7.21.2   hdss7-21.host.com   <none>           <none>
nginx-dp-5dfc689474-rhp84   1/1     Running   1          8h    172.7.22.2   hdss7-22.host.com   <none>           <none>
[root@hdss7-21 ~]# ping 172.7.21.2
[root@hdss7-21 ~]# ping 172.7.22.2     # ping 不通
```

* HDSS7-21.host.com、HDSS7-21.host.com

  ```shell
  [root@hdss7-21 ~]# cd /opt/
  [root@hdss7-21 opt]# mkdir flannel-v0.11.0
  [root@hdss7-21 opt]# cd flannel-v0.11.0/
  [root@hdss7-21 flannel-v0.11.0]# wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz
  [root@hdss7-21 flannel-v0.11.0]# tar -zxvf /data/flannel-v0.11.0-linux-amd64.tar.gz -C /opt/flannel-v0.11.0/
  [root@hdss7-21 opt]# ln -s /opt/flannel-v0.11.0/ /opt/flannel
  [root@hdss7-21 opt]# cd flannel
  [root@hdss7-21 flannel]# mkdir cert
  [root@hdss7-21 flannel]# cd cert/
  [root@hdss7-21 cert]# scp hdss7-200:/opt/certs/ca.pem .
  [root@hdss7-21 cert]# scp hdss7-200:/opt/certs/client.pem .
  [root@hdss7-21 cert]# scp hdss7-200:/opt/certs/client-key.pem .
  # 配置文件
  [root@hdss7-21 cert]# cd ..
  [root@hdss7-21 flannel]# vi subnet.env
  FLANNEL_NETWORK=172.7.0.0/16
  FLANNEL_SUBNET=172.7.21.1/24    # 22
  FLANNEL_MTU=1500
  FLANNEL_IPMASQ=false
  # 启动脚本
  [root@hdss7-21 flannel]# vi flanneld.sh
  #!/bin/sh
  ./flanneld \
  	--public-ip=192.168.0.21 \                  # 
	  --etcd-endpoints=https://192.168.0.12:2379,https://192.168.0.21:2379,https://192.168.0.22:2379 \
  	--etcd-keyfile=./cert/client-key.pem \
	  --etcd-certfile=./cert/client.pem \
  	--etcd-cafile=./cert/ca.pem \
  	--iface=enp0s3 \               #  ip a 查看
  	--subnet-file=./subnet.env \
  	--healthz-port=2401
  ```
  

![image-20200813082611856](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813082611856.png)

```shell
[root@hdss7-21 flannel]# chmod +x /opt/flannel/flanneld.sh
[root@hdss7-21 flannel]# mkdir -p /data/logs/flanneld
[root@hdss7-21 flannel]# cd /opt/etcd
[root@hdss7-21 etcd]# ./etcdctl member list
b8fffb7f5b2f26e: name=etcd-server-7-12 peerURLs=https://192.168.0.12:2380 clientURLs=http://127.0.0.1:2379,https://192.168.0.12:2379 isLeader=true
b3e330dca330e585: name=etcd-server-7-22 peerURLs=https://192.168.0.22:2380 clientURLs=http://127.0.0.1:2379,https://192.168.0.12:2379 isLeader=false
e4c0813c987fdb4c: name=etcd-server-7-21 peerURLs=https://192.168.0.21:2380 clientURLs=http://127.0.0.1:2379,https://192.168.0.21:2379 isLeader=false
# 22上不用操作etcd
[root@hdss7-21 etcd]# ./etcdctl set /coreos.com/network/config '{"Network":"172.7.0.0/16","Backend": {"Type": "host-gw"}}'
  {"Network":"172.7.0.0/16","Backend": {"Type": "host-gw"}}
  [root@hdss7-21 etcd]# ./etcdctl get /coreos.com/network/config
  {"Network":"172.7.0.0/16","Backend": {"Type": "host-gw"}}
  
# ./etcdctl set /coreos.com/network/config '{"Network": "172.7.0.0/16","Backend": {"Type": "host-gw"}}'  
  
  [root@hdss7-21 etcd]# vi /etc/supervisord.d/flannel.ini
  
  [program:flanneld-7-21]
  command=sh /opt/flannel/flanneld.sh                        ; the program (relative uses PATH, can take args)
  numprocs=1                                                 ; number of processes copies to start (def 1)
  directory=/opt/flannel                                     ; directory to cwd to before exec (def no cwd)
  autostart=true                                             ; start at supervisord start (default: true)
  autorestart=true                                           ; retstart at unexpected quit (default: true)
  startsecs=30                                               ; number of secs prog must stay running (def. 1)
  startretries=3                                             ; max # of serial start failures (default 3)
  exitcodes=0,2                                              ; 'expected' exit codes for process (default 0,2)
  stopsignal=QUIT                                            ; signal used to kill process (default TERM)
  stopwaitsecs=10                                            ; max num secs to wait b4 SIGKILL (default 10)
  user=root                                                  ; setuid to this UNIX account to run the program
  redirect_stderr=true                                       ; redirect proc stderr to stdout (default false)
  stdout_logfile=/data/logs/flanneld/flanneld.stdout.log      ; stdout log path, NONE for none; default AUTO
  stdout_logfile_maxbytes=64MB                               ; max # logfile bytes b4 rotation (default 50MB)
  stdout_logfile_backups=4                                   ; # of stdout logfile backups (default 10)
  stdout_capture_maxbytes=1MB                                ; number of bytes in 'capturemode' (default 0)
  stdout_events_enabled=false                                ; emit events on stdout writes (default false)
  
  [root@hdss7-21 supervisord.d]# supervisorctl update
  flanneld-7-: added process group
  [root@hdss7-21 supervisord.d]# supervisorctl status
  etcd-server-7-21                 RUNNING   pid 1351, uptime 1:21:23
  flanneld-7-21                      RUNNING   pid 24675, uptime 0:00:44
  kube-apiserver-7-21              RUNNING   pid 1352, uptime 1:21:23
  kube-controller-manager-7-21     RUNNING   pid 1354, uptime 1:21:23
  kube-kubelet-7-21                RUNNING   pid 1350, uptime 1:21:23
  kube-proxy-7-21                  RUNNING   pid 1355, uptime 1:21:23
  kube-scheduler-7-21              RUNNING   pid 1362g, uptime 1:21:23
  
  [root@hdss7-21 supervisord.d]# tail -fn 200 /data/logs/flanneld/flanneld.stdout.log  
```

![image-20200813203647819](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813203647819.png)

![image-20200813224204819](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813224204819.png)

```shell
# 测试互ping，能够ping通
[root@hdss7-21 ~]# kubectl get pods -n kube-public -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES
nginx-dp-5dfc689474-kl77v   1/1     Running   1          9h    172.7.21.2   hdss7-21.host.com   <none>           <none>
nginx-dp-5dfc689474-rhp84   1/1     Running   1          8h    172.7.22.2   hdss7-22.host.com   <none>           <none>
[root@hdss7-21 ~]# ping 172.7.21.2
[root@hdss7-21 ~]# ping 172.7.22.2 
```

![image-20200813215202752](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813215202752.png)

![image-20200813220545952](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813220545952.png)



* host-gw适用于二层网络相同==【推荐】==

![image-20200813223910351](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200813223910351.png)

* VxLAN二层网络不同

```shell
【不用操作】
# VxLAN 模型
supervisorctl stop flanneld-7-22
ps aux|grep flannel
kill -9 1091
[root@hdss7-21 ~]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    100    0        0 enp0s3
172.7.21.0      0.0.0.0         255.255.255.0   U     0      0        0 docker0
172.7.85.0      192.168.0.22    255.255.255.0   UG    0      0        0 enp0s3
192.168.0.0     0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
[root@hdss7-21 ~]# route del -net 172.7.85.0/24 gw 192.168.0.22
[root@hdss7-22 flannel]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    100    0        0 enp0s3
172.7.22.0      0.0.0.0         255.255.255.0   U     0      0        0 docker0
172.7.74.0      192.168.0.21    255.255.255.0   UG    0      0        0 enp0s3
192.168.0.0     0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
[root@hdss7-22 flannel]# route del -net 172.7.74.0/24 gw 192.168.0.21
```

```shell
[root@hdss7-21 ~]# cd /opt/etcd
[root@hdss7-21 etcd]# ./etcdctl get /coreos.com/network/config
{"Network":"172.7.0.0/16","Backend": {"Type": "host-gw"}}
[root@hdss7-21 etcd]# ./etcdctl rm /coreos.com/network/config
PrevNode.Value: {"Network":"172.7.0.0/16","Backend": {"Type": "host-gw"}}
[root@hdss7-21 etcd]# ./etcdctl get /coreos.com/network/config
Error:  100: Key not found (/coreos.com/network/config) [39]
[root@hdss7-21 etcd]# ./etcdctl set /coreos.com/network/config '{"Network":"172.7.0.0/16","Backend": {"Type": "VxLAN"}}'
{"Network":"172.7.0.0/16","Backend": {"Type": "VxLAN"}}
[root@hdss7-21 etcd]# ./etcdctl get /coreos.com/network/config
{"Network":"172.7.0.0/16","Backend": {"Type": "VxLAN"}}
[root@hdss7-21 etcd]# supervisorctl start flanneld-7-21
[root@hdss7-22 flannel]# ip a
....
10: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether 9e:eb:d6:a3:22:21 brd ff:ff:ff:ff:ff:ff
    inet 172.7.74.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::9ceb:d6ff:fea3:2221/64 scope link
       valid_lft forever preferred_lft forever
[root@hdss7-21 etcd]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.0.1     0.0.0.0         UG    100    0        0 enp0s3
172.7.21.0      0.0.0.0         255.255.255.0   U     0      0        0 docker0
172.7.85.0      172.7.85.0      255.255.255.0   UG    0      0        0 flannel.1
192.168.0.0     0.0.0.0         255.255.255.0   U     100    0        0 enp0s3       
```

```shell
# 直接路由模型
./etcdctl set /coreos.com/network/config '{"Network":"172.7.0.0/16","Backend": {"Type": "VxLAN","Directrouting":true}}'
```

## Flannel之SNAT规则优化

```shell
# 制作nginx:curl镜像
# docker exec -it Nginx_ContainerID /bin/sh
[root@hdss7-200 certs]# docker run -it 84581e99d807 /bin/sh

tee /etc/apt/sources.list << EOF
deb http://mirrors.163.com/debian/ jessie main non-free contrib
deb http://mirrors.163.com/debian/ jessie-updates main non-free contrib
EOF

apt-get update && apt-get install curl -y
curl -k https://www.baidu.com

[root@hdss7-200 certs]# docker start 2630b755d524
# docker commit -p nginx_with_baidu harbor.od.com/public/nginx:curl
[root@hdss7-200 certs]# docker commit -p nginx_curl harbor.od.com/public/nginx:curl
docker push harbor.od.com/public/nginx:curl
[root@hdss7-200 certs]# docker push harbor.od.com/public/nginx:curl
```

```yaml
# 创建pod
# /opt/yaml/nginx-ds-curl.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-ds-curl
  labels:
    app: nginx-ds-curl
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-ds-curl
  template:
    metadata:
      labels:
        app: nginx-ds-curl
    spec:
      containers:
      - name: nginx-curl
        image: harbor.od.com/public/nginx:curl
        ports:
        - containerPort: 80
        
kubectl apply -f nginx-ds-curl.yaml
# 
```

```shell
[root@hdss7-21 yaml]# kubectl get pods -o wide
NAME                                        READY   STATUS             RESTARTS   AGE     IP           NODE                NOMINATED NODE   READINESS GATES
nginx-deployment-5754944d6c-mf6md           1/1     Running            0          54s     172.7.22.2   hdss7-22.host.com   <none>           <none>
nginx-deployment-5754944d6c-rbhch           1/1     Running            0          3m7s    172.7.21.2   hdss7-21.host.com   <none>           <none>
[root@hdss7-21 yaml]# kubectl exec -it nginx-deployment-5754944d6c-mf6md /bin/sh
# ping 172.7.21.2
# ping 172.7.22.2
```

![image-20200815215658570](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815215658570.png)

* 做了SNAT转换

  ![image-20200815221259387](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815221259387.png)

![image-20200815220057964](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815220057964.png)

![image-20200815220203338](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815220203338.png)

```shell
# 7-21、7-22
[root@hdss7-21 ~]# iptables-save|grep -i postrouting
[root@hdss7-21 ~]# yum install iptables-services -y
[root@hdss7-21 yaml]# systemctl start iptables
[root@hdss7-21 yaml]# systemctl enable iptables
[root@hdss7-21 ~]# iptables-save|grep -i postrouting
# 注意ip
[root@hdss7-21 yaml]# iptables -t nat -D POSTROUTING -s 172.7.21.0/24 ! -o docker0 -j MASQUERADE
# 来源地址是172.7.21.0/24 目标地址不是172.7.0.0/16，不是从docker0通往的，才做SNAT转换
[root@hdss7-21 yaml]# iptables -t nat -I POSTROUTING -s 172.7.21.0/24 ! -d 172.7.0.0/16 ! -o docker0 -j MASQUERADE
[root@hdss7-21 yaml]# iptables-save > /etc/sysconfig/iptables
[root@hdss7-21 yaml]# iptables-save | grep -i reject
[root@hdss7-21 yaml]# iptables -t filter -D INPUT -j REJECT --reject-with icmp-host-prohibited
[root@hdss7-21 yaml]# iptables -t filter -D FORWARD -j REJECT --reject-with icmp-host-prohibited
[root@hdss7-21 yaml]# ping 172.7.22.2
[root@hdss7-21 yaml]# iptables-save > /etc/sysconfig/iptables
```

![image-20200815221137123](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815221137123.png)

![image-20200815221442952](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815221442952.png)

* k8s集群内部，容器之间应该是容器之间的IP，而不是node的IP

  ![image-20200815222712209](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815222712209.png)

  ![image-20200815222554332](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815222554332.png)

# Kubernetes的服务发现插件coredns

![image-20200815223533548](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815223533548.png)

![image-20200815224020849](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815224020849.png)

## 部署k8s资源配置清单的内网http服务

* ·在运维主机`HDSS7-200.host.com`上，配置一个nginx虚拟主机，用以提供k8s统一的资源配置清单访问入口

  ```shell
  [root@hdss7-200 ~]# vim /etc/nginx/conf.d/k8s-yaml.od.com.conf
  server {
      listen       80;
      server_name  k8s-yaml.od.com;
  
      location / {
          autoindex on;
          default_type text/plain;
          root /data/k8s-yaml;
      }
  }
  
  # 7-11上
  [root@hdss7-11 keepalived]# vi /var/named/od.com.zone
  $ORIGIN od.com.
  $TTL 600    ; 10 minutes
  @           IN SOA    dns.od.com. dnsadmin.od.com. (
                  2020081403 ; serial                       # 序号加1
                  10800      ; refresh (3 hours)
                  900        ; retry (15 minutes)
                  604800     ; expire (1 week)
                  86400      ; minimum (1 day)
                  )
                  NS   dns.od.com.
  $TTL 60    ; 1 minute
  dns                A    192.168.0.11
  harbor             A    192.168.0.200
  k8s-yaml           A    192.168.0.200               # 新增
  [root@hdss7-11 keepalived]# systemctl restart named
  [root@hdss7-11 keepalived]# named-checkconf
  [root@hdss7-11 keepalived]# echo $?
  0
  [root@hdss7-11 keepalived]# dig -t A k8s-yaml.od.com @192.168.0.11 +short
  192.168.0.200
   
  [root@hdss7-200 ~]# mkdir /data/k8s-yaml
  [root@hdss7-200 ~]# nginx -t
  [root@hdss7-200 k8s-yaml]# nginx -s reload
  [root@hdss7-200 ~]# cd /data/k8s-yaml/
  [root@hdss7-200 k8s-yaml]# mkdir coredns
  # 浏览器访问 http://k8s-yaml.od.com/
  ```

  ```shell
  # 部署coredns
  [root@hdss7-200 k8s-yaml]# cd coredns/
  [root@hdss7-200 coredns]# docker pull docker.io/coredns/coredns:1.6.1
  [root@hdss7-200 coredns]# docker images | grep coredns
  coredns/coredns                 1.6.1               c0f6e815079e        12 months ago       42.2MB
  [root@hdss7-200 coredns]# docker tag c0f6e815079e harbor.od.com/public/coredns:v1.6.1
  [root@hdss7-200 coredns]# docker push harbor.od.com/public/coredns:v1.6.1
  ## 资源配置清单
  ```

  ```yaml
  # vi /data/k8s-yaml/coredns/rbac.yaml
  
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: coredns
    namespace: kube-system
    labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
      addonmanager.kubernetes.io/mode: Reconcile
    name: system:coredns
  rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    annotations:
      rbac.authorization.kubernetes.io/autoupdate: "true"
    labels:
      kubernetes.io/bootstrapping: rbac-defaults
      addonmanager.kubernetes.io/mode: EnsureExists
    name: system:coredns
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:coredns
  subjects:
  - kind: ServiceAccount
    name: coredns
    namespace: kube-system
    
  # vi /data/k8s-yaml/coredns/configmap.yaml
  
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: coredns
    namespace: kube-system
  data:
    Corefile: |
      .:53 {
          errors
          log
          health
          ready
          kubernetes cluster.local 10.254.0.0/16   
          forward . 192.168.0.11
          cache 30
          loop
          reload
          loadbalance
         }
  
  # vi /data/k8s-yaml/coredns/deployment.yaml
  
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: coredns
    namespace: kube-system
    labels:
      k8s-app: coredns
      kubernetes.io/name: "CoreDNS"
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: coredns
    template:
      metadata:
        labels:
          k8s-app: coredns
      spec:
        priorityClassName: system-cluster-critical
        serviceAccountName: coredns
        containers:
        - name: coredns
          image: harbor.od.com/public/coredns:v1.6.1
          args:
          - -conf
          - /etc/coredns/Corefile
          volumeMounts:
          - name: config-volume
            mountPath: /etc/coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
  	        name: metrics
            protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
        dnsPolicy: Default
        volumes:
          - name: config-volume
            configMap:
              name: coredns
              items:
              - key: Corefile
                path: Corefile
                
  # vi /data/k8s-yaml/coredns/svc.yaml
  
  apiVersion: v1
  kind: Service
  metadata:
    name: coredns
    namespace: kube-system
    labels:
      k8s-app: coredns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: "CoreDNS"
  spec:
    selector:
      k8s-app: coredns
    clusterIP:  10.254.0.2                  # 和kubelet.sh中保持一致
    ports:
    - name: dns
      port: 53
      protocol: UDP
    - name: dns-tcp
      port: 53
    - name: metrics
      port: 9153
      protocol: TCP
  ```

  ![image-20200815233108282](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815233108282.png)

  ```shell
  # 7-21上
  [root@hdss7-21 yaml]# kubectl apply -f http://k8s-yaml.od.com/coredns/rbac.yaml
  serviceaccount/coredns created
  clusterrole.rbac.authorization.k8s.io/system:coredns created
  clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
  [root@hdss7-21 yaml]# kubectl apply -f http://k8s-yaml.od.com/coredns/configmap.yaml
  [root@hdss7-21 yaml]# kubectl apply -f http://k8s-yaml.od.com/coredns/deployment.yaml
  [root@hdss7-21 yaml]# kubectl apply -f http://k8s-yaml.od.com/coredns/svc.yaml
  [root@hdss7-21 ~]# kubectl get all -n kube-system -o wide
  NAME                           READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES
  pod/coredns-6b6c4f9648-ns59q   1/1     Running   0          24s   172.7.21.3   hdss7-21.host.com   <none>           <none>
  
  NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
  service/coredns   ClusterIP   10.254.0.2   <none>        53/UDP,53/TCP,9153/TCP   17s   k8s-app=coredns
  
  NAME                      READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                SELECTOR
  deployment.apps/coredns   1/1     1            1           24s   coredns      harbor.od.com/public/coredns:v1.6.1   k8s-app=coredns
  
  NAME                                 DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES                                SELECTOR
  replicaset.apps/coredns-6b6c4f9648   1         1         1       24s   coredns      harbor.od.com/public/coredns:v1.6.1   k8s-app=coredns,pod-template-hash=6b6c4f9648
  ```

  ![image-20200815235632074](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200815235632074.png)



![image-20200816000437181](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816000437181.png)

![image-20200816000540468](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816000540468.png)

* kubelet.sh中的cluster-dns要和svc.yaml中保持一致
  * 自己的则都是 10.254.0.2



![image-20200816001521675](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816001521675.png)

```shell
# 7-21
[root@hdss7-21 ~]# yum install bind-utils -y
[root@hdss7-21 ~]# dig -t A www.baidu.com @192.168.0.11 +short
www.a.shifen.com.
14.215.177.39
14.215.177.38
[root@hdss7-21 ~]# dig -t A www.baidu.com @10.254.0.2 +short
www.a.shifen.com.
14.215.177.39
14.215.177.38
[root@hdss7-21 ~]# dig -t A hdss7-21.host.com @10.254.0.2 +short
192.168.0.21
[root@hdss7-21 ~]# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
kubernetes   ClusterIP   10.254.0.1   <none>        443/TCP   13h   <none>

[root@hdss7-21 ~]# kubectl get pods
NAME                                        READY   STATUS             RESTARTS   AGE
nginx-deployment-5754944d6c-mf6md           1/1     Running            0          177m
nginx-deployment-5754944d6c-rbhch           1/1     Running            0          179m
[root@hdss7-21 ~]# kubectl expose deployment nginx-deployment --port=80
service/nginx-deployment exposed
[root@hdss7-21 ~]# kubectl get svc
NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes         ClusterIP   10.254.0.1     <none>        443/TCP   13h
nginx-deployment   ClusterIP   10.254.236.2   <none>        80/TCP    10s
# default 为名称空间namespace     nginx-deployment.default.svc.cluster.local. 只在集群内部有效
[root@hdss7-21 ~]# dig -t A nginx-deployment.default.svc.cluster.local. @10.254.0.2 +short
10.254.236.2
```

![image-20200816005133207](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816005133207.png)

```shell
[root@hdss7-21 ~]# kubectl get pod -o wide
NAME                                        READY   STATUS             RESTARTS   AGE     IP           NODE                NOMINATED NODE   READINESS GATES
nginx-deployment-5754944d6c-mf6md           1/1     Running            0          3h7m    172.7.22.2   hdss7-22.host.com   <none>           <none>
nginx-deployment-5754944d6c-rbhch           1/1     Running            0          3h10m   172.7.21.2   hdss7-21.host.com   <none>           <none>
# 更改了网络（dns），一定要删掉之前的pod，然后会自动重启新的pod，否则原来的pod还是用之前的网络
[root@hdss7-21 ~]# kubectl exec -it nginx-deployment-5754944d6c-rbhch /bin/bash
root@nginx-deployment-5754944d6c-l69n5:/# cat /etc/resolv.conf
nameserver 10.254.0.2                              # dns
search default.svc.cluster.local svc.cluster.local cluster.local host.com
options ndots:5
root@nginx-deployment-5754944d6c-rbhch:/# ping 10.254.236.2              # 可以ping通
root@nginx-deployment-5754944d6c-rbhch:/# ping nginx-deployment.default.svc.cluster.local.  # 可以ping通
root@nginx-deployment-5754944d6c-rbhch:/# ping nginx-deployment          # 可以ping通
root@nginx-deployment-5754944d6c-rbhch:/# ping www.baidu.com             # 也可以ping通
```

![image-20200816010811437](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816010811437.png)

![image-20200816010900318](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816010900318.png)

![image-20200816010935184](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816010935184.png)

![image-20200816010950081](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816010950081.png)

# Kubernetes的服务暴露插件--traefik（Ingress控制器）

![image-20200816010603184](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816010603184.png)

![image-20200816085110031](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816085110031.png)

```shell
# 7-200
[root@hdss7-200 ~]# docker pull traefik:v1.7.2-alpine
[root@hdss7-200 ~]# docker images|grep traefik
traefik                         v1.7.2-alpine       add5fac61ae5        22 months ago       72.4MB
[root@hdss7-200 ~]# docker tag add5fac61ae5 harbor.od.com/public/traefik:v1.7.2
[root@hdss7-200 ~]# docker push harbor.od.com/public/traefik:v1.7.2
## 资源配置清单
[root@hdss7-200 ~]# mkdir -p /data/k8s-yaml/traefik/
[root@hdss7-200 ~]# vi /data/k8s-yaml/traefik/rbac.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: traefik-ingress-controller
rules:
  - apiGroups:
      - ""
    resources:
      - services
      - endpoints
      - secrets
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - extensions
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: traefik-ingress-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: traefik-ingress-controller
subjects:
- kind: ServiceAccount
  name: traefik-ingress-controller
  namespace: kube-system
  
# vi /data/k8s-yaml/traefik/daemonset.yaml  

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: traefik-ingress-controller
  namespace: kube-system
  labels:
    k8s-app: traefik-ingress-lb
spec:
  template:
    metadata:
      labels:
        k8s-app: traefik-ingress-lb
        name: traefik-ingress-lb
    spec:
      serviceAccountName: traefik-ingress-controller
      terminationGracePeriodSeconds: 60
      containers:
      - image: harbor.od.com/public/traefik:v1.7.2
        name: traefik-ingress-lb
        ports:
        - name: http
          containerPort: 80
          hostPort: 81
        - name: admin-web
          containerPort: 8080
        securityContext:
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        args:
        - --api
        - --kubernetes
        - --logLevel=INFO
        - --insecureskipverify=true
        - --kubernetes.endpoint=https://192.168.0.10:7443
        - --accesslog
        - --accesslog.filepath=/var/log/traefik_access.log
        - --traefiklog
        - --traefiklog.filepath=/var/log/traefik.log
        - --metrics.prometheus
      
# vi /data/k8s-yaml/traefik/svc.yaml

kind: Service
apiVersion: v1
metadata:
  name: traefik-ingress-service
  namespace: kube-system
spec:
  selector:
    k8s-app: traefik-ingress-lb
  ports:
    - protocol: TCP
      port: 80
      name: web
    - protocol: TCP
      port: 8080
      name: admin-web
      
# vi /data/k8s-yaml/traefik/ingress.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: traefik-web-ui
  namespace: kube-system
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
  - host: traefik.od.com
    http:
      paths:
      - path: /
        backend:
          serviceName: traefik-ingress-service
          servicePort: 8080
```

```shell
# 7-21
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/traefik/rbac.yaml
serviceaccount/traefik-ingress-controller created
clusterrole.rbac.authorization.k8s.io/traefik-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/traefik-ingress-controller created
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/traefik/daemonset.yaml
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/traefik/svc.yaml
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/traefik/ingress.yaml

[root@hdss7-21 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS              RESTARTS   AGE
coredns-6b6c4f9648-ns59q           1/1     Running             0          9h
traefik-ingress-controller-mzcdq   0/1     ContainerCreating   0          7m2s
traefik-ingress-controller-sshf4   0/1     ContainerCreating   0          7m2s
[root@hdss7-21 ~]# kubectl describe pod traefik-ingress-controller-mzcdq -n kube-system
```

![image-20200816094354679](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816094354679.png)

```shell
# 重启kubelet      7-21、7-22
[root@hdss7-22 ~]# ps aux|grep kubelet
root     15656  1.1  0.9 858940 76088 ?        Sl   00:29   6:36 ./kubelet --anonymous-auth=false --cgroup-driver systemd --cluster-dns 10.254.0.2 --cluster-domain cluster.local --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice --fail-swap-on=false --client-ca-file ./cert/ca.pem --tls-cert-file ./cert/kubelet.pem --tls-private-key-file ./cert/kubelet-key.pem --hostname-override hdss7-22.host.com --image-gc-high-threshold 20 --image-gc-low-threshold 10 --kubeconfig ./conf/kubelet.kubeconfig --log-dir /data/logs/kubernetes/kube-kubelet --pod-infra-container-image harbor.od.com/public/pause:latest --root-dir /data/kubelet
[root@hdss7-22 ~]# kill -9 15656
[root@hdss7-21 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS              RESTARTS   AGE
coredns-6b6c4f9648-ns59q           1/1     Running             0          9h
traefik-ingress-controller-mzcdq   0/1     ContainerCreating   0          17m
traefik-ingress-controller-sshf4   0/1     ContainerCreating   0          17m
[root@hdss7-21 ~]# kubectl delete pod traefik-ingress-controller-mzcdq -n kube-system
[root@hdss7-21 ~]# kubectl delete pod traefik-ingress-controller-sshf4 -n kube-system
[root@hdss7-21 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS              RESTARTS   AGE
coredns-6b6c4f9648-ns59q           1/1     Running             0          9h
traefik-ingress-controller-9hjmm   0/1     ContainerCreating   0          26s
traefik-ingress-controller-b8wjl   0/1     ContainerCreating   0          9s
# 重启docker
[root@hdss7-21 ~]# systemctl restart docker.service
[root@hdss7-22 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-6b6c4f9648-ns59q           1/1     Running   0          9h
traefik-ingress-controller-9hjmm   1/1     Running   0          3m4s
traefik-ingress-controller-b8wjl   1/1     Running   0          2m47s
[root@hdss7-21 ~]# netstat -luntp | grep 81
tcp6       0      0 :::81                   :::*                    LISTEN      1698/docker-proxy
```

```shell
# 7-11、7-12
vi /etc/nginx/conf.d/od.com.conf

upstream default_backend_traefik {
    server 192.168.0.21:81    max_fails=3 fail_timeout=10s;
    server 192.168.0.22:81    max_fails=3 fail_timeout=10s;
}
server {
    server_name *.od.com;

    location / {
        proxy_pass http://default_backend_traefik;
        proxy_set_header Host       $http_host;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
    }
}

[root@hdss7-11 ~]#  nginx -s reload
```

```shell
# 7-11
[root@hdss7-11 etc]# vi /var/named/od.com.zone
$ORIGIN od.com.
$TTL 600    ; 10 minutes
@           IN SOA    dns.od.com. dnsadmin.od.com. (
                2019120904 ; serial                  # 加1
                10800      ; refresh (3 hours)
                900        ; retry (15 minutes)
                604800     ; expire (1 week)
                86400      ; minimum (1 day)
                )
                NS   dns.od.com.
$TTL 60    ; 1 minute
dns                A    192.168.0.11
harbor             A    192.168.0.200
k8s-yaml           A    192.168.0.200
traefik            A    192.168.0.10              # 新增

[root@hdss7-11 etc]# systemctl restart named
# 浏览器访问
http://traefik.od.com/dashboard/
```

![image-20200816140533793](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816140533793.png)

# Kubernetes的GUI管理工具--dashboard

![image-20200816141037328](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816141037328.png)

* HDSS7-200.host.com

  ```shell
  [root@hdss7-200 ~]# docker pull k8scn/kubernetes-dashboard-amd64:v1.8.3
  [root@hdss7-200 traefik]# docker images | grep dashboard
  k8scn/kubernetes-dashboard-amd64   v1.8.3              fcac9aa03fd6        2 years ago         102MB
  [root@hdss7-200 ~]# docker tag fcac9aa03fd6 harbor.od.com/public/dashboard:v1.8.3
  [root@hdss7-200 ~]# docker push harbor.od.com/public/dashboard:v1.8.3
  
  # 准备资源配置清单
  [root@hdss7-200 ~]# mkdir -p /data/k8s-yaml/dashboard && cd /data/k8s-yaml/dashboard
  # 官方  https://github.com/kubernetes/kubernetes/tree/release-1.18/cluster/addons/dashboard
  # vi /data/k8s-yaml/dashboard/rbac.yaml
  
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
    name: kubernetes-dashboard-admin
    namespace: kube-system
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: kubernetes-dashboard-admin
    namespace: kube-system
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard-admin
    namespace: kube-system
    
  # vi /data/k8s-yaml/dashboard/secret.yaml
  
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      # Allows editing resource and makes sure it is created first.
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kubernetes-dashboard-certs
    namespace: kube-system
  type: Opaque
  ---
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      # Allows editing resource and makes sure it is created first.
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kubernetes-dashboard-key-holder
    namespace: kube-system
  type: Opaque
  
  # vi /data/k8s-yaml/dashboard/configmap.yaml
  
  apiVersion: v1
  kind: ConfigMap
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      # Allows editing resource and makes sure it is created first.
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kubernetes-dashboard-settings
    namespace: kube-system
    
  # vi /data/k8s-yaml/dashboard/svc.yaml  
  
  apiVersion: v1
  kind: Service
  metadata:
    name: kubernetes-dashboard
    namespace: kube-system
    labels:
      k8s-app: kubernetes-dashboard
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  spec:
    selector:
      k8s-app: kubernetes-dashboard
    ports:
    - port: 443
      targetPort: 8443
      
  # vi /data/k8s-yaml/dashboard/ingress.yaml
  
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kubernetes-dashboard
    namespace: kube-system
    annotations:
      kubernetes.io/ingress.class: traefik
  spec:
    rules:
    - host: dashboard.od.com
      http:
        paths:
        - backend:
            serviceName: kubernetes-dashboard
            servicePort: 443
  
  # vi /data/k8s-yaml/dashboard/secret.yaml
  
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      # Allows editing resource and makes sure it is created first.
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kubernetes-dashboard-certs
    namespace: kube-system
  type: Opaque
  ---
  apiVersion: v1
  kind: Secret
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      # Allows editing resource and makes sure it is created first.
      addonmanager.kubernetes.io/mode: EnsureExists
    name: kubernetes-dashboard-key-holder
    namespace: kube-system
  type: Opaque
            
  # vi /data/k8s-yaml/dashboard/deployment.yaml
  
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: kubernetes-dashboard
    namespace: kube-system
    labels:
      k8s-app: kubernetes-dashboard
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  spec:
    selector:
      matchLabels:
        k8s-app: kubernetes-dashboard
    template:
      metadata:
        labels:
          k8s-app: kubernetes-dashboard
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ''
      spec:
        priorityClassName: system-cluster-critical
        containers:
        - name: kubernetes-dashboard
          image: harbor.od.com/public/dashboard:v1.8.3
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 50m
              memory: 100Mi
          ports:
          - containerPort: 8443
            protocol: TCP
          args:
            # PLATFORM-SPECIFIC ARGS HERE
            - --auto-generate-certificates
          volumeMounts:
          - name: kubernetes-dashboard-certs
            mountPath: /certs
          - name: tmp-volume
            mountPath: /tmp
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
        volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
        serviceAccountName: kubernetes-dashboard-admin
        tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
  ```

  ```shell
  # HDSS7-11.host.com
  vi /var/named/od.com.zone
  $ORIGIN od.com.
  $TTL 600    ; 10 minutes
  @           IN SOA    dns.od.com. dnsadmin.od.com. (
                  2019120905 ; serial                     # 加1
                  10800      ; refresh (3 hours)
                  900        ; retry (15 minutes)
                  604800     ; expire (1 week)
                  86400      ; minimum (1 day)
                  )
                  NS   dns.od.com.
  $TTL 60    ; 1 minute
  dns                A    192.168.0.11
  harbor             A    192.168.0.200
  k8s-yaml           A    192.168.0.200
  traefik            A    192.168.0.10
  dashboard          A    192.168.0.10                 # 新增
  
  [root@hdss7-11 etc]# systemctl restart named
  [root@hdss7-11 etc]# dig -t A dashboard.od.com @192.168.0.11 +short
  192.168.0.10
  
  # 浏览器打开：http://k8s-yaml.od.com/dashboard 检查资源配置清单文件是否正确创建
  # 在任意运算节点应用资源配置清单
  
  # 7-21
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/rbac.yaml 
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/secret.yaml 
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/configmap.yaml 
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/svc.yaml 
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/ingress.yaml 
  [root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/deployment.yaml 
  
  [root@hdss7-21 bin]# kubectl get pods -n kube-system
  NAME                                    READY   STATUS    RESTARTS   AGE
  coredns-6b6c4f9648-ns59q                1/1     Running   0          14h
  kubernetes-dashboard-6fb55bd766-6vwbp   1/1     Running   0          9s
  traefik-ingress-controller-9hjmm        1/1     Running   0          5h20m
  traefik-ingress-controller-b8wjl        1/1     Running   0          5h20m
  [root@hdss7-21 bin]# kubectl get svc -n kube-system
  NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
  coredns                   ClusterIP   10.254.0.2      <none>        53/UDP,53/TCP,9153/TCP   14h
  kubernetes-dashboard      ClusterIP   10.254.67.233   <none>        443/TCP                  4m17s
  traefik-ingress-service   ClusterIP   10.254.86.27    <none>        80/TCP,8080/TCP          5h39m
  [root@hdss7-21 bin]# kubectl get ingress -n kube-system
  NAME                   HOSTS              ADDRESS   PORTS   AGE
  kubernetes-dashboard   dashboard.od.com             80      4m52s
  traefik-web-ui         traefik.od.com               80      5h40m
  [root@hdss7-21 bin]# dig -t A dashboard.od.com @10.254.0.2 +short
  192.168.0.10
  # 浏览器访问，先跳过token验证
  http://dashboard.od.com
  ```

  ![image-20200816152447196](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200816152447196.png)

  * 第一步：创建账户
  * 第二步：定义角色
  * 第三步：绑定角色

  ```shell
  [root@hdss7-21 bin]# kubectl get clusterrole
  traefik-ingress-controller                                             6h6m
  [root@hdss7-21 bin]# kubectl get clusterrole cluster-admin -o yaml
  ```

  ```shell
  # 创建证书
  # 7-200
  [root@hdss7-200 certs]# (umask 077;openssl genrsa -out dashboard.od.com.key 2048)
  Generating RSA private key, 2048 bit long modulus
  .....................+++
  ......................+++
  e is 65537 (0x10001)
  [root@hdss7-200 certs]# openssl req -new -key dashboard.od.com.key -out dashboard.od.com.csr -subj "/CN=dashboard.od.com/C=CN/ST=BJ/L=Beijing/O=OldboyEdu/OU=ops"
  
  -rw------- 1 root root 1675 8月  16 16:57 dashboard.od.com.key
  -rw-r--r-- 1 root root 1005 8月  16 16:58 dashboard.od.com.csr
  [root@hdss7-200 certs]# openssl x509 -req -in dashboard.od.com.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out dashboard.od.com.crt -days 3650
  # dashboard.od.com.crt  dashboard.od.com.key
  [root@hdss7-200 certs]# cfssl-certinfo -cert dashboard.od.com.crt
  
  # 7-11
  [root@hdss7-11 etc]# cd /etc/nginx/
  [root@hdss7-11 nginx]# mkdir certs
  [root@hdss7-11 nginx]# cd certs/
  [root@hdss7-11 certs]# scp hdss7-200:/opt/certs/dashboard.od.com.crt .
  [root@hdss7-11 certs]# scp hdss7-200:/opt/certs/dashboard.od.com.key .
  
  [root@hdss7-11 conf.d]# vi /etc/nginx/conf.d/dashboard.od.com.conf
  server {
      listen       80;
      server_name  dashboard.od.com;
  
      rewrite ^(.*)$ https://${server_name}$1 permanent;
  }
  server {
      listen       443 ssl;
      server_name  dashboard.od.com;
  
      ssl_certificate "certs/dashboard.od.com.crt";
      ssl_certificate_key "certs/dashboard.od.com.key";
      ssl_session_cache shared:SSL:1m;
      ssl_session_timeout  10m;
      ssl_ciphers HIGH:!aNULL:!MD5;
      ssl_prefer_server_ciphers on;
  
      location / {
          proxy_pass http://default_backend_traefik;
  	      proxy_set_header Host       $http_host;
          proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
      }
  }
  
  [root@hdss7-11 conf.d]# nginx -t
  [root@hdss7-11 conf.d]# nginx -s reload
  
  # 浏览器访问    Chrome无法访问，Safari可以访问
  https://dashboard.od.com/#!/login
  
  # 7-21
  [root@hdss7-21 bin]# kubectl get secret -n kube-system
  NAME                                     TYPE                                  DATA   AGE
  coredns-token-rznw2                      kubernetes.io/service-account-token   3      16h
  default-token-zz27k                      kubernetes.io/service-account-token   3      28h
  kubernetes-dashboard-admin-token-lj4ps   kubernetes.io/service-account-token   3      143m
  kubernetes-dashboard-key-holder          Opaque                                2      142m
  traefik-ingress-controller-token-dtmdk   kubernetes.io/service-account-token   3      8h
  [root@hdss7-21 bin]# kubectl describe secret kubernetes-dashboard-admin-token-lj4ps -n kube-system
  eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1sajRwcyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjZhODg3M2FlLWU1NjItNGIyZi1hNzFhLTljMjBhZTRhNTVhZSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.QvC4PoRnZ7EuVBkUr-qJym5NwIElkMjQ-wBVUFt1eRWIcA-DViVsND89txqJ0QS9WrAbs2tvnjTdPhVzagz1tN9E_o8GpmkF6gzOw_mHtQXSho0vOV3Z2_-mcAx0U0q-YK1bXJdQAO4Pd-dk3GD8tbvv9lUqHuaH8i7nRwzddHphcX1r7GdNBQ8hwYW1ntLSYCs-h6TOCQeszIhv7UMh-m75vfUMyc_yNLiXZrgzZ-N_UD8sl7UVXIS-c_xvvK_ktbXvpMn0Iq605qtOIhG1Z8EETOJyYzBcqBDp84jKUA_pXIeU__4gcsBQke89Eb0jQPak1kxVi_MmEETkA6IUkA
  # 登陆页面的token上输入该token
  ```

  ```shell
  # 应用新版dashboard
  # 7-200
  [root@hdss7-200 certs]# docker pull hexun/kubernetes-dashboard-amd64:v1.10.1
  [root@hdss7-200 certs]# docker images|grep dashboard
  hexun/kubernetes-dashboard-amd64   v1.10.1             f9aed6605b81        20 months ago       122MB
  [root@hdss7-200 certs]# docker tag f9aed6605b81 harbor.od.com/public/dashboard:v1.10.1
  [root@hdss7-200 certs]# docker push harbor.od.com/public/dashboard:v1.10.1
  [root@hdss7-200 certs]# cd /data/k8s-yaml/dashboard/
  [root@hdss7-200 dashboard]# vi deployment.yaml
  # 改为harbor.od.com/public/dashboard:v1.10.1
  
  # dashboard页面上
  kube-system - kubernetes-dashboard - 查看/编辑YAML文件 - 更改为 harbor.od.com/public/dashboard:v1.10.1 - 更新
  # 7-21
  [root@hdss7-21 bin]# kubectl get pods -n kube-system
  NAME                                    READY   STATUS    RESTARTS   AGE
  coredns-6b6c4f9648-ns59q                1/1     Running   0          17h
  kubernetes-dashboard-79f99598bf-grrwr   1/1     Running   0          117s         # 已更新
  traefik-ingress-controller-9hjmm        1/1     Running   0          7h58m
  traefik-ingress-controller-b8wjl        1/1     Running   0          7h57m
  # dashboard页面刷新，重新输入token登陆
  [root@hdss7-21 bin]# kubectl describe secret kubernetes-dashboard-admin-token-lj4ps -n kube-system
  ```

  * **创建不同权限的token**

  ```shell
  # todo 只能查看指定namespace下的资源，不能修改 
  # mini token
  # 7-200
  cd /data/k8s-yaml/dashboard
  [root@hdss7-200 dashboard]# vi rbac-mini.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
    name: kubernetes-dashboard
    namespace: kube-system
  ---
  # 两部分内容合在一起
  # 上面是官方dashboard-deployment.yaml文档中的第一部分内容 https://github.com/kubernetes/kubernetes/blob/release-1.18/cluster/addons/dashboard/dashboard-deployment.yaml
  # 以下是官方dashboard-rbac.yaml文档中的全部内容 https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.18/cluster/addons/dashboard/dashboard-rbac.yaml
  kind: Role
  apiVersion: rbac.authorization.k8s.io/v1
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
    name: kubernetes-dashboard-minimal
    namespace: kube-system
  rules:
    # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics from heapster.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:"]
    verbs: ["get"]
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: kubernetes-dashboard-minimal
    namespace: kube-system
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: kubernetes-dashboard-minimal
  subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kube-system
    
    
  # 7-21  
  [root@hdss7-21 bin]# kubectl apply -f http://k8s-yaml.od.com/dashboard/rbac-mini.yaml
  serviceaccount/kubernetes-dashboard created
  role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
  rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
  [root@hdss7-21 bin]#
  
  # 7-200
  [root@hdss7-200 dashboard]# vi /data/k8s-yaml/dashboard/deployment.yaml
  改为serviceAccountName: kubernetes-dashboard          # 对应mini文档中的accountname
  
  # 7-21
  [root@hdss7-21 bin]# kubectl apply -f http://k8s-yaml.od.com/dashboard/deployment.yaml
  deployment.apps/kubernetes-dashboard configured
  [root@hdss7-21 bin]# kubectl get pods -n kube-system
  NAME                                    READY   STATUS    RESTARTS   AGE
  coredns-6b6c4f9648-ns59q                1/1     Running   0          17h
  kubernetes-dashboard-85f97cb957-v85bk   1/1     Running   0          23s      # 滚动更新好了
  traefik-ingress-controller-9hjmm        1/1     Running   0          8h
  traefik-ingress-controller-b8wjl        1/1     Running   0          8h
  [root@hdss7-21 bin]# kubectl get secret -n kube-system
  NAME                                     TYPE                                  DATA   AGE
  coredns-token-rznw2                      kubernetes.io/service-account-token   3      17h
  default-token-zz27k                      kubernetes.io/service-account-token   3      29h
  kubernetes-dashboard-admin-token-lj4ps   kubernetes.io/service-account-token   3      3h16m  # admin
  kubernetes-dashboard-certs               Opaque                                0      3h15m
  kubernetes-dashboard-key-holder          Opaque                                2      3h15m
  kubernetes-dashboard-token-pptnz         kubernetes.io/service-account-token   3      6m7s   # mini
  traefik-ingress-controller-token-dtmdk   kubernetes.io/service-account-token   3      8h
  # mini的
  [root@hdss7-21 bin]# kubectl describe secret kubernetes-dashboard-token-pptnz -n kube-system
  eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1wcHRueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjNjN2U4MDMyLTAyODgtNDRjZC05ZTI1LWFlZTc4ZTBhYTg3OCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.Yv8kZ91J8JduORRy37sbp1dcxkokgQrSmzO3roTwBXCspnHpa5pl_zoLA5uzd7C8ZnRMFSeNUfQ562vbaCkYF8fTsaWGWqejg04Om3pjBsg7GE73I3e9mbVn8VlpicYXote07IF9T1n8sSHiu60bLuv-MZKP1Zoo2TREulsDOX4ftQevCn1RarXmnq5IKNPFiQgUZKgW3oXGxdfcq7wvCz0toXCQbV3IRdGfpbe0u1rzmA2Yopd7KwoF7Kpzs11WKYBHIxro6wij1MldMqy-kSDbyxe6JqOSir2uNpwSWEPPwsqI_GJDbuQglaQXhlkroj1dyTl6JEz-Enpbt2_X0Q
  # 浏览器重新token登陆dashboard
  # 页面可看到的内容非常少
  # 使用admin的token还是可以登陆到dashboard中
  [root@hdss7-21 bin]# kubectl describe secret kubernetes-dashboard-admin-token-lj4ps -n kube-system
  
  # dashboard页面上切回v1.8.3版本
  
  # 7-200  最后还是改为admin v1.8.3
  [root@hdss7-200 dashboard]# vi /data/k8s-yaml/dashboard/deployment.yaml
  改为serviceAccountName: kubernetes-dashboard-admin         
  v1.8.3
  # 7-21
  [root@hdss7-21 bin]# kubectl apply -f http://k8s-yaml.od.com/dashboard/deployment.yaml
  deployment.apps/kubernetes-dashboard configured
  [root@hdss7-21 bin]# kubectl get pods -n kube-system
  
  # 参考的官方内容
  # https://github.com/kubernetes/kubernetes/blob/release-1.18/cluster/addons/dashboard/dashboard-deployment.yaml
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      k8s-app: kubernetes-dashboard
      addonmanager.kubernetes.io/mode: Reconcile
    name: kubernetes-dashboard
    namespace: kube-system
  ---
  ```

# Kubernetes的dashboard的监控小插件--heapster

```shell
# heapster 已淘汰
# 部署heapster
## HDSS7-200.host.com
[root@hdss7-200 ~]# docker pull quay.io/bitnami/heapster:1.5.4
[root@hdss7-200 dashboard]# docker images|grep heapster
quay.io/bitnami/heapster           1.5.4               c359b95ad38b        18 months ago       136MB
[root@hdss7--200 ~]# docker tag c359b95ad38b harbor.od.com/public/heapster:v1.5.4
[root@hdss7--200 ~]# docker push !$
## 准备资源配置清单
[root@hdss7-200 harbor]# mkdir -p /data/k8s-yaml/dashboard/heapster/
[root@hdss7-200 harbor]# vi /data/k8s-yaml/dashboard/heapster/rbac.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: heapster
  namespace: kube-system
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: heapster
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:heapster
subjects:
- kind: ServiceAccount
  name: heapster
  namespace: kube-system
  
# vi /data/k8s-yaml/dashboard/heapster/deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: heapster
  namespace: kube-system
spec:
  replicas: 1
  template:
    metadata:
      labels:
        task: monitoring
        k8s-app: heapster
    spec:
      serviceAccountName: heapster
      containers:
      - name: heapster
        image: harbor.od.com/public/heapster:v1.5.4
        imagePullPolicy: IfNotPresent
        command:
        - /opt/bitnami/heapster/bin/heapster
        - --source=kubernetes:https://kubernetes.default

# vi /data/k8s-yaml/dashboard/heapster/svc.yaml

apiVersion: v1
kind: Service
metadata:
  labels:
    task: monitoring
    # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons)
    # If you are NOT using this as an addon, you should comment out this line.
    kubernetes.io/cluster-service: 'true'
    kubernetes.io/name: Heapster
  name: heapster
  namespace: kube-system
spec:
  ports:
  - port: 80
    targetPort: 8082
  selector:
    k8s-app: heapster

# 应用资源配置清单
## 任意运算节点上
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/rbac.yaml 
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/deployment.yaml 
[root@hdss7-21 ~]# kubectl apply -f http://k8s-yaml.od.com/dashboard/heapster/svc.yaml 
[root@hdss7-21 ~]# kubectl get pods -n kube-system
# 重启dashboard
http://dashboard.od.com
```

# Kubernetes集群的生产维护经验

## 平滑升级

```shell
[root@hdss7-21 bin]# kubectl get node
NAME                STATUS   ROLES         AGE   VERSION
hdss7-21.host.com   Ready    master,node   29h   v1.15.2
hdss7-22.host.com   Ready    master,node   29h   v1.15.2
[root@hdss7-21 bin]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES
coredns-6b6c4f9648-ns59q                1/1     Running   0          18h   172.7.21.3   hdss7-21.host.com   <none>           <none>
kubernetes-dashboard-6fb55bd766-ljskk   1/1     Running   0          13m   172.7.22.8   hdss7-22.host.com   <none>           <none>
traefik-ingress-controller-9hjmm        1/1     Running   0          9h    172.7.22.3   hdss7-22.host.com   <none>           <none>
traefik-ingress-controller-b8wjl        1/1     Running   0          9h    172.7.21.7   hdss7-21.host.com   <none>           <none>
# 7-21
[root@hdss7-21 data]# kubectl delete node hdss7-21.host.com
node "hdss7-21.host.com" deleted
[root@hdss7-21 data]# kubectl get nodes
NAME                STATUS   ROLES         AGE   VERSION
hdss7-22.host.com   Ready    master,node   32h   v1.15.2
# 都调度到7-22上了
[root@hdss7-21 data]# kubectl get pod -n kube-system -o wide
NAME                                    READY   STATUS    RESTARTS   AGE     IP            NODE                NOMINATED NODE   READINESS GATES
coredns-6b6c4f9648-hwmw8                1/1     Running   0          36s     172.7.22.10   hdss7-22.host.com   <none>           <none>
kubernetes-dashboard-6fb55bd766-ljskk   1/1     Running   0          3h58m   172.7.22.8    hdss7-22.host.com   <none>           <none>
traefik-ingress-controller-9hjmm        1/1     Running   0          12h     172.7.22.3    hdss7-22.host.com   <none>           <none>
[root@hdss7-21 data]# dig -t A kubernetes.default.svc.cluster.local @10.254.0.2 +short
10.254.0.1

# 7-11上
[root@hdss7-11 conf.d]# vi /etc/nginx/nginx.conf
# 21给注释掉
#        server 192.168.0.21:6443     max_fails=3 fail_timeout=30s;
[root@hdss7-11 conf.d]# vi /etc/nginx/conf.d/od.com.conf
# 21给注释掉
#    server 192.168.0.21:81    max_fails=3 fail_timeout=10s;
[root@hdss7-11 conf.d]# nginx -t
[root@hdss7-11 conf.d]# nginx -s reload

# 7-21
[root@hdss7-21 opt]# mkdir k8s_v1.15.4
[root@hdss7-21 opt]# tar -zxvf /data/kubernetes-server-linux-amd64.tar.gz -C /opt/k8s_v1.15.4/
[root@hdss7-21 opt]# cd k8s_v1.15.4/
[root@hdss7-21 k8s_v1.15.4]# ls
kubernetes
[root@hdss7-21 k8s_v1.15.4]# mv kubernetes/* ../kubernetes-v1.15.4/
[root@hdss7-21 opt]# rm -rf kubernetes
[root@hdss7-21 opt]# ln -s /opt/kubernetes-v1.15.4/ /opt/kubernetes
[root@hdss7-21 opt]# cd kubernetes-v1.15.4/
[root@hdss7-21 kubernetes-v1.15.4]# rm -rf kubernetes-src.tar.gz
[root@hdss7-21 kubernetes-v1.15.4]# cd server/bin/
[root@hdss7-21 bin]# rm -rf *.tar *_tag
[root@hdss7-21 cert]# cp /opt/kubernetes-v1.15.2/server/bin/cert/* .
[root@hdss7-21 cert]# ll
总用量 40
-rw------- 1 root root 1679 8月  17 22:32 apiserver-key.pem
-rw-r--r-- 1 root root 1598 8月  17 22:32 apiserver.pem
-rw------- 1 root root 1675 8月  17 22:32 ca-key.pem
-rw-r--r-- 1 root root 1346 8月  17 22:32 ca.pem
-rw------- 1 root root 1675 8月  17 22:32 client-key.pem
-rw-r--r-- 1 root root 1363 8月  17 22:32 client.pem
-rw------- 1 root root 1679 8月  17 22:32 kubelet-key.pem
-rw-r--r-- 1 root root 1468 8月  17 22:32 kubelet.pem
-rw------- 1 root root 1679 8月  17 22:32 kube-proxy-client-key.pem
-rw-r--r-- 1 root root 1375 8月  17 22:32 kube-proxy-client.pem
[root@hdss7-21 cert]# cp /opt/kubernetes-v1.15.2/server/bin/conf/* ../conf/
[root@hdss7-21 cert]# cd ../conf/
[root@hdss7-21 conf]# ll
总用量 24
-rw-r--r-- 1 root root 2223 8月  17 22:33 audit.yaml
-rw-r--r-- 1 root root  258 8月  17 22:33 k8s-node.yaml
-rw------- 1 root root 6198 8月  17 22:33 kubelet.kubeconfig
-rw------- 1 root root 6222 8月  17 22:33 kube-proxy.kubeconfig
[root@hdss7-21 conf]# cd ..
[root@hdss7-21 bin]# cp /opt/kubernetes-v1.15.2/server/bin/*.sh .
[root@hdss7-21 bin]# supervisorctl restart all
# 如果启动失败，查看日志，有的是端口被占用，则 ps -ef | grep etcd  然后 kill -9 pid ，再supervisorctl start etcd-server-7-21   
# ps -ef | grep xxx    按顺序检查etcd-server、kube-apiserver、kube-controller-manager、kube-scheduler、kube-kubelet、kube-proxy、flanneld
[root@hdss7-21 bin]# supervisorctl status
etcd-server-7-21                 RUNNING   pid 14925, uptime 0:05:37
flanneld-7-21                    RUNNING   pid 17227, uptime 0:00:36
kube-apiserver-7-21              RUNNING   pid 15818, uptime 0:03:47
kube-controller-manager-7-21     RUNNING   pid 14010, uptime 0:06:56
kube-kubelet-7-21                RUNNING   pid 12932, uptime 0:07:29
kube-proxy-7-21                  RUNNING   pid 12937, uptime 0:07:29
kube-scheduler-7-21              RUNNING   pid 16222, uptime 0:02:46
[root@hdss7-21 bin]# kubectl get nodes
NAME                STATUS   ROLES         AGE     VERSION
hdss7-21.host.com   Ready    <none>        7m42s   v1.15.4                    # 升级为v1.15.4
hdss7-22.host.com   Ready    master,node   2d9h    v1.15.2
# 再回滚到 v1.15.2
[root@hdss7-21 bin]# cd /opt/
# 删除软连接，再新建软连接
[root@hdss7-21 opt]# rm -rf kubernetes
[root@hdss7-21 opt]# ln -s /opt/kubernetes-v1.15.2/ /opt/kubernetes
# 重启
[root@hdss7-21 opt]# supervisorctl restart all
[root@hdss7-21 opt]# supervisorctl status
etcd-server-7-21                 RUNNING   pid 20418, uptime 0:07:01
flanneld-7-21                    RUNNING   pid 22966, uptime 0:01:10
kube-apiserver-7-21              RUNNING   pid 20948, uptime 0:05:50
kube-controller-manager-7-21     RUNNING   pid 22184, uptime 0:03:00
kube-kubelet-7-21                RUNNING   pid 18968, uptime 0:09:05
kube-proxy-7-21                  RUNNING   pid 18424, uptime 0:09:13
kube-scheduler-7-21              RUNNING   pid 22506, uptime 0:02:13
[root@hdss7-21 opt]# kubectl get nodes
NAME                STATUS   ROLES         AGE    VERSION
hdss7-21.host.com   Ready    <none>        19m    v1.15.2                  # 回滚成功
hdss7-22.host.com   Ready    master,node   2d9h   v1.15.2
# 7-11上
[root@hdss7-11 conf.d]# vi /etc/nginx/nginx.conf
# 21给注释取消掉
        server 192.168.0.21:6443     max_fails=3 fail_timeout=30s;
[root@hdss7-11 conf.d]# vi /etc/nginx/conf.d/od.com.conf
# 21给注释取消掉
    server 192.168.0.21:81    max_fails=3 fail_timeout=10s;
[root@hdss7-11 conf.d]# nginx -t
[root@hdss7-11 conf.d]# nginx -s reload
```

```shell
# 7-12上
[root@hdss7-12 data]# cd /etc/nginx/
[root@hdss7-12 nginx]# mkdir certs
[root@hdss7-12 nginx]# cd certs/
[root@hdss7-12 certs]# scp hdss7-200:/opt/certs/dashboard.od.com.crt .
[root@hdss7-12 certs]# scp hdss7-200:/opt/certs/dashboard.od.com.key .
[root@hdss7-12 certs]# cd ..
[root@hdss7-12 nginx]# cd conf.d/
[root@hdss7-12 conf.d]# vi dashboard.od.com.conf
# 复制 7-11上 /etc/nginx/conf.d/dashboard.od.com.conf 内容
server {
    listen       80;
    server_name  dashboard.od.com;

    rewrite ^(.*)$ https://${server_name}$1 permanent;
}
server {
    listen       443 ssl;
    server_name  dashboard.od.com;

    ssl_certificate "certs/dashboard.od.com.crt";
    ssl_certificate_key "certs/dashboard.od.com.key";
    ssl_session_cache shared:SSL:1m;
    ssl_session_timeout  10m;
    ssl_ciphers HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers on;

    location / {
        proxy_pass http://default_backend_traefik;
          proxy_set_header Host       $http_host;
        proxy_set_header x-forwarded-for $proxy_add_x_forwarded_for;
    }
}
[root@hdss7-12 conf.d]# nginx -t
[root@hdss7-12 conf.d]# nginx -s reload
```

# 总结

![image-20200817232451819](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200817232451819.png)

```shell
[root@hdss7-21 opt]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.254.0.1:443 nq
  -> 192.168.0.21:6443            Masq    1      1          0
  -> 192.168.0.22:6443            Masq    1      1          0
TCP  10.254.0.2:53 nq
  -> 172.7.21.5:53                Masq    1      0          0
TCP  10.254.0.2:9153 nq
  -> 172.7.21.5:9153              Masq    1      0          0
TCP  10.254.67.233:443 nq
  -> 172.7.21.6:8443              Masq    1      0          0
  -> 172.7.22.7:8443              Masq    1      0          0
TCP  10.254.86.27:80 nq
  -> 172.7.21.3:80                Masq    1      0          0
  -> 172.7.22.8:80                Masq    1      0          0
TCP  10.254.86.27:8080 nq
  -> 172.7.21.3:8080              Masq    1      0          0
  -> 172.7.22.8:8080              Masq    1      0          0
TCP  10.254.168.167:80 nq
  -> 172.7.22.9:8082              Masq    1      0          0
TCP  10.254.236.2:80 nq
  -> 172.7.22.6:80                Masq    1      0          0
UDP  10.254.0.2:53 nq
  -> 172.7.21.5:53                Masq    1      0          0
```

