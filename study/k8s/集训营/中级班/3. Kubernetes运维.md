# **使用Prometheus全方位监控K8s**

```shell
为什么要监控？
1、对系统不间断的实时监控
2、实时反馈系统当前状态
3、历史数据追溯

保证业务持续性运行
```

## **Prometheus 是什么**

* Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目
* https://prometheus.io
* https://github.com/prometheus

## **Prometheus组成及架构**

![image-20200907100120257](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907100120257.png)

![image-20200909065847655](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200909065847655.png)

* Prometheus Server：收集指标和存储时间序列数据，并提供查询接口
* ClientLibrary：客户端库
* Push Gateway：短期存储指标数据。主要用于临时性的任务
* Exporters：采集已有的第三方服务监控指标并暴露metrics
* Alertmanager：告警
* Web UI：简单的Web控制台

## 数据模型

* Prometheus将所有数据存储为时间序列；具有相同度量名称以及标签属于同一个指标
* 每个时间序列都由**度量标准名称和一组键值对（也成为标签）**唯一标识
* **时间序列格式：**
  * `<metric name>{<label name>=<label value>, ...}`
  * 示例：api_http_requests_total{method="POST", handler="/messages"}

## 作业和实例

* 实例：可以抓取的目标称为实例（Instances）
* 作业：具有相同目标的实例集合称为作业（Job）

```yaml
# 监控一台机器，这台机器就成为实例
# 一组实例称为作业

scrape_configs:
 - job_name: 'prometheus'
   static_configs:
   - targets: ['localhost:9090']
 - job_name: 'node'
   static_configs:
 - targets: ['192.168.1.10:9090']
```

## **K8S监控指标及实现思路**

```shell
在K8s中我们应监控哪些指标？
1、Node节点资源利用率（硬件，系统）
2、Pod级监控（系统，应用）
3、K8s资源监控（应用）
4、业务层面监控

硬件、系统、应用、API监控、业务监控、流量分析
```

* **Kubernetes本身监控**
  * Node资源利用率
  * Node数量
  * Pods数量（Node）
  * 资源对象状态
* **Pod监控**
  * Pod数量（项目）
  * Pod状态
  * 容器资源利用率
  * 应用程序

## **Prometheus监控K8S架构**

![image-20200907100646187](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907100646187.png)

```shell
# 监控指标		具体实现						举例
Pod性能			 cAdvisor	 					容器CPU，内存利用率
Node性能		 node-exporter			节点CPU，内存利用率
K8S资源对象		kube-state-metrics Pod/Deployment/Service

Nginx <- expoter -> prometheus、Tomcat（agent）
https://prometheus.io/docs/instrumenting/exporters/
```

* 服务发现
  * https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config

## **在K8S平台部署Prometheus** + Grafana

![image-20200909072001160](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200909072001160.png)

```shell
部署prometheus前提：需要数据持久化存储（pv，pvc）

[root@k8s-master prometheus]# kubectl get sc
NAME                  PROVISIONER      AGE
managed-nfs-storage   fuseim.pri/ifs   34h
[root@k8s-master prometheus]# kubectl get pods | grep nfs-client
nfs-client-provisioner-6cc5fc86cf-5pnq9   1/1     Running     4          34h

# https://github.com/kubernetes/kubernetes/releases/tag/v1.16.15
# scp -r /Users/dingyuanjie/work/k8s/my/ root@192.168.0.41:/root/prometheus

# prometheus-statefulset.yaml 修改点
storageClassName: managed-nfs-storage

terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
        # 自定义告警规则    看第21视频的34分40秒处
				- name: prometheus-rules
          configMap:
            name: prometheus-rules

# prometheus-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    kubernetes.io/name: "Prometheus"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
      nodePort: 30090               # 修改点
  selector:
    k8s-app: prometheus
  type: NodePort                    # 修改点  

[root@k8s-master prometheus]# ls prometheus-* |xargs -i kubectl apply -f {}
[root@k8s-master prometheus]# kubectl get pods -n kube-system | grep prometheus
prometheus-0                         2/2     Running   0          76s
[root@k8s-master prometheus]# kubectl get svc -n kube-system | grep prometheus
prometheus   NodePort    10.97.66.16   <none>        9090:30090/TCP           67s
# 访问
http://192.168.0.42:30090
```

```yaml
# node-exporter-ds.yml
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs    # 修改点
          hostPath:       
            path: /
        - name: dev       # 修改点
          hostPath:
            path: /dev
```

```shell
[root@k8s-master prometheus]# kubectl apply -f node-exporter-ds.yml
[root@k8s-master prometheus]# kubectl apply -f node-exporter-service.yaml
[root@k8s-master prometheus]# kubectl get pods -n kube-system|grep node
node-exporter-b7ppj                  1/1     Running   0          3m11s
node-exporter-d7psl                  1/1     Running   0          3m11s
[root@k8s-master prometheus]# kubectl get ds -n kube-system
NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
node-exporter             2         2         2       2            2           <none>                        3m30s
[root@k8s-master prometheus]# vi kube-state-metrics-deployment.yaml
image: quay.io/coreos/kube-state-metrics:v1.3.0 改为
image: lizhenliang/kube-state-metrics:v1.8.0
[root@k8s-master prometheus]# ls kube-state-metrics-* | xargs -i kubectl apply -f {}
```

* 安装：https://grafana.com/grafana/download

  ```shell
  wget https://dl.grafana.com/oss/release/grafana-7.1.5-1.x86_64.rpm
  sudo yum install grafana-7.1.5-1.x86_64.rpm
  ```

  ```yaml
  # grafana.yaml
  
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: grafana
    template:
      metadata:
        labels:
          app: grafana
  		spec:
  		  containers:
  		  - name: grafana
  		    image: grafana/grafana
  		    ports:
  		      - containerPort: 3000
  		        protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
            - name: grafana-data
              mountPath: /var/lib/grafana
              subPath: grafana
        securityContext:
          fsGroup: 472
          runAsUser: 472
        volumes:
        - name: grafana-data
          persistentVolumeClaim:
            claimName: grafana
  
  ---
  
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    storageClassName: "managed-nfs-storage"
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  
  ---
  
  apiVersion: v1
  kind: Service
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    type: NodePort
    ports:
    - port: 80
      targetPort: 3000
      nodePort: 30007
    selector:
      app: grafana
  ```

* 仪表盘模板：https://grafana.com/grafana/dashboards

## **基于K8S服务发现的配置解析**

## **监控K8S集群中Pod、Node、资源对象**

### Pod

* kubelet的节点使用cAdvisor提供的metrics接口获取该节点所有Pod和容器相关的性能指标数据。
* 暴露API接口地址：https://NodeIP:10250/metrics/cadvisor

### Node

* 使用node_exporter收集器采集节点资源利用率。
* https://github.com/prometheus/node_exporter
* 使用文档：https://prometheus.io/docs/guides/node-exporter/ 

### 资源对象

* kube-state-metrics采集了k8s中各种资源对象的状态信息。
* https://github.com/kubernetes/kube-state-metrics

## **使用Grafana可视化展示Prometheus监控数据**

![image-20200907101257876](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101257876.png)

## 在K8S中部署**告警利器Alertmanager**

![image-20200907101332256](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101332256.png)

1. 部署Alertmanager

2. 配置Prometheus与Alertmanager通信

3. 配置告警
   1. prometheus指定rules目录
   2. configmap存储告警规则
   3. configmap挂载到容器rules目录
   4. 增加alertmanager告警配置

* 钉钉告警：https://github.com/timonwong/prometheus-webhook-dingtalk

## **Prometheus告警状态**

* Inactive：这里什么都没有发生
* Pending：已触发阈值，但未满足告警持续时间
* Firing：已触发阈值且满足告警持续时间。警报发送给接受者。
* Inactive —> Pending —> Firing —> Inactive

## **Prometheus告警收敛**

* **分组（group）：**将类似性质的警报分类为单个通知
* **抑制（Inhibition）：**当警报发出后，停止重复发送由此警报引发的其他警报
* **静默（Silences）：**是一种简单的特定时间静音提醒的机制

## **Prometheus一条告警怎么触发的？**

![image-20200907101714316](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101714316.png)

# **使用 ELK Stack 收集 Kubernetes 平台日志**

## **收集哪些日志**

* K8S系统的组件日志
* K8S Cluster里面部署的应用程序日志
  * 标准输出
  * 日志文件

## **ELK Stack日志方案**

![image-20200907101848164](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101848164.png)

## **容器中的日志怎么收集**

![image-20200907102047955](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907102047955.png)

### 方案一：**Node上部署一个日志收集程序**

* DaemonSet方式部署日志收集程序
* 对本节点/var/log/kubelet/pods和/var/lib/docker/containers/两个目录下的日志进行采集
* Pod中容器日志目录挂载到宿主机统一目录上
* 优点
  * 每个Node仅需部署一个日志收集程序，资源消耗少，对应用无侵入
* 缺点
  * 应用程序日志如果写到标准输出和标准错误输出，那就不支持多行日志

### **方案二：Pod中附加专用日志收集的容器**

* 每个运行应用程序的Pod中增加一个日志收集容器，使用emtyDir共享日志目录让日志收集程序读取到
* 优点：低耦合
* 缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本

```yaml
containers: 
 - name: web
   image: reg.example.com/project/web:1.1
   ports:
   - containerPort: 8080
   volumeMounts:
   - name: tomcat-catalina 
     mountPath: /usr/local/tomcat/logs 
     
 - name: filebeat
   image: filebeat:7.3.2
   args: [
    "-c", "/etc/filebeat.yml",
    "-e",
   ]
   volumeMounts:
   - name: filebeat-config
     mountPath: /etc/filebeat.yml
     subPath: filebeat.yml
   - name: tomcat-catalina 
     mountPath: /usr/local/tomcat/logs
     
 volumes:
 - name: tomcat-catalina
   emptyDir: {}
 - name: filebeat-config
   configMap:
    name: filebeat-config
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
 name: filebeat-config
 
data:
 filebeat.yml: |-
  filebeat.inputs: 
   - type: log
     paths:
      - /usr/local/tomcat/logs/catalina*.log 
     fields:
      app: tomcat
      type: project-catalina 
     fields_under_root: true
     multiline:
      pattern: '^\['
      negate: true
      match: after
     output.elasticsearch:
      hosts: ['elasticsearch:9200']
      index: "tomcat-catalina-%{+yyyy.MM.dd}"
```

### **方案三：应用程序直接推送日志**

* 超出Kubernetes范围
* 优点：无需额外收集工具
* 缺点：浸入应用，增加应用复杂度

## **K8S平台中应用日志收集**

* K8S组件日志收集
* Nginx应用日志收集
* Tomcat应用日志收集

# **基于 Kubernetes 构建企业 Jenkins 持续集成平台**

## **项目发布方案概述**

### 蓝绿发布

* 项目逻辑上分为AB组，在项目升级时，首先把A组从负载均衡中摘除，进行新版本的部署。B组仍然继续提供服务。A组升级完成上线，B组从负载均衡中摘除
* 特点：
  *  策略简单
  * 升级/回滚速度快
  * 用户无感知，平滑过渡
* 缺点：
  * 需要两倍以上服务器资源
  * 短时间内浪费一定资源成本

### 灰度发布

* 灰度发布：只升级部分服务，即让一部分用户继续用老版本，一部分用户开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来
* 特点：
  * 保证整体系统稳定性
  * 用户无感知，平滑过渡
* 缺点：
  * 自动化要求高

![image-20200907103457061](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103457061.png)

### 滚动发布

* 滚动发布：每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版升级新版本
* 特点：
  * 用户无感知，平滑过渡
* 缺点：
  * 部署周期长
  * 发布策略较复杂
  * 不易回滚
* **K8S默认发布策略**
  * 1个Deployment
  * 2个ReplicaSet

![image-20200907103656618](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103656618.png)

## **在K8S中实现灰度发布方案**

## **发布流程设计**

![image-20200907103725166](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103725166.png)

![image-20200907103752894](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103752894.png)

## **准备代码版本仓库Git和容器镜像仓库Harbor**

## **在Kubernetes中部署Jenkins**

![image-20200907103840372](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103840372.png)

* 参考：https://github.com/jenkinsci/kubernetes-plugin/tree/fc40c869edfd9e3904a9a56b0f80c5a25e988fa1/src/main/kubernetes

## **Jenkins在K8S中动态创建代理**

![image-20200907103926857](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103926857.png)

![image-20200907103959024](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103959024.png)

* Kubernetes插件：Jenkins在Kubernetes集群中运行动态代理
* 插件介绍：https://github.com/jenkinsci/kubernetes-plugin

![image-20200907104056060](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907104056060.png)

## **构建Jenkins-Slave镜像**

```dockerfile
FROM centos:7
LABEL maintainer lizhenliang
RUN yum install -y java-1.8.0-openjdk maven curl git libtool-ltdl-devel && \
 yum clean all && \
 rm -rf /var/cache/yum/* && \
 mkdir -p /usr/share/jenkins
COPY slave.jar /usr/share/jenkins/slave.jar 
COPY jenkins-slave /usr/bin/jenkins-slave
COPY settings.xml /etc/maven/settings.xml
RUN chmod +x /usr/bin/jenkins-slave
ENTRYPOINT ["jenkins-slave"]
```

* 参考：https://github.com/jenkinsci/docker-jnlp-slave

## **Jenkins Pipeline构建流水线发布**

*  Jenkins Pipeline是一套插件，支持在Jenkins中实现集成和持续交付管道；
* Pipeline通过特定语法对简单到复杂的传输管道进行建模；
  * 声明式：遵循与Groovy相同语法。pipeline { }
  * 脚本式：支持Groovy大部分功能，也是非常表达和灵活的工具。node { }
* Jenkins Pipeline的定义被写入一个文本文件，称为Jenkinsfile

![image-20200907104301023](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907104301023.png)

* 拉取代码 - 代码编译 - 单元测试 - 构建镜像 - 部署到K8S - 测试

## **Jenkins在Kubernetes中持续部署**

* Kubernetes Continuous Deploy插件：用于将资源配置部署到Kubernetes。

* 插件介绍：https://plugins.jenkins.io/kubernetes-cd

* 支持以下资源类型: 

  • Deployment

  • ReplicaSet

  • DaemonSet

  • StatefulSet

  • Pod

  • Job

  • Service

  • Ingress

  • Secret

# 小结

## 使用Jenkins的插件

* Git
* Kubernetes
* Pipeline
* Kubernetes Continuous Deploy

## CI/CD环境特点

* Slave弹性伸缩
* 基于镜像隔离构建环境
* 流水线发布，易维护

## Jenkins参数化构建可帮助你完成更复杂环境CI/CD