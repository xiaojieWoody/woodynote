# **使用Prometheus全方位监控K8s**

```shell
为什么要监控？
1、对系统不间断的实时监控
2、实时反馈系统当前状态
3、历史数据追溯

保证业务持续性运行
```

## **Prometheus 是什么**

* Prometheus（普罗米修斯）是一个最初在SoundCloud上构建的监控系统。自2012年成为社区开源项目，拥有非常活跃的开发人员和用户社区。为强调开源及独立维护，Prometheus于2016年加入云原生云计算基金会（CNCF），成为继Kubernetes之后的第二个托管项目
* https://prometheus.io
* https://github.com/prometheus

## **Prometheus组成及架构**

![image-20200907100120257](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907100120257.png)

![image-20200909065847655](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200909065847655.png)

* Prometheus Server：收集指标和存储时间序列数据，并提供查询接口
* ClientLibrary：客户端库
* Push Gateway：短期存储指标数据。主要用于临时性的任务
* Exporters：采集已有的第三方服务监控指标并暴露metrics
* Alertmanager：告警
* Web UI：简单的Web控制台

## 数据模型

* Prometheus将所有数据存储为时间序列；具有相同度量名称以及标签属于同一个指标
* 每个时间序列都由**度量标准名称和一组键值对（也成为标签）**唯一标识
* **时间序列格式：**
  * `<metric name>{<label name>=<label value>, ...}`
  * 示例：api_http_requests_total{method="POST", handler="/messages"}

## 作业和实例

* 实例：可以抓取的目标称为实例（Instances）
* 作业：具有相同目标的实例集合称为作业（Job）

```yaml
# 监控一台机器，这台机器就成为实例
# 一组实例称为作业

scrape_configs:
 - job_name: 'prometheus'
   static_configs:
   - targets: ['localhost:9090']
 - job_name: 'node'
   static_configs:
 - targets: ['192.168.1.10:9090']
```

## **K8S监控指标及实现思路**

```shell
在K8s中我们应监控哪些指标？
1、Node节点资源利用率（硬件，系统）
2、Pod级监控（系统，应用）
3、K8s资源监控（应用）
4、业务层面监控

硬件、系统、应用、API监控、业务监控、流量分析
```

* **Kubernetes本身监控**
  * Node资源利用率
  * Node数量
  * Pods数量（Node）
  * 资源对象状态
* **Pod监控**
  * Pod数量（项目）
  * Pod状态
  * 容器资源利用率
  * 应用程序

## **Prometheus监控K8S架构**

![image-20200907100646187](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907100646187.png)

```shell
# 监控指标		具体实现						举例
Pod性能			 cAdvisor	 					容器CPU，内存利用率
Node性能		 node-exporter			节点CPU，内存利用率
K8S资源对象		kube-state-metrics Pod/Deployment/Service

Nginx <- expoter -> prometheus、Tomcat（agent）
https://prometheus.io/docs/instrumenting/exporters/
```

* 服务发现
  * https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config

## **在K8S平台部署Prometheus** + Grafana

![image-20200909072001160](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200909072001160.png)

```shell
部署prometheus前提：需要数据持久化存储（pv，pvc）

[root@k8s-master prometheus]# kubectl get sc
NAME                  PROVISIONER      AGE
managed-nfs-storage   fuseim.pri/ifs   34h
[root@k8s-master prometheus]# kubectl get pods | grep nfs-client
nfs-client-provisioner-6cc5fc86cf-5pnq9   1/1     Running     4          34h

# https://github.com/kubernetes/kubernetes/releases/tag/v1.16.15
# scp -r /Users/dingyuanjie/work/k8s/my/ root@192.168.0.41:/root/prometheus

# prometheus-statefulset.yaml 修改点
storageClassName: managed-nfs-storage

volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: prometheus-data
              mountPath: /data
              subPath: ""
            - name: prometheus-rules         # 新增
              mountPath: /etc/config/rules

terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: prometheus-config
        # 自定义告警规则    看第21视频的34分40秒处
				- name: prometheus-rules
          configMap:
            name: prometheus-rules
            
            
# prometheus-configmap.yaml
data:
  prometheus.yml: |
    rule_files:                      # 新增
    - /etc/config/rules/*.rules
            

# prometheus-service.yaml
kind: Service
apiVersion: v1
metadata:
  name: prometheus
  namespace: kube-system
  labels:
    kubernetes.io/name: "Prometheus"
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
      nodePort: 30090               # 修改点
  selector:
    k8s-app: prometheus
  type: NodePort                    # 修改点  

[root@k8s-master prometheus]# ls prometheus-* |xargs -i kubectl apply -f {}
[root@k8s-master prometheus]# kubectl get pods -n kube-system | grep prometheus
prometheus-0                         2/2     Running   0          76s
[root@k8s-master prometheus]# kubectl get svc -n kube-system | grep prometheus
prometheus   NodePort    10.97.66.16   <none>        9090:30090/TCP           67s

[root@k8s-master prometheus]# kubectl get pods -n kube-system  -o wide
NAME                                 READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES
prometheus-0                         2/2     Running   0          11m     10.244.1.131   k8s-node2    <none>           <none>
# 访问,prometheus-0 pod在哪个节点，就用哪个节点的ip访问
http://192.168.0.42:30090

[root@k8s-master prometheus]# kubectl logs prometheus-0 -n kube-system
Error from server (BadRequest): a container name must be specified for pod prometheus-0, choose one of: [prometheus-server-configmap-reload prometheus-server] or one of the init containers: [init-chown-data]
[root@k8s-master prometheus]# kubectl logs prometheus-0 -c prometheus-server -n kube-system
```

```yaml
# node-exporter-ds.yml
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: rootfs    # 修改点
          hostPath:       
            path: /
        - name: dev       # 修改点
          hostPath:
            path: /dev
```

```shell
[root@k8s-master prometheus]# kubectl apply -f node-exporter-ds.yml
[root@k8s-master prometheus]# kubectl apply -f node-exporter-service.yaml
[root@k8s-master prometheus]# kubectl get pods -n kube-system|grep node
node-exporter-b7ppj                  1/1     Running   0          3m11s
node-exporter-d7psl                  1/1     Running   0          3m11s
[root@k8s-master prometheus]# kubectl get ds -n kube-system
NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
node-exporter             2         2         2       2            2           <none>                        3m30s
[root@k8s-master prometheus]# vi kube-state-metrics-deployment.yaml
image: quay.io/coreos/kube-state-metrics:v1.3.0 改为
image: lizhenliang/kube-state-metrics:v1.8.0
image: k8s.gcr.io/addon-resizer:1.8.7 改为
fengbb/addon-resizer
[root@k8s-master prometheus]# ls kube-state-metrics-* | xargs -i kubectl apply -f {}
```

* 安装：https://grafana.com/grafana/download

  ```shell
  # wget https://dl.grafana.com/oss/release/grafana-7.1.5-1.x86_64.rpm
  # sudo yum install grafana-7.1.5-1.x86_64.rpm
  ```

  ```yaml
  # grafana.yaml
  
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: grafana
    template:
      metadata:
        labels:
          app: grafana
  		spec:
  		  containers:
  		  - name: grafana
  		    image: grafana/grafana
  		    ports:
  		      - containerPort: 3000
  		        protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 256Mi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
            - name: grafana-data
              mountPath: /var/lib/grafana
              subPath: grafana
        securityContext:
          fsGroup: 472
          runAsUser: 472
        volumes:
        - name: grafana-data
          persistentVolumeClaim:
            claimName: grafana
  
  ---
  
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    storageClassName: "managed-nfs-storage"
    accessModes:
      - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  
  ---
  
  apiVersion: v1
  kind: Service
  metadata:
    name: grafana
    namespace: kube-system
  spec:
    type: NodePort
    ports:
    - port: 80
      targetPort: 3000
      nodePort: 30007
    selector:
      app: grafana
  ```

  ```shell
  [root@k8s-master prometheus]# kubectl apply -f grafana.yaml
  [root@k8s-master prometheus]# kubectl get svc -n kube-system
  NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
  grafana              NodePort    10.110.191.157   <none>        80:30007/TCP             8h
  kube-dns             ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   6d1h
  kube-state-metrics   ClusterIP   10.99.253.62     <none>        8080/TCP,8081/TCP        8h
  node-exporter        ClusterIP   None             <none>        9100/TCP                 3d3h
  prometheus           NodePort    10.102.188.88    <none>        9090:30090/TCP           9h
  # 访问，grafana在哪个节点，就用哪个节点的ip访问
  http://192.168.0.42:30007
  admin/admin
  admin/111111
  # 添加数据源 - prometheus - URL(prometheus页面访问url) 
  - http://192.168.0.42:30090 
  或者 http://10.102.188.88:9090 
  或者(推荐) http://prometheus-0.prometheus.kube-system:9090
  # 添加仪表盘
  ```

* 仪表盘模板：https://grafana.com/grafana/dashboards

  ```shell
  K8S工作节点监控-20191219.json
  K8S资源对象状态监控-20191219.json
  K8S集群资源监控-20191219.json
  # 导入Import
  
  # 服务发现怎么配置？
  # grafana是怎么展示的？
  # 图表没数据？
  1、时间没有同步
  2、数据库没有数据
  3、PromQL写的有问题（在prometheus的UI界面Graph中调试）
  
  relabel_configs租用？
  1、重新命名标签（action: replace）
  2、只保留指定的标签或者删除指定的标签（action: keep/drop）
  ```

## **基于K8S服务发现的配置解析**

## **监控K8S集群中Pod、Node、资源对象**

### Pod

* kubelet的节点使用cAdvisor提供的metrics接口获取该节点所有Pod和容器相关的性能指标数据。
* 暴露API接口地址：https://NodeIP:10250/metrics/cadvisor

### Node

* 使用node_exporter收集器采集节点资源利用率。
* https://github.com/prometheus/node_exporter
* 使用文档：https://prometheus.io/docs/guides/node-exporter/ 

### 资源对象

* kube-state-metrics采集了k8s中各种资源对象的状态信息。
* https://github.com/kubernetes/kube-state-metrics

## **使用Grafana可视化展示Prometheus监控数据**

![image-20200907101257876](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101257876.png)

## 在K8S中部署**告警利器Alertmanager**

![image-20200907101332256](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101332256.png)

1. 部署Alertmanager

   ```shell
   [root@k8s-master prometheus]# cat alertmanager-configmap.yaml
   global:
         resolve_timeout: 5m
         smtp_smarthost: 'smtp.163.com:25'
         smtp_from: 'baojingtongzhi@163.com'
         smtp_auth_username: 'baojingtongzhi@163.com'
         smtp_auth_password: 'liang12345678'
         
         ntmyuzqcphpbjgag
   		# 邮件接收者，可配置多个
       receivers:
       - name: default-receiver
         email_configs:
         - to: "zhenliang369@163.com"
         
   # 自己的
   data:
     alertmanager.yml: |
       global:
         resolve_timeout: 5m
         smtp_smarthost: 'smtp.qq.com:465'
         smtp_from: '1093520060@qq.com'
         smtp_auth_username: '1093520060@qq.com'
         smtp_auth_password: 'ntmyuzqcphpbjgag'
         smtp_require_tls: false
   ```

2. 配置Prometheus与Alertmanager通信

   ```shell
   # Prometheus UI - Status - Configuration
   [root@k8s-master prometheus]# vi prometheus-configmap.yaml
   # 添加
       alerting:
         alertmanagers:
         - static_configs:
             - targets: ["alertmanager:80"]          
   ```

3. 配置告警
   1. prometheus指定rules目录
   2. configmap存储告警规则
   3. configmap挂载到容器rules目录
   4. 增加alertmanager告警配置

   ```shell
   [root@k8s-master prometheus]# vi prometheus-rules.yaml
   ```

* 钉钉告警：https://github.com/timonwong/prometheus-webhook-dingtalk

```shell
prometheus-configmap.yaml
prometheus-rbac.yaml
prometheus-service.yaml
prometheus-statefulset.yaml
[root@k8s-master prometheus]# ls alertmanager-* | xargs -i kubectl apply -f {}
[root@k8s-master prometheus]# kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
alertmanager-7866dbb64c-j7fcr         2/2     Running   0          79s
[root@k8s-master prometheus]# kubectl get svc -n kube-system
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
alertmanager         NodePort    10.100.97.51     <none>        80:30093/TCP             112s

[root@k8s-master prometheus]# vi prometheus-rules.yaml

改为node_filesystem_size_bytes{fstype=~"ext4|xfs"} * 100) > 20
# 系统字段改名字了（Graph中验证），也要对应的改
100 - (node_filesystem_free_bytes{fstype=~"ext4|xfs"} /
          node_filesystem_size_bytes{fstype=~"ext4|xfs"} * 100) > 20
改为
100 - (node_filesystem_free{fstype=~"ext4|xfs"} /
          node_filesystem_size{fstype=~"ext4|xfs"} * 100) > 20
# UI - Graph中输入，验证是否有值
# UI - Alerts中查看是否有报警信息

[root@k8s-master prometheus]# kubectl apply -f prometheus-rules.yaml
# prometheus UI - Status - Rules 中修改值还没有reload，则手动reload
# 手动reload
[root@k8s-master prometheus]# kubectl get pods -n kube-system -o wide
prometheus-0                          2/2     Running   0          66m     10.244.1.156   k8s-node2    <none>           <none>
[root@k8s-master prometheus]# curl -X POST http://10.244.1.156:9090/-/reload
# 刷新，查看prometheus UI - Status - Rules

# 进入到容器内
[root@k8s-master prometheus]# kubectl exec -it prometheus-0 -c prometheus-server sh -n kube-system
/prometheus $ cat /etc/config/rules/node.rules
```

## **Prometheus告警状态**

```shell
# promethues UI - Alerts
```

* Inactive：这里什么都没有发生
* Pending：已触发阈值，但未满足告警持续时间
* Firing：已触发阈值且满足告警持续时间。警报发送给接受者。
* Inactive —> Pending —> Firing —> Inactive

## **Prometheus告警收敛**

* **分组（group）：**将类似性质的警报分类为单个通知
* **抑制（Inhibition）：**当警报发出后，停止重复发送由此警报引发的其他警报
* **静默（Silences）：**是一种简单的特定时间静音提醒的机制

## **Prometheus一条告警怎么触发的？**

![image-20200907101714316](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101714316.png)

```shell
1、采集时间
2、分组时间
3、for: 1m

钉钉 webhook
企业微信 webhook

多个组，收件人
https://prometheus.io/docs/alerting/configuration/
```

# **使用 ELK Stack 收集 Kubernetes 平台日志**

```shell
# 容器本身特性
容器密集，采集目标多
容器的弹性伸缩性

带来的新挑战？
1、如果日志还是放到容器内部，会随着容器删除而被删除
2、容器数量很多，按照传统的查看日志方式已变不太现实

本身特性：
1、容器日志输出控制台，本身Docker提供了一种日志采集能力。如果落地到本地文件，目前还没有一种好的采集方式
2、新扩容Pod属性信息（日志文件路径，日志源）可能发生变化

方案1：DaemonSet部署日志采集器
标准输出：
/var/lib/docker/containers/*/*-json.log
日志文件：
方式1：/var/lib/kubelet/pods/*/volumes/kubernetes.io~empty-dir/
方式2：开发根据容器名命名日志文件，并且部署时统一挂载到宿主机日志目录

方案2：sidecar部署一个专用日志采集容器
日志文件：通过emptydir共享日志目录，手动配置日志源

阿里云开源日志采集工具：log-pilot
```

## **收集哪些日志**

* K8S系统的组件日志
* K8S Cluster里面部署的应用程序日志
  * 标准输出
  
  * 日志文件
  
    ```shell
    # cat /var/lib/docker/containers/02b84493309303c41f554db81d9a8f79a617c70303c79966ce9191bc1c707028/02b84493309303c41f554db81d9a8f79a617c70303c79966ce9191bc1c707028-json.log
    ```

## **ELK Stack日志方案**

![image-20200907101848164](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907101848164.png)

## **容器中的日志怎么收集**

![image-20200907102047955](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907102047955.png)

### 方案一：**Node上部署一个日志收集程序**

* DaemonSet方式部署日志收集程序
* 对本节点/var/log/kubelet/pods和/var/lib/docker/containers/两个目录下的日志进行采集
* Pod中容器日志目录挂载到宿主机统一目录上
* 优点
  * 每个Node仅需部署一个日志收集程序，资源消耗少，对应用无侵入
* 缺点
  * 应用程序日志如果写到标准输出和标准错误输出，那就不支持多行日志

```shell
[root@k8s-master pods]# kubectl create deployment tomcat --image=nginx --dry-run -o yaml > tomcat.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: tomcat
  name: tomcat
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: tomcat
    spec:
      containers:
      - image: tomcat
        name: tomcat
        resources: {}
        volumeMounts:
          - name: logs
            mountPath: /usr/local/tomcat/logs
      volumes:
        - name: logs
          emptyDir:
[root@k8s-master 17]# kubectl get pods -o wide| grep tomcat
tomcat-7784d9cb4f-dk6nm                   1/1     Running     0          57s     10.244.2.149   k8s-node1   <none>           <none>
# node1
[root@k8s-node1 ~]# cd /var/lib/kubelet/pods/
[root@k8s-node1 pods]# docker ps 
# 获取 deee22a2-972c-4959-987f-3d82e372ac43
[root@k8s-node1 pods]# cd /var/lib/kubelet/pods/deee22a2-972c-4959-987f-3d82e372ac43/volumes/kubernetes.io~empty-dir/logs
[root@k8s-node1 logs]# ls
catalina.2020-09-13.log  host-manager.2020-09-13.log  localhost.2020-09-13.log  localhost_access_log.2020-09-13.txt  manager.2020-09-13.log
```

### ==方案二：Pod中附加专用日志收集的容器==

* 每个运行应用程序的Pod中增加一个日志收集容器，使用emtyDir共享日志目录让日志收集程序读取到
* 优点：低耦合
* 缺点：每个Pod启动一个日志收集代理，增加资源消耗，并增加运维维护成本

```yaml
containers: 
 - name: web
   image: reg.example.com/project/web:1.1
   ports:
   - containerPort: 8080
   volumeMounts:
   - name: tomcat-catalina 
     mountPath: /usr/local/tomcat/logs 
     
 - name: filebeat
   image: filebeat:7.3.2
   args: [
    "-c", "/etc/filebeat.yml",
    "-e",
   ]
   volumeMounts:
   - name: filebeat-config
     mountPath: /etc/filebeat.yml
     subPath: filebeat.yml
   - name: tomcat-catalina 
     mountPath: /usr/local/tomcat/logs
     
 volumes:
 - name: tomcat-catalina
   emptyDir: {}
 - name: filebeat-config
   configMap:
    name: filebeat-config
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
 name: filebeat-config
 
data:
 filebeat.yml: |-
  filebeat.inputs: 
   - type: log
     paths:
      - /usr/local/tomcat/logs/catalina*.log 
     fields:
      app: tomcat
      type: project-catalina 
     fields_under_root: true
     multiline:
      pattern: '^\['
      negate: true
      match: after
     output.elasticsearch:
      hosts: ['elasticsearch:9200']
      index: "tomcat-catalina-%{+yyyy.MM.dd}"
```

```shell
[root@k8s-master fek]# kubectl apply -f elasticsearch.yaml
[root@k8s-master fek]# kubectl apply -f kibana.yaml
[root@k8s-master fek]# kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
elasticsearch-0                       1/1     Running   0          12m
kibana-7c648d6b6b-bkpfj               1/1     Running   0          12m
[root@k8s-master fek]# kubectl get pvc -n kube-system
NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
alertmanager                         Bound    pvc-9e98fdd1-9d52-40f0-bc34-2909b4937f2b   2Gi        RWO            managed-nfs-storage   4h17m
elasticsearch-data-elasticsearch-0   Bound    pvc-79de777b-ce0b-4c97-b49a-2e9f026c8752   20Gi       RWO            managed-nfs-storage   20m
grafana                              Bound    pvc-ea714c53-6e1a-423e-ad68-9c8f9e8474d3   5Gi        RWX            managed-nfs-storage   27h
prometheus-data-prometheus-0         Bound    pvc-361d6448-144e-4bc6-b560-b957b329cc28   16Gi       RWO            managed-nfs-storage   4d3h
[root@k8s-master fek]# kubectl get pv -n kube-system
[root@k8s-master fek]# kubectl get ep -n kube-system
NAME                      ENDPOINTS                                                  AGE
elasticsearch             10.244.1.157:9200                                          15m
kibana                    10.244.1.158:5601                                          14m
[root@k8s-master fek]# kubectl get svc -n kube-system
NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
elasticsearch        ClusterIP   None             <none>        9200/TCP                 14m
kibana               NodePort    10.110.126.147   <none>        5601:30601/TCP           13m

# 访问
http://192.168.0.42:30601
```

```shell
# 采集标准输出
传统配置日志采集工具重要设置什么？
1、日志路径
2、写正则，格式化日志
3、日志源（Pod、命名空间，service，项目）

# filebeat-kubernetes.yaml
env:
        - name: ELASTICSEARCH_HOST
          value: elasticsearch-0.elasticsearch.kube-system          # 修改点
          
[root@k8s-master fek]# kubectl apply -f filebeat-kubernetes.yaml
[root@k8s-master fek]# kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
filebeat-n8wp8                        1/1     Running   0          4m25s
filebeat-pnrjs                        1/1     Running   0          4m25s
# Kibana UI - 管理 - 索引管理
# 创建索引模式：
## 第一步：filebeat-7.3.2-*
## 第二步：@timestamp
# Kibana UI - Discover
# node中 curl 一个nginx容器来产生一条日志
# Kibana UI - Discover - Filters - 612
```

![image-20200913134008320](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200913134008320.png)

```shell
# k8s-logs.yaml
output.elasticsearch:
      hosts: ['elasticsearch-0.elasticsearch.kube-system:9200']           # 修改
[root@k8s-master elk]# kubectl apply -f k8s-logs.yaml
[root@k8s-master elk]# kubectl get pods -n kube-system
NAME                                  READY   STATUS    RESTARTS   AGE
k8s-logs-87qms                        1/1     Running   0          16s
k8s-logs-qxpt5                        1/1     Running   0          16s
# Kibana UI - 管理 - 索引管理
# 创建索引模式：
## 第一步：k8s-module-*
## 第二步：@timestamp
# Kibana UI - Discover
```

![image-20200913135302837](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200913135302837.png)

```shell
# nginx-deployment.yaml
output.elasticsearch:
      hosts: ['elasticsearch-0.elasticsearch.kube-system:9200']      # 修改点
- name: "docker-regsitry-auth"
      containers:
      - name: nginx
        image: lizhenliang/nginx-php                                 # 修改点  
[root@k8s-master elk]# docker pull lizhenliang/nginx-php
[root@k8s-master elk]# kubectl create ns test
[root@k8s-master elk]# kubectl apply -f nginx-deployment.yaml
[root@k8s-master elk]# kubectl get pods -n test -o wide
[root@k8s-master elk]# curl 10.244.2.155/status.html
# Kibana UI - 管理 - 索引管理
# 创建索引模式：
## 第一步：nginx-access-*
## 第二步：@timestamp
# Kibana UI - Discover
```

![image-20200913140559372](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200913140559372.png)

```shell
# tomcat-deployment.yaml
output.elasticsearch:
      hosts: ['elasticsearch-0.elasticsearch.kube-system:9200']      # 修改点
imagePullSecrets:
      - name: "docker-regsitry-auth"
      containers:
      - image: 192.168.0.35/library/tomcat:v1                        # 修改点
[root@k8s-master elk]# kubectl apply -f tomcat-deployment.yaml
# pending 一般是资源不够导致，删除不用的pod
[root@k8s-master elk]# kubectl get pods -n test -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP             NODE        NOMINATED NODE   READINESS GATES
java-demo-6c459465d6-nvzhf   1/2     Running   0          17s   10.244.2.163   k8s-node1   <none>           <none>
java-demo-6c459465d6-pm4ms   1/2     Running   0          17s   10.244.2.164   k8s-node1   <none>           <none>
[root@k8s-master elk]# curl http://10.244.2.164:8080/test/status.html
# Kibana UI - 管理 - 索引管理
# 索引模式
# 创建索引模式：
## 第一步：tomcat-catalina-*
## 第二步：@timestamp
# Kibana UI - Discover
```

![image-20200913142614333](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200913142614333.png)

### 方案三：应用程序直接推送日志

* 超出Kubernetes范围
* 优点：无需额外收集工具
* 缺点：浸入应用，增加应用复杂度

## **K8S平台中应用日志收集**

* K8S组件日志收集
* Nginx应用日志收集
* Tomcat应用日志收集

# **基于 Kubernetes 构建企业 Jenkins 持续集成平台**

## **项目发布方案概述**

### 蓝绿发布

* 项目逻辑上分为AB组，在项目升级时，首先把A组从负载均衡中摘除，进行新版本的部署。B组仍然继续提供服务。A组升级完成上线，B组从负载均衡中摘除
* 特点：
  *  策略简单
  * 升级/回滚速度快
  * 用户无感知，平滑过渡
* 缺点：
  * 需要两倍以上服务器资源
  * 短时间内浪费一定资源成本

### 灰度发布

* 灰度发布：只升级部分服务，即让一部分用户继续用老版本，一部分用户开始用新版本，如果用户对新版本没有什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来
* 特点：
  * 保证整体系统稳定性
  * 用户无感知，平滑过渡
* 缺点：
  * 自动化要求高

![image-20200907103457061](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103457061.png)

### 滚动发布

* 滚动发布：每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版升级新版本
* 特点：
  * 用户无感知，平滑过渡
* 缺点：
  * 部署周期长
  * 发布策略较复杂
  * 不易回滚
* **K8S默认发布策略**
  * 1个Deployment
  * 2个ReplicaSet

![image-20200907103656618](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103656618.png)

## **在K8S中实现灰度发布方案**

```shell
项目文档：
1、项目背景
2、业务现状
3、项目实施（测试环境，迁移到线上）
4、应急预案
```

## **发布流程设计**

![image-20200907103725166](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103725166.png)

![image-20200907103752894](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103752894.png)

## **准备代码版本仓库Git和容器镜像仓库Harbor**

## **在Kubernetes中部署Jenkins**

![image-20200907103840372](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103840372.png)

* 参考：https://github.com/jenkinsci/kubernetes-plugin/tree/fc40c869edfd9e3904a9a56b0f80c5a25e988fa1/src/main/kubernetes

```shell
[root@k8s-master jenkins]# kubectl get sc
NAME                  PROVISIONER      AGE
managed-nfs-storage   fuseim.pri/ifs   5d18h
[root@k8s-master jenkins]# ls
deployment.yml  ingress.yml  rbac.yml  service-account.yml  service.yml
[root@k8s-master jenkins]# kubectl apply -f .
[root@k8s-master jenkins]# kubectl get pods -o wide
NAME                      READY   STATUS    RESTARTS   AGE     IP             NODE        NOMINATED NODE   READINESS GATES
jenkins-fcc9d45fc-rmww7   0/1     Running   0          8m16s   10.244.1.226   k8s-node2   <none>           <none>
[root@k8s-master jenkins]# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
etcd         ClusterIP   None          <none>        80/TCP                         5d22h
jenkins      NodePort    10.104.78.9   <none>        80:30006/TCP,50000:31678/TCP   9m42s
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP                        7d4h
[root@k8s-master jenkins]# kubectl logs jenkins-fcc9d45fc-rmww7
# 访问
http://192.168.0.43:30006

# 配置国内源
[root@k8s-master var]# kubectl exec -it jenkins-fcc9d45fc-rmww7 bash
jenkins@jenkins-fcc9d45fc-rmww7:/$ cd /var/jenkins_home/updates/
jenkins@jenkins-fcc9d45fc-rmww7:~/updates$ ls
default.json  hudson.tasks.Maven.MavenInstaller
jenkins@jenkins-fcc9d45fc-rmww7:~/updates$ sed -i 's/http:\/\/updates.jenkins-ci.org\/download/https:\/\/mirrors.tuna.tsinghua.edu.cn\/jenkins/g' default.json && \
> sed -i 's/http:\/\/www.google.com/https:\/\/www.baidu.com/g' default.json

# 删除jenkins pod，新建新的pod

# 安装git、pipeline、kubernetes、Kubernetes Continuous Deploy、Git Parameter插件
```

## **Jenkins在K8S中动态创建代理**

![image-20200907103926857](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103926857.png)

![image-20200907103959024](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907103959024.png)

* Kubernetes插件：Jenkins在Kubernetes集群中运行动态代理
* 插件介绍：https://github.com/jenkinsci/kubernetes-plugin

![image-20200907104056060](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907104056060.png)

```shell
# Manage Jenkins - Configure System - Cloud - Kubernetes
名称:kubernetes
Kubernetes 地址(kubernets的svc name):https://kubernetes
Jenkins 地址（jenkins的svc name）：http://jenkins.default
```

```shell
# jenkins创建的job是要发布一个java项目？
1. jdk
2. maven
3. git
4. tomcat

job:拉取代码(git)、代码编译(jdk、maven)、构建镜像(docker)
```

## **构建Jenkins-Slave镜像**

```dockerfile
FROM centos:7
LABEL maintainer lizhenliang
RUN yum install -y java-1.8.0-openjdk maven curl git libtool-ltdl-devel && \
 yum clean all && \
 rm -rf /var/cache/yum/* && \
 mkdir -p /usr/share/jenkins
COPY slave.jar /usr/share/jenkins/slave.jar 
COPY jenkins-slave /usr/bin/jenkins-slave
COPY settings.xml /etc/maven/settings.xml
RUN chmod +x /usr/bin/jenkins-slave
ENTRYPOINT ["jenkins-slave"]
```

* 参考：https://github.com/jenkinsci/docker-jnlp-slave

```shell
[root@k8s-master jenkins-slave]# ls
Dockerfile  jenkins-slave  settings.xml  slave.jar
[root@k8s-master jenkins-slave]# docker build . -t 192.168.0.43:81/library/jenkins-slave-jdk:1.8
[root@k8s-master jenkins-slave]# docker push 192.168.0.43:81/library/jenkins-slave-jdk:1.8
```

## **Jenkins Pipeline构建流水线发布**

*  Jenkins Pipeline是一套插件，支持在Jenkins中实现集成和持续交付管道；
* Pipeline通过特定语法对简单到复杂的传输管道进行建模；
  * 声明式：遵循与Groovy相同语法。pipeline { }
  * 脚本式：支持Groovy大部分功能，也是非常表达和灵活的工具。node { }
* Jenkins Pipeline的定义被写入一个文本文件，称为Jenkinsfile

![image-20200907104301023](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200907104301023.png)

* 拉取代码 - 代码编译 - 单元测试 - 构建镜像 - 部署到K8S - 测试

```shell
部署项目到k8s中参数化构建怎么配置？
1. 分支
2. 命名空间
3. Pod副本数

[root@k8s-master k8s-ci]# kubectl create secret --help
[root@k8s-master k8s-ci]# kubectl create secret docker-registry --help
[root@k8s-master k8s-ci]# kubectl create secret docker-registry registry-pull-secret --docker-username=admin --docker-password=Harbor12345 --docker-server=192.168.0.43:81
```

```shell
# 说明
分两次构建：
1. 先不部署到K8S平台（注释掉第4步），构建一次  
```

```shell
// 公共
def registry = "192.168.0.43:81"
// 项目
def project = "dev"
def app_name = "java-demo"
def image_name = "${registry}/${project}/${app_name}:${Branch}-${BUILD_NUMBER}"
// def git_address = "git@192.168.0.43:9999/root/java-demo.git"     会报错
def git_address = "http://192.168.0.43:9999/root/java-demo.git"
// 认证 
def secret_name = "registry-pull-secret"
// jenkins全局凭据中配置
def docker_registry_auth = "62cb6f3d-2add-458b-ac83-0ae1245cbca3"
def git_auth = "389826fd-4eef-4a7b-9240-33161e35992a"
def k8s_auth = "5b656cf8-4afc-4f44-b4ff-e67cc7495f34"

pipeline {
  agent {
    kubernetes {
        label "jenkins-slave"
        yaml """
kind: Pod
metadata:
  name: jenkins-slave
spec:
  containers:
  - name: jnlp
    image: "${registry}/library/jenkins-slave-jdk:1.8"
    imagePullPolicy: Always
    volumeMounts:
      - name: docker-cmd
        mountPath: /usr/bin/docker
      - name: docker-sock
        mountPath: /var/run/docker.sock
      - name: maven-cache
        mountPath: /root/.m2
  volumes:
    - name: docker-cmd
      hostPath:
        path: /usr/bin/docker
    - name: docker-sock
      hostPath:
        path: /var/run/docker.sock
    - name: maven-cache
      path: /tmp/m2
"""      
    }
  }
  // 会报错，还是在页面上配置
  // This project is parameterized - 
  // Git Parameter：Name-Branch、Description-要发布的分支、Parameter Type-BRANCH、defaultValue-master、branchFilter-.*、tagFilter-*、selectedValue-NONE, sortMode-NONE
  // Choice - ReplicaCount、Namespace
  //parameters {
  // gitParameter branch: '',branchFilter: '.*',defaultValue: 'master',description:'选择发布的分支',name: 'Branch',quickFilterEnabled: false, selectedValue: 'NONE', sortMode: 'NONE', tagFilter: '*',type: 'PT_BRANCH'
  // choice (choices: ['1', '3', '5', '7'], description: '副本数', name: 'ReplicaCount')
  // choice (choices: ['dev','test','prod'], description: '命名空间', name: 'Namespace')
 // }
  stages {
      // 第一步
      stage('拉取代码'){
        steps {
            checkout([$class: 'GitSCM', branches: [[name: '${Branch}']], userRemoteConfigs: [[credentialsId: "${git_auth}", url: "${git_address}"]]])
        }
      }
      // 第二步
      stage('代码编译'){
          steps {
              sh """
                 mvn clean package -Dmaven.test.skip=true
              """                 
          }
      }
      // 第三步
      stage('构建镜像'){
          steps {
             withCredentials([usernamePassword(credentialsId: "${docker_registry_auth}", passwordVariable: 'password', usernameVariable: 'username')]) {
            sh """
              echo '
                FROM lizhenliang/tomcat
                RUN rm -rf /usr/local/tomcat/webapps/*
                ADD target/*.war /usr/local/tomcat/webapps/ROOT.war
              ' > Dockerfile
              docker build -t ${image_name} .
              docker login -u ${username} -p '${password}' ${registry}
              docker push ${image_name}
            """
            }
          }
      }
      // 第一次先注释掉第四步
      // 第四步
      stage('部署到K8S平台'){
        steps {
          sh """
          ls
          pwd
          sed -i 's#\$IMAGE_NAME#${image_name}#' deploy.yaml
          sed -i 's#\$SECRET_NAME#${secret_name}#' deploy.yaml
          """
          kubernetesDeploy configs: 'deploy.yaml', kubeconfigId: "${k8s_auth}"
        }
      }
  }
}
```

![image-20200914075440327](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20200914075440327.png)

```shell
# 
[root@k8s-master k8s-ci]# kubectl create deployment java-demo --image=192.168.0.43:81/dev/java-demo:master-16 --dry-run -o yaml > deployment.yaml
[root@k8s-master k8s-ci]# kubectl get svc -o yaml
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2020-09-06T07:49:27Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "152"
    selfLink: /api/v1/namespaces/default/services/kubernetes
    uid: f5eca04f-7a5e-4c0c-9571-532e58634611
  spec:
    clusterIP: 10.96.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
    
# 最终   deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: java-demo
  name: java-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: java-demo
  template:
    metadata:
      labels:
        app: java-demo
    spec:
      containers:
      - image: 192.168.0.43:81/dev/java-demo:master-16
        name: java-demo

---

apiVersion: v1
kind: Service
metadata:
  name: kubernetes
  namespace: NS
  labels:
     app: java-demo
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: java-demo
```

```shell
1. 怎么让这个持续部署插件读取到yaml文件？
yaml进行与代码一起版本管理
# [root@k8s-master ~]# cat .kube/config
# Jenkins - 项目Pipeline - 最下面Pipeline Syntax链接 - 片段生成器 - Sample Step选择 KubernetesDeploy：Deploy to Kubernetes - Add Jenkins - Kind：Kubernetes Configuration（kubeconfig） - Kubeconfig 选择：Enter directly Content （将.kube/config内容复制进去）- Description：k8s-auth
# - Config Files : deploy.yaml - Generate Pipeline Script

kubernetesDeploy configs: 'deploy.yaml', kubeConfig: [path: ''], kubeconfigId: '5b656cf8-4afc-4f44-b4ff-e67cc7495f34', secretName: '', ssh: [sshCredentialsId: '*', sshServer: ''], textCredentials: [certificateAuthorityData: '', clientCertificateData: '', clientKeyData: '', serverUrl: 'https://']

# 简化，添加到  stage('部署到K8S平台')，kubeconfigId 设置成变量
 stage('部署到K8S平台'){
        steps {
          sh """
          ls
          pwd
          sed -i 's#IMAGE_NAME#${image_name}#' deploy.yaml
          sed -i 's#SECRET_NAME#${secret_name}#' deploy.yaml
          """
          kubernetesDeploy configs: 'deploy.yaml', kubeconfigId: "${k8s_auth}"
        }
      }
# gitlab项目pom.xml同级上新建deploy.yaml文件，内容为上步生成的deployment.yaml

2. 怎么与参数化构建结合
通过sed免交互修改yaml的值
# 修改gitlab上的deploy.yaml文件
metadata:
  labels:
    app: java-demo
  name: java-demo
  namespace: NS                        # 修改点
spec:
  replicas: RSCOUNT                    # 修改点
  selector:
    matchLabels:
      app: java-demo
  template:
    metadata:
      labels:
        app: java-demo
    spec:
      imagePullSecrets:        # 修改点
      - name: SECRET_NAME      # 修改点
      containers:
      - image: IMAGE_NAME       # 修改点
        name: java-demo
        
apiVersion: v1
kind: Service
metadata:
  name: kubernetes
  namespace: NS                  # 修改点
        
# Pipeline script
stage('部署到K8S平台'){
        steps {
          sh """
          ls
          pwd
          sed -i 's#IMAGE_NAME#${image_name}#' deploy.yaml
          sed -i 's#SECRET_NAME#${secret_name}#' deploy.yaml
          sed -i 's#RSCOUNT#${ReplicaCount}#' deploy.yaml
          sed -i 's#NS#${Namespace}#' deploy.yaml
          """
          kubernetesDeploy configs: 'deploy.yaml', kubeconfigId: "${k8s_auth}"
        }
      }
```

```shell
# 最终 Pipeline Script
// 公共
def registry = "192.168.0.43:81"
// 项目
def project = "dev"
def app_name = "java-demo"
def image_name = "${registry}/${project}/${app_name}:${Branch}-${BUILD_NUMBER}"
// def git_address = "git@192.168.0.43:9999/root/java-demo.git"
def git_address = "http://192.168.0.43:9999/root/java-demo.git"
// 认证 
def secret_name = "registry-pull-secret"
// jenkins全局凭据中配置
def docker_registry_auth = "62cb6f3d-2add-458b-ac83-0ae1245cbca3"
def git_auth = "389826fd-4eef-4a7b-9240-33161e35992a"
def k8s_auth = "5b656cf8-4afc-4f44-b4ff-e67cc7495f34"

pipeline {
  agent {
    kubernetes {
        label "jenkins-slave"
        yaml """
kind: Pod
metadata:
  name: jenkins-slave
spec:
  containers:
  - name: jnlp
    image: "${registry}/library/jenkins-slave-jdk:1.8"
    imagePullPolicy: Always
    volumeMounts:
      - name: docker-cmd
        mountPath: /usr/bin/docker
      - name: docker-sock
        mountPath: /var/run/docker.sock
      - name: maven-cache
        mountPath: /root/.m2
  volumes:
    - name: docker-cmd
      hostPath:
        path: /usr/bin/docker
    - name: docker-sock
      hostPath:
        path: /var/run/docker.sock
    - name: maven-cache
      path: /tmp/m2
"""      
    }
  }
  //parameters {
   //gitParameter branch: '', branchFilter: '.*', defaultValue: 'master', description:'选择发布的分支', name: 'Branch', quickFilterEnabled: false, selectedValue: 'NONE', sortMode: 'NONE', tagFilter: '*', type: 'PT_BRANCH'
    // gitParameter branch: '', branchFilter: '.*', defaultValue: 'master', description: '选择要发布的分支', name: 'Branch', quickFilterEnabled: false, selectedValue: 'NONE', sortMode: 'NONE', tagFilter: '*', type: 'PT_BRANCH'
  // choice (choices: ['1', '3', '5', '7'], description: '副本数', name: 'ReplicaCount')
  // choice (choices: ['dev','test','prod'], description: '命名空间', name: 'Namespace')
  //}
  stages {
      // 第一步
      stage('拉取代码'){
        steps {
            checkout([$class: 'GitSCM', branches: [[name: '${Branch}']], userRemoteConfigs: [[credentialsId: "${git_auth}", url: "${git_address}"]]])
        }
      }
      // 第二步
      stage('代码编译'){
          steps {
              sh """
                 mvn clean package -Dmaven.test.skip=true
              """                 
          }
      }
      // 第三步
      stage('构建镜像'){
          steps {
             withCredentials([usernamePassword(credentialsId: "${docker_registry_auth}", passwordVariable: 'password', usernameVariable: 'username')]) {
            sh """
              echo '
                FROM lizhenliang/tomcat
                RUN rm -rf /usr/local/tomcat/webapps/*
                ADD target/*.war /usr/local/tomcat/webapps/ROOT.war
              ' > Dockerfile
              docker build -t ${image_name} .
              docker login -u ${username} -p '${password}' ${registry}
              docker push ${image_name}
            """
            }
          }
      }
      // 第四步
      stage('部署到K8S平台'){
        steps {
          sh """
          ls
          pwd
          sed -i 's#IMAGE_NAME#${image_name}#' deploy.yaml
          sed -i 's#SECRET_NAME#${secret_name}#' deploy.yaml
          sed -i 's#RSCOUNT#${ReplicaCount}#' deploy.yaml
          sed -i 's#NS#${Namespace}#' deploy.yaml
          """
          kubernetesDeploy configs: 'deploy.yaml', kubeconfigId: "${k8s_auth}"
        }
      }
  }
}

# Jenkins Build with Parameters - ReplicaCount 3
[root@k8s-master ~]# kubectl create ns dev
namespace/dev created
[root@k8s-master ~]# kubectl create secret docker-registry registry-pull-secret --docker-username=admin --docker-password=Harbor12345 --docker-server=192.168.0.43:81 -n dev
secret/registry-pull-secret created

# Jenkins - Build
[root@k8s-master ~]# kubectl get pods,svc -n dev
NAME                             READY   STATUS         RESTARTS   AGE
pod/java-demo-59d497d9f6-bckpd   1/1     Running        0          31s
pod/java-demo-59d497d9f6-nfw5c   1/1     Running        0          31s
pod/java-demo-59d497d9f6-r97wp   0/1     ErrImagePull   0          31s

NAME                 TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   NodePort   10.101.166.187   <none>        80:30829/TCP   31s
# deploy.yaml的kind: Service 中没有添加selector导致
[root@k8s-master ~]# kubectl get ep -n dev
No resources found in dev namespace.
[root@k8s-master ~]# kubectl get ep -n dev
NAME         ENDPOINTS                                                     AGE
kubernetes   10.244.1.3:8080,10.244.1.4:8080,10.244.1.7:8080 + 1 more...   56s
# 浏览器访问
http://192.168.0.42:30829/
```

## **Jenkins在Kubernetes中持续部署**

* Kubernetes Continuous Deploy插件：用于将资源配置部署到Kubernetes。

* 插件介绍：https://plugins.jenkins.io/kubernetes-cd

* 支持以下资源类型: 

  • Deployment

  • ReplicaSet

  • DaemonSet

  • StatefulSet

  • Pod

  • Job

  • Service

  • Ingress

  • Secret

# 小结

## 使用Jenkins的插件

* Git
* Kubernetes
* Pipeline
* Kubernetes Continuous Deploy

## CI/CD环境特点

* Slave弹性伸缩
* 基于镜像隔离构建环境
* 流水线发布，易维护

## Jenkins参数化构建可帮助你完成更复杂环境CI/CD

1. 涉及的所有插件都要安装
2. pipeline修改变量值
3. 参数化构建的名称就是变量，可以在pipeline任意地方引用
4. 拉取代码这块是通过git参数化构建动态获取的分支名
5. 一些认证信息都是保存到了jenkins凭据里面，pipeline通过凭据ID获取
6. pipeline大多数代码片段都可以自动生成 pipeline-syntax
7. kubectl rollback deployment

