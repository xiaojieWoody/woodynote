## 深度学习范畴

* 机器学习是人工智能的一个分支，而深度学习（如AlphaGo）又是机器学习下的一个分支
* 深度学习典型算法
  * CNN：卷积神经网络
  * RNN-LSTM：循环神经网络-长短期记忆网络
  * GANs：生成对抗网络
  * RL：强化学习
* 深度学习 约等于 神经网络
  * 深度学习在传统神经网络基础上的升级

## 深度学习基本概念

* 深度学习类似一个水流系统

* 假设深度学习要处理的信息是“水流”，⽽处理数据的深度学习网络是一个由管道和阀⻔组成的巨⼤水管网络。网络的⼊口是若⼲管道开口，网络的出口也是若⼲管道开口。这个⽔管网络有许多层，每一层由许多个可以控制⽔流向与流量的调节阀。根据不同任务的需要，水管⽹络的层数、每层的调节阀数量可以有不同的变化组合。对复杂任务来说，调节阀的总数可以成千上万甚至更更多。水管网络中，每一层的每个调节阀都通过水管与下一层的所有调节阀连接起来，组成一个从前到后，逐层完全连通的水流系统
* 水管⽹络是一个训练好的深度学习模型
  * 调节⽔管网络里的每一个流量调节阀，来使管道⽹络符合要求
* 深度学习大致就是这么一个用人类的数学知识与计算机算法构建起来的整体架构，再结合尽可能多的训练数据以及计算机的大规模运算能力去调节内部参数，尽可能逼近问题⽬标的半理论、半经验的建模方式

* 与传统机器学习对比
  * 传统机器学习
    * 数据预处理-特征提取-选择分类器
  * 深度学习
    * 数据预处理-设计模型-训练
  * 相同点
    * 在数据准备和预处理方便比较相似
    * 都可能对数据进行一些操作：数据清洗、数据标签、归一化、去噪、降维
  * 区别
    * 传统机器学习的特征提取主要依赖人工，针对特定简单任务的时候人工提取特征会简单有效，但是并不能通用
    * 深度学习的特征提取并不依靠人工，而是机器自动提取的

## 深度学习的优缺点

* 优点
  * 学习能力强
  * 覆盖范围广、适应性好
  * 数据驱动、上限高
  * 可移植性好
* 缺点
  * 计算量大、便携性差
  * 硬件需求高
  * 模型设计复杂
  * 没有“人性”、容易存在偏见

## 典型深度学习算法

### 卷积神经网络-CNN

* 最擅长图片的处理。它受到人类视觉神经系统的启发
* ⼈类的视觉原理
  * 从原始信号摄⼊开始(瞳孔摄⼊像素 Pixels)，接着做初步处理 (大脑⽪层某些细胞发现边缘和⽅向)，然后抽象(⼤脑判定，眼前的物体的形状，是圆形的)，然后进一步抽象(大脑进一步判定该物体是只气球)
  * 对于不同的物体，人类视觉也是通过这样逐层分级，来进⾏认知的
    * 在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征(轮子、眼睛、躯⼲等)，到最上层，不同的高级特征最终组合成相应的图像，从而能够让⼈类准确的区分不同的物体
    * 可以模仿人类⼤脑的这个特点，构造多层的神经⽹络，较低层的识别初级的图像特征，若⼲底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类

#### 价值（解决的问题）

* 问题1：图像需要处理的数据量太大，导致成本很高，效率很低
  * 解决：能够将大数据量的图片有效的降维成⼩数据量(并不影响结果)
    * 「将复杂问题简化」，把⼤量参数降维成少量参数，再做处理
    * 在⼤部分场景下，降维并不会影响结果。⽐如**1000**像素的图⽚缩小成 **200**像素，并不影响⾁眼认出来图片中是一只猫还是⼀一只狗，机器也是如此
* 问题2：	图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高
  * 解决：能够保留图⽚的特征，类似⼈类的视觉原理
    * ⽤类似视觉的⽅式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像

#### 基本原理

* 卷积层
  * 负责提取图像中的局部特征
  * 可以理解为使用⼀个过滤器(卷积核)来过滤图像的各个⼩区域，从⽽得到这些小区域的特征值
  * 在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某 个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核
  * 卷积层的通过卷积核的过滤提取出图⽚中局部的特征，跟人类视觉的特征提取类似
* 池化层
  * 主要作⽤是把数据降维，可以有效的避免过拟合
  * 如原始图片是20×20的，对其进行下采样，采样窗口为 10×10，最终将其下采样成为一个2×2⼤小的特征图
  * 因为即使做完了卷积，图像仍然很大(因为卷积核⽐较小)，所以为了降低数据维度，就进行下采样
  * 池化层相⽐卷积层可以更有效的降低数据维度，这么做不但可以⼤大减少运算量， 还可以有效的避免过拟合
* 全连接层
  * 根据不同任务输出我们想要的结果（类似传统神经⽹络的部分）
  * 经过卷积层和池化层处理过的数据输⼊到全连接层，得到最终想要的结果
  * 经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下
*  CNN 并⾮只是上⾯提到的3层结构，⽽是多层结构，例如 LeNet-5 的结构：卷积层 **–** 池化层**-** 卷积层 **–** 池化层 **–** 卷积层 **–** 全连接层

#### 实际应用

* 图⽚分类、检索：图像搜索
* 目标定位检测：自动驾驶、安防、医疗
* 目标分割：美图秀秀、视频后期加工、图像生成
* 人脸识别：安防、金融、生活
* 骨骼识别：安防、电影、图像视频生成、游戏

### 循环神经网络-RNN

* 能有效的处理序列数据的算法
  * CNN 和普通的算法大部分都是输⼊和输出的⼀一对应，也就是一个输入得到⼀个输出。不同的输⼊之间没有联系
  * 序列列数据 **–** ⼀串相互依赖的数据流的场景就需要使⽤用 **RNN** 来解决了
    * ⽐如：⽂章里的文字内容、语⾳里的音频内容、股票价格⾛势

#### 基本原理

* RNN 跟传统神经⽹络最⼤的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中，一起训练

* 在序列中前面的输入也会影响到后面的输出，相当于有 了“记忆功能”。但是 RNN 存在严重的短期记忆问题，长期的数据影响很小(哪怕他是重要的信息)
  * RNN 有短期记忆问题，无法处理很长的输⼊序列
  * 训练 RNN 需要投⼊极大的成本

#### 优化算法

* RNN是一种死板的逻辑，越晚的输⼊影响越大，越早的输入影响越小，且无法改变这个逻辑
*  LSTM
  * 挑选重要信息保留，不重要的信息会选择“遗忘”

*  GRU
  * 是 LSTM 的一个变体
  * 保留了 LSTM 划重点，遗忘不重要信息的特点，在long-term 传播的时候也不会被丢失
  * 主要是在 LSTM 的模型上做了一些简化和调整，在训练数据集⽐较大的情况下可以节省很多时间

#### 典型应用

* 只要涉及到序列数据的处理问题，都可以使用到，NLP 就是一个典型的应⽤场景

* 文本生成：类似填空题，给出前后文，然后预测空格中的词是什么
* 语音识别：根据输⼊音频判断对应的⽂字是什么
* 机器翻译：翻译⼯作也是典型的序列问题，词的顺序直接影响了翻译的结果
* 生成图像描述：类似看图说话，给一张图，能够描述出图片中的内容。这个往是 RNN 和 CNN 的结合
* 视频标记：将视频分解为图片，然后⽤图像描述来描述图⽚内容

### 长短期记忆网络-LSTM

* 是一种特殊的 RNN，能够学习⻓期依赖性
* 被明确设计⽤来避免⻓期依赖性问题。长时间记住信息实际上是 LSTM 的默认⾏为，⽽不是需要努力学习的东⻄

### 生成对抗网络-GANs

* “道高一尺，魔高一丈”，互相提高，无监督算法
* 设计动机就是—自动化
* 训练集需要⼤量的⼈工标注数据，这个过程是成本很高且效率很低的。⽽⼈工判断⽣成结果的好坏也是如此，有成本高和效率低的问题。而 GANs 能自动完成这个过程，且不断的优化，这是一种效率⾮常高，且成本很低的方式

#### 基本原理

* 两个重要部分组成
  * ⽣成器**(Generator**):通过机器生成数据(大部分情况下是图像)，目的是“骗过”判别器
  * 判别器**(Discriminator**):判断这张图像是真实的还是机器⽣成的，目的是找出生成器做的“假数据”
* 过程
  * 第⼀阶段：固定「判别器**D**」，训练「生成器**G**」
  * 第二阶段：固定「生成器**G**」，训练「判别器**D**」
  * 循环阶段⼀和阶段⼆：通过不断的循环，「⽣成器G」和「判别器D」的能力都越来越强

#### 优缺点

* 优点
  * 能更好建模数据分布(图像更锐利、清晰)
  * 理论上，GANs 能训练任何一种⽣成器网络。其他的框架需要生成器网络有一些特定的函数形式，比如输出层是⾼斯的
  * 无需利用⻢尔科夫链反复采样，无需在学习过程中进行推断，没有复杂的变分下界，避开近似计算棘⼿的概率的难题
* 缺点
  * 难训练，不稳定。⽣成器和判别器之间需要很好的同步，但是在实际训练中很容易D收敛，G发散。D/G 的训练需要精心的设计
  * 模式缺失(Mode Collapse)问题。GANs的学习过程可能出现模式缺失，⽣成器开始退化，总是⽣成同样的样本点，⽆法继续学习

#### 实际应用

* 生成图像数据集
  * ⼈⼯智能的训练是需要⼤量的数据集的，如果全部靠⼈工收集和标注，成本是很高的。 GANs 可以⾃动的生成一些数据集，提供低成本的训练数据
* 生成人脸图片
* 生成照片、漫画人物
* 图像到图像的转换
  * 把一种形式的图像转换成另外⼀种形式的图像，就好像加滤镜⼀样神奇
  * 把草稿转换成照⽚、把照⽚转换成油画
* 文字到图像的转换
* 语意 **–** 图像 **–** 照⽚ 的转换
* 自动生成模特
* 照片到Emojis
* 照⽚编辑
  * 使用GANs可以生成特定的照片，例如更换头发颜色、更改面部表情、甚至是改变性别
* 预测不同年龄的长相
* 提⾼照⽚分辨率，让照片更清晰
* 照片修复
* 自动生成**3D**模型
  * 给出多个不同⻆度的2D图像，就可以生成一个3D模型

### 深度强化学习-RL

* “绩效奖励”，以游戏为例，如果在游戏中采取某种策略可以取得较高的得分，那么就进一步「强化」这种策略，以期继续取得较好的结果
* 强化学习并不是某一种特定的算法，而是一类算法的统称
  * 强化学习和监督学习、⽆监督学习最大的不同就是不需要大量的“数据喂养”。 而是通过⾃己不停的尝试来学会某些技能
* 最⼤的应用场景就是游戏

#### 主流算法

* 免模型学习(**Model-Free**) **vs** 有模型学习(**Model-Based**)
* 强化学习算法的2⼤分类，重要差异是：智能体是否能完整了解或学习到所在环境的模型
* 有模型学习(Model-Based)
  * 对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好
  * 纯规划
  *  **Expert Iteration**
* 免模型学习(Model-Free)
  * 放弃了模型学习，在效率上不如前者，但是这种⽅式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习⽅方法更受欢迎，得到更加⼴泛的开发和测试
  * 策略优化(**Policy Optimization**)
  *  **Q-Learning**

