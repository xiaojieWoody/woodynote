# 分布式系统

从Paxos到Zookeeper  分布式一致性原理与实践 [倪超著][电子工业出版社][2015.02]

## 简介

* 定义：==是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统==

* 什么是幂等？如何实现？

  * ==对同一个系统，使用同样的条件，一次请求和重复的多次请求对系统资源的影响是一致的==

  * 数据在网络通信情形下，由于网络延迟等不确定因素，客户端不知道请求状态的情况下，服务端需要保证数据的幂等

  * 解决

    1. ==数据库的唯一约束（唯一索引）==

       * 第一次请求，向数据库中插入一条数据，对某个关键的数据设置唯一约束，如果第二次请求过来，那么就没办法正常插入，会抛出重复异常，服务端可以捕获到这个异常进行相应处理，不处理接下来的逻辑

    2. token机制（客户端发送，放入redis）

    3. 状态机幂等

       * 确保对象能够随时知道自己所处的状态，作出在该状态下所能做出的响应

       * 适合在有状态机流转的情况下，比如订单的创建和付款，订单的付款肯定是在之前，这时可以通过在设计状态字段时，使用int类型，并且通过值类型的大小来做幂等，比如订单的创建为0，付款成功为100，付款失败为99。在做状态机更新时，就这可以这样控制：

         `update goods_order set status=#{status} where id=#{id} and status<#{status}`

## 特征

* 分布性
  * ==分布式系统中的多台计算机在空间上随意分布==
* 对等性
  * ==组成分布式系统的所有计算机节点都是对等的==
  * ==副本：分布式系统对数据和服务提供的一种冗余方式==
    * 数据副本：不同的节点上持久化同一份数据
    * 服务副本：多个节点提供同样的服务
* 并发性
  * ==多个节点可能会并发操作一些共享资源，如数据库或分布式存储==
* 无序性
  * ==空间上任意分布的进程之间通过交换信息来进行通信，缺乏全局的时钟序列控制，很难定义两个事件究竟谁先谁后==
* 故障总是会发生

## 问题

- ==通信异常==
  - 网络本身不可靠，消息丢失和消息延迟
- ==网络分区（脑裂）==
  - 由于网络异常导致分布式系统中只有部分节点之间能正常通信
- ==三态==
  - 分布式系统的每次请求与响应：成功、失败与超时
  - 网络异常可能导致超时现象：
    - 请求消息在发送过程中丢失
    - 响应消息发送过程中丢失
- 节点故障

## ACID

* 定义：
  * 访问并可能更新数据库中各种数据项的一个程序执行单元，一般是指数据库事务
* 特点：
  * 原子性（Atomicity）
    * 要么全部成功，要么全部失败
  * 一致性（Consistency）
    * 执行前后，数据库都必须处于一致性状态
  * 隔离性（Isolation）
    * 一个事务的执行不能被其他事务干扰
  * 持久性（Durability）
    * 一旦提交，数据状态的变更就是永久的

## CAP理论

* 单机数据库中很容易能够实现一套满足ACID特性的事务处理系统
* 分布式事务：是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于分布式系统的不同节点上。==一个分布式事务可以看作是由多个分布式的操作序列组成的==。

* 定义：
  * 一个分布式系统不可能同时满足一致性（Consistency）、可用性（Avaliability）和分区容错性（Partition tolerance）这三个基本需求，最多只能同时满足其中两项
    * ==一致性：数据在多个副本之间是否能够保持一致的特性==
      * 如果放弃C，放弃数据的强一致性，而保留数据的最终一致性，引入了一个时间窗口概念，具体多久能够达到数据一致取决于系统的设计
    * ==可用性：系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果==
      * 如果放弃A，受到影响的服务需要等待一定的时间，因此在等待期间系统无法对外提供正常服务，即不可用
    * ==分区容错性：分布式系统在遇到任何网络分区故障时，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障==
      * 如果放弃P，将所有的数据都放在一个分布式节点上，但是意味着放弃了系统的可扩展性

CA：没有扩展性，相当于单机

CP：强一致性，可能导致同步时间无限延长，不可用

AP：导致全局数据不一致性

## BASE理论

* 面向的是大型高可用可扩展的分布式系统
* ==基于CAP理论逐步演化而来，是对CAP中一致性和可用性权衡的结果，其核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性==
  * 基本可用（Basically Available）
    * ==分布式系统在出现不可预知故障的时候，允许损失部分可用性，如响应时间上的损失和功能上的损失==
  * 软状态（Soft state）
    * ==允许系统在不同节点之间进行数据同步的过程存在延时==
  * 最终一致性（Eventually consistent）
    * ==系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态==

## 一致性协议

* 协调者（TM）来统一调度所有分布式节点（AP）的执行逻辑，并最终决定这些节点（AP） 是否要把事务真正进行提交

* ==2PC二阶段提交协议==
  * ==将一个事务的处理过程分为了投票和执行两个阶段，其核心是对每个事务都采用先尝试后提交的处理方式，统一决定参与者事务的提交或回滚，可以看作是一个强一致性的算法==
  * 解决了分布式事务的原子性问题，保证了分布式事务的多个参与者要么都执行成功，要么都执行失败
  * ==阶段一：提交事务请求（投票阶段）==
    1. 协调者向所有参与者发送事务询问
    2. 各参与者执行事务操作
    3. 各参与者向协调者反馈事务询问的响应（yes或者no）
  * ==阶段二：执行事务提交==
    * 协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作，如果所有的参与者获得的反馈都是yes响应，那么会执行事务提交，如果任何一个参与者反馈No响应，那么中断事务
  * 优点：原理简单，实现方便
  * 缺点：
    1. ==同步阻塞：==
       * 各个参与者在等待其他参与者响应的过程中，将无法进行任何其他操作
    2. ==单点问题==
       * 协调者出现问题后，其他参与者将会一直处于锁定事务资源的状态中，而无法继续完成事务操作
    3. ==数据不一致==
       * 执行事务提交时，当协调者向所有参与者发送commit请求之后，协调者挂了，未发送完所有commit，那么只有收到commit请求的参与者会进行事务提交，导致出现数据不一致性
    4. ==太过保守==
       * 没有设计较为完善的容错机制，任意一个节点的失败都会导致整个事务的失败
* 3PC三阶段提交协议
  - 为解决2PC的缺点而设计的
    - 提交是“非阻塞”协议
    - ==在两阶段提交的第一阶段与第二阶段之间插入了PreCommit阶段，避免由于协调者发生故障而导致参与者无限期等待问题，可以重新协调过程或直接否决，不会因不一致决定而出现问题==
* Paxos算法
  * 少数服从多数的原则，同时支持分布式节点角色之间的轮换，极大地避免了分布式单点的出现，既解决了无限期等待问题，也解决了脑裂“问题

## 服务注册中心

* 需要一个能够动态注册和获取服务信息的地方，来统一管理服务名称和其对应的服务器列表信息，称之为服务配置中心。
* 服务提供者在启动时，将其提供的服务名称、服务器地址注册到服务配置中心，服务消费者通过服务配置中心来获得需要调用的服务的机器列表。
* 通过相应的负载均衡算法，选取其中一台服务器进行调用。当服务器宕机或者下线时，相应的机器需要能够动态地从服务配置中心里面移除，并通知相应的服务消费者，否则服务消费者就有可能因为调用到已经失效服务而发生错误，在这个过程中，服务消费者只有在第一次调用服务时需要查询服务配置中心，然后将查询到的信息缓存到本地，后面的调用直接使用本地缓存的服务地址列表信息，而不需要重新发起请求到服务配置中心去获取相应的服务地址列表，直到服务的地址列表有变更(机器上线或者下线)。
* 这种无中心化的结构解决了之前负载均衡设备所导致的单点故障问题，并且大大减轻了服务配置中心的压力

## 应用问题

* 高CPU高并发场景如何处理？
  * ==比较耗CPU的任务摆在这里，程序也无法提升性能了，访问的流量也很大，应当如何优化？==
    1. 使用缓存
    2. 用偏重CPU性能的机器
    3. 使用集群、负载均衡分担CPU压力
    4. 使用分布式限流控制CPU压力
    5. 使用消息队列降低瞬时流量压力
    6. 使用异步交互如WebSocket提升前端体验

# 分布式事务

* 主要是为了解决数据一致性（zk）
* 为什么会有分布式事务

  * 对数据库进行分表分库，达到分摊数据库压力以及减少数据库单表数据量的目的
* 分库分表以后带来的问题

  * 分库分表以后，一方面分担了单库带来的性能压力;另一方面，减少了单表的数据量
  * 在数据库分库分表之前，所有数据都在同一个库里面，可以通过事务操作就很容易达到数据一致性的目的
  * 但是在数据库做了拆分后。==原本单库的事务操作就变成了多库的事务操作，但是每个库的事务只有自己知道==，所以就造就了分布式事务的问题，其实也叫分布式数据一致性

* 分布式事务常用解决方法及原理
  * 经典的 **X/OpenDTP** 事务模型
    * 定义了规范和 API 接口，由各个厂商进行具体的实现
    * 这个标准提出了使用二阶段提交(2PC – Two-Phase-Commit)来保证分布式事务的完整性。
    * 后来 J2EE 也遵循了 X/OpenDTP 规范，设计并实现了 java 里的分布式事务编程接口规范-JTA
      * 基于 JTA 规范的第三方分布式事务框架有 Jotm 和 Atomikos
        * JOTM， 存在一个问题，在使用中不能自动 rollback，无论什么情况都 commit
        * Atomikos，与 JOTM 相比，其更加稳定
    * **X/OpenDTP** 角色
      * AP、TM、RM（资源管理器，一般是数据库，也可以是消息队列、文件系统）
      * 在分布式系统中，每个机器节点能知道自己事务操作成功或失败，但是无法知道其他节点的结果  
      * 因此当一个事务操作需要跨越多个分布式节点的时候，为了保持事务处理的 ACID 特性，就需要引入一个“协调者”(TM)来统一调度所有分布式节点AP的执行逻辑，TM 负责调度 AP 的行为，并最终决定这些 AP 是否要把事务真正进行提交到(RM)
  * ==业务接口整合，避免分布式事务==
* 是把一个业务流程中需要在一个事务里执行的多个相关业务接口包装整合到一个事务中，比如可以将A/B/C 整合为一个服务 D 来实现单一事务的业务流程服务

* ==基于消息队列的最终一致性==
  * 基于可靠消息中间件来实现异步的最终一致性
    * 通过异步消息执行方案的本质是，把两个事物转化成两个本地事务，然后依靠消息本身的可靠性，以及消息的重试机制达到最终一致性


# Zookeeper 

## 简介

* 定义：

  * 是Apache软件基金会的一个软件项目，为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册（==是一个开源的分布式协调服务==）

* 设计目标：

  * 是将那些复杂且容易出错的分布式一致性服务封装起来，提供一些简单易用的接口给用户使用
  * 基于ZAB算法的实现，该框架能够很好地保证分布式环境中数据的一致性，解决分布式一致性问题的利器
  * ==是一个典型的分布式数据一致性的解决方案，分布式应用程序可以基于它实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能==
    * ==Watcher事件注册与异步通知机制==
    * ==命名服务（树形结构）==
    * ==临时节点、顺序节点、唯一节点（同级目录节点唯一）==
  * zookeeper并不是用来存储数据的（1个节点1M大小），通过监控数据状态的变化，达到基于数据的集群管理

* 保证如下分布式一致性特性

  1. ==顺序一致性==
     * 从同一个客户端发起的事务请求，最终将会严格地按照其发起顺序被应用到zookeeper中去
  2. ==原子性==
     * 要么整个集群中所有机器都成功应用了某一个事务，要么都没有应用
  3. ==单一视图==
     * 无论客户端连接的是哪个Zookeeper服务器，其看到的服务端数据模型都是一致的
  4. ==可靠性==
     * 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非有另一个事务又对其进行了变更
  5. ==实时性==
     * Zookeeper仅仅保证在一定的时间段内，客户端最终一定能够从服务器上读取到最新的数据状态

* 安装

  * 下载3.4.10、解压
    * 单机
      *  conf 目录下的配置文件 copy 一份重命名为 zoo.cfg
      *  修改日志文件路径dataDir
    * 集群
      * 解压，copy 一份zoo.cfg
      * 修改配置文件
        * server.1=ip1:2888:3888
      * 创建 myid文件，内容为对应每台机器的 Server ID 数字

* 使用

  * 启动：`bin/zkServer.sh start`
  * 客户端和服务端建⽴连接的端口号：`2181`
  * 创建节点`create`： -s顺序节点 -e临时节点 默认持久节点
  * `ls`、`get`、`set`、`delete`
  * 客户端和服务端会话的建立是一个异步的过程
    * 当会话真正创建完毕后，Zookeeper服务端会向会话对应的客户端发送一个事件通知，以告知客户端，客户端只有在获取这个通知之后，才算真正建立了会话
  * 应用框架
    * zkClient
    * Curator

* 数据存储

  * Zookeeper将所有数据存储在内存中，数据模型是一颗树，由斜杆进行分割的路径，就是一个Znode，例如/foo/path1

  * ==每个ZNode上都会保存自己的数据内容和维护一个Stat的数据结构==，其中记录了这个ZNode的三个数据版本：

    1. version：当前ZNode的版本，变更次数

    2. cversion：当前ZNode子节点的版本

    3. aversion：当前ZNode的ACL版本

       Zookeeper采用ACL策略来进行权限控制：

       - CREATE：创建子节点的权限
        - READ：获取节点数据和子节点列表的权限

       * WRITE：更新节点数据的权限

       * DELETE：删除子节点的权限

       * ADMIN：设置节点ACL的权限

    4. pzxid：该节点的子节点列表最后一次被修改时的事务ID（只有子节点列表变更了才会变更pzid，子节点内容变更不会影响pzxid）

    5. czxid、mzxid、ctime、mtime、datalength、numChildren

* 节点特性

  * ==节点有序==
  * ==同级节点必须唯一==
  * ==持久节点==
  * ==临时节点：生命周期和客户端会话绑定，不能创建子节点==
  * 组合：
    1. 持久节点
    2. 持久顺序节点
    3. 临时节点
    4. 临时顺序节点
  * 会话：NOT_CONNECTED - > CONNECTING ->CONNECTED ->ClOSE
  * Zookeeper允许创建节点时为每个节点添加SEQUENTIAL属性，节点名后面会追加一个整型数字（是由父节点维护的递增数字）

* ACL

  * Access Control List权限控制机制来保障数据的安全

  * 权限模式(Scheme)：授权对象(ID)：权限(Permission)标识一个有效的ACL信息

  * 权限模式(Scheme)

    用来确定权限验证过程中使用的验证策略

    - IP模式：通过IP地址粒度来进行权限控制
    - Digest：最常用，类似于"username:password"形式的权限标识来进行权限配置，便于区分不同应用来进行权限控制
    - World：最开放，对所有用户开放，特殊Digest模式“world:anyone”
    - Super：超级用户

  * 授权对象ID

    指权限赋予的用户或一个指定实体，例如IP地址或是机器等。在不同的权限模式下，授权对象是不同的

    IP：通常是一个IP地址或IP段，如"192.168.0.110"或"192.168.0.1/24"

    Digest：自定义，例如“username:password"

    World：只有一个ID："anyone"

    Super：与Digest模式一致

  * 权限Permission

    通过权限验证后可以被允许执行的操作

    CREATE(C)、DELETE(D)、READ(R)、WRITE(W)、ADMIN(A)

* 日志

  * 事务日志 事务操作的日志记录 datadir DataLogDir
    * ==为每次事务操作记录到日志文件，这样就可以通过执行这些日志文件来恢复数据==
      * 日志文件记录zookeeper服务器上的每一次事务操作
      * 日志文件格式：log.ZXID，ZXID非常重要，它表示该文件起始的事务id
  * ==为加快ZooKeeper恢复的速度，ZooKeeper还提供了对树结构和session信息进行数据快照持久化的操作==
    - 数据快照用来记录zookeeper服务器上某一时刻的全量内存数据内容，并写入指定磁盘文件中
      - 数据快照文件格式：snapshot.ZXID，ZXID非常重要，ZooKeeper会根据ZXID来确定数据恢复的起始点
    - 镜像文件主要存储zookeeper的树结构和session信息
  * 快照日志  存储某一时刻的全量数据
  * 运行时日志 bin/zookeeper.out

## 设计猜想

* Zookeeper致力于提供一个高性能、高可用，且具有严格的顺序访问控制能力（主要是写操作的严格顺序性）的分布式协调服务

  * 高性能：能够应用于那些对系统吞吐有明确要求的大型分布式系统中
  * 高可用：使得分布式的单点问题得到很好的解决
  * 严格的顺序访问控制：使得客户端能够基于Zookeeper实现一些复杂的同步原语

* 简单的数据模型

  * Zookeeper使得分布式程序能够通过一个共享的树型结构的名字空间来进行相互协调

* 可以构建集群

  * 集群能够分担客户端的请求流量 

  * 高可用，防止单点故障，集群中的某一个节点宕机以后，不影响整个集群的数据和继续提供服务的可能性

  * leader选举

    * 基于 paxos 理论所衍生出来的ZAB 协议 

    * ==启动时的 leader 选举==

      每个节点启动的时候状态都是 LOOKING，处于观望状态，接下来就开始进行选主流程

      1. ==每个 Server 发出一个投票== 

         由于是初始情况，Server1 和 Server2 都会将自己作为 Leader 服务器来进行投票，每次投票会包含所推举的==服务器的 myid 和 ZXID、 epoch==，使用(myid, ZXID,epoch)来表示，此时 Server1 的投票为(1, 0)，Server2 的投票为(2, 0)，然后各自将这个投票发给集群中其他机器 

      2. ==接受来自各个服务器的投票==

         集群的每个服务器收到投票后，通过检查是否是本轮投票(epoch)、是否来自 LOOKING 状态的服务器来判断该投票的有效性

      3. ==处理投票==

         针对每一个投票，服务器都需要将别人的投票和自己的投票进行 PK，PK 规则如下 

         - ==优先检查 ZXID。ZXID 比较大的服务器优先作为Leader==
         - ==如果 ZXID 相同，那么就比较myid。myid 较大的服务器作为 Leader 服务器==
         - ==更新自己的投票信息为对比比自己（ZXID大或者myid大）节点的投票信息==
         - ZXID比较大或者myid比较大的节点再次向集群中所有机器发出上一次投票信息即可

      4. ==统计投票==

         每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息 

      5. ==改变服务器状态==

         一旦确定了 Leader，每个服务器就会更新自己的状态，如果是 Follower，那么就变更为 FOLLOWING，如果是 Leader，就变更为 LEADING 

    * ==leader 崩溃时的选举==

      服务器运行期间的 Leader 选举和启动时期的 Leader 选举基本过程是一致的

      1. 变更状态 

         Leader 挂后，余下的非 Observer服务器都会将自己的服务器状态变更为 LOOKING，然后开始进入 Leader 选举过程 

      2. 剩下的和启动时leader的选举相同

* 顺序访问

  * 对于来自客户端的每个更新请求，Zookeeper都会分配一个全局唯一的递增编号，这个编号反映了所有事务操作的先后顺序

* 高性能

  * Zookeeper将全量数据存储在内存中，并直接服务于客户端的所有非事务请求，因此它尤其适用于以读操作为主的应用场景

* 数据一致性

  * ==需要一个leader节点负责协调和数据同步操作，使集群中每个节点的数据保持一致==
  * 在分布式系统中，每个机器节点能知道自己事务操作是成功和失败， 但无法直接获取其他分布式节点的操作结果。所以当一个事务操作涉及到跨节点的时候，就需要用到分布式事务，分布式事务的数据一致性协议有 2PC 协议和 3PC 协议

## ZAB协议

* ZAB(Zookeeper Atomic Broadcast) 协议是为分布式协调服务 ZooKeeper 专门设计的一种==支持崩溃恢复的原子广播协议==

* 作为数据一致性的核心算法

* 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性

* ==核心：定义了对于那些会改变Zookeeper服务器数据状态的事务请求的处理方式==

* ==所有事务请求必须由Leader服务器来协调处理==

  * ==Leader服务器负责将一个客户端事务请求转换成一个事务提议Proposal，并将该提议Proposal分发给集群中所有的Follower服务器==
  * ==Leader服务器需等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那么Leader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个提议Proposal进行提交==

* 两种基本模式

  * ==崩溃恢复==

* 当整个集群在启动时，或者当 leader 节点出现网络中断、崩溃等情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader，当 leader 服务器选举出来后，并且集群中有过半的机器和该 leader 节点完成数据同步后，ZAB 协议就会退出恢复模式，整个集群就进入了消息广播模式

* 在 Leader 节点正常工作时，启动一台新的服务器加入到集群，那这个服务器会直接进入数据恢复模式，和leader 节点进行数据同步。同步完成后即可正常对外提供非事务请求的处理

* 为了使 leader 挂了后系统能正常工作，需要解决以下两个问题

  1. 已经被处理的消息不能丢失（已经被 leader 提交的事务Proposal 最终能被所有服务器都提交）

     * leader发出commint命令之前挂了；
       * leader发出commint命令之后，有些follower执行了，有些没收到commit就已经挂了；
       * 在 zab 协议下需要保证所有机器都要执行这个事务消息

  2. 被丢弃的消息不能再次出现（丢弃已经被跳过的事务 Proposal）

     * 当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal

     * 之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，与整个系统的状态是不一致的，需要将其删除

* 解决：

  1. 如果 leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群中所有机器最高编号(**ZXID 最大**)的事务Proposal，那么就可以保证这个新选举出来的 Leader 一定具有已经提交的提案
          * 因为所有提案被 COMMIT 之 前必须有超过半数的 followerACK，即必须有超过半数节点的服务器的事务日志上有该提案的 proposal，因此，只要有合法数量的节点正常工作，就必然有一个节点保存了所有被 COMMIT 消息的 proposal 状态 
         2. zxid 是 64 位，高 32 位是 epoch 编号，每经过一次 Leader 选举产生一个新的 leader，新的 leader 会将epoch 号+1，低 32 位是消息计数器，每接收到一条消息这个值+1，新 leader 选举后这个值重置为 0.
            * 老的 leader 挂了以后重启，它不会被选举为 leader，因此此时它的 zxid 肯定小于当前新的 leader。当老的leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的proposal 清除

* zxid，事务 id

  * 为了保证事务的顺序一致性，zookeeper 采用了递增的事务 id 号(zxid)来标识事务
    * 所有的提议(proposal)都在被提出的时候加上了 zxid

    * 在/tmp/zookeeper/VERSION-2 路径下会看到一个 currentEpoch 文件。文件中显示的是当前的 epoch

* ==消息广播==

  * ==消息广播过程使用的是一个原子广播协议，类似于一个二阶提交过程==
    * 针对客户端的事务请求，Leader服务器会为其生成对应的事务Proposal，并将其发送给集群中其余所有的机器，然后再分别收集各自的选票，最后进行事务提交
      1. leader 接收到消息请求后，将消息赋予一个全局唯一的64 位自增 id，叫:zxid，通过 zxid 的大小比较既可以实现因果有序这个特征 
      2. leader 为每个 follower 准备了一个 FIFO 队列(通过 TCP 协议来实现，以实现了全局有序这一个特点)将带有 zxid 的消息作为一个提案(proposal)分发给所有的 follower
      3. 当 follower 接收到 proposal，先把 proposal 写到磁盘， 写入成功以后再向 leader 回复一个 ack 
      4. 当 leader 接收到合法数量(超过半数节点)的 ACK 后， leader 就会向这些 follower 发送 commit 命令，同时会在本地执行该消息 
      5. 当 follower 收到消息的 commit 命令以后，会提交该消息

* ==数据同步==

  * Leader服务器会为每个Follower服务器都准备一个队列，并将那些没有被各Follower服务器同步的事务以Prosoal消息的形式逐个发送给Follower服务器，并在每一个Prosoal消息后面紧接着在发送一个Commit消息，以表示该事务已被提交。
    * 等到Follower服务器将所有其尚未同步的事务Prososal都从Leader服务器上同步过来并成功应用到本地数据库中后，Leader服务器就会将该Follower服务器加入到真正的可用Follower列表中，并开始之后的其他流程

## 集群

* 在 zookeeper 中，客户端会随机连接到 zookeeper 集群中的一个节点

* 如果是读请求，直接从当前节点读，如果是写请求，则转发给leader提交事务，然后leader会广播事务，只要有超过半数节点写入成功，那么写请求就会被提交(类 2PC 事务)

* ==Leader 角色==

  1. 事务请求的唯一调度和处理者，保证集群事物处理的顺序性
  2. 集群内部各服务器的调度者，负责与所有的Follwer进行内部的数据交换

* ==Follower 角色==

  1. 处理客户端非事务请求、转发事务请求给leader服务器
  2. 参与事物请求 Proposal 的投票（过半写成功）
  3. 参与 Leader 选举的投票

* ==Observer 角色==

  * 观察 zookeeper 集群中的最新状态变化并将这些状态变化同步到 observer 服务器上
  * 和 follower 角色唯一的不同在于 observer 不参与任何形式的投票（事务请求和leader选举）
  * 只提供非事物请求服务，通常在于不影响集群事物处理能力的前提下提升集群非事物处理的能力

* 集群组成

  由 2n+1 台 server 组成（考虑投票，半数同意），每个 server 都知道彼此的存在

  只要有 n+1 台(大多数)server 可用，整个系统保持可用

## 应用场景

* 数据发布/订阅
  * 即配置中心
  * ==发布者将数据发布到Zookeeper的一个或一系列节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新==
  * Zookeeper采用的是推拉相结合的方式：客户端向服务端注册自己需要关注的节点，一旦该节点的数据发生变更，那么服务端就会向相应的客户端发送Watcher事件通知，客户端接收到这个消息通知后，需要主动到服务端获取最新的数据
  * 例如数据库切换应用场景，利用Zookeeper来实现配置管理
* 负载均衡
  * ==每台工作服务器在启动时都会去zookeeper的servers节点下注册临时节点，每台客户端在启动时都会去servers节点下取得所有可用的工作服务器列表，并通过一定的负载均衡算法计算得出一台工作服务器，并与之建立网络连接==
  * 优化资源使用、最大化吞吐率、最小化响应时间和避免过载
  * 请求/数据分摊多个计算机单元上
* 命名服务
  * 较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据指定名字来获取资源的实体、服务地址和提供者的信息等
  * ==能够帮助应用系统通过一个资源引用的方式来实现对资源的定位与使用==
  * 在分布式环境中，上层应用仅仅需要一个全局唯一的名字，类似于数据库中的唯一主键
* ==分布式协调/通知==
  * Watcher注册与异步通知机制
  * 不同的客户端都对Zookeeper上同一个数据节点进行Watcher注册，监听数据节点的变化（包括数据节点本身及其子节点）
* 集群管理
  * 对在Zookeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么临时节点也会被自动清除
  * 客户端如果对Zookeeper的一个数据节点注册Watcher监听，那么当该数据节点的内容或是其子节点列表发生变更时，Zookeeper服务器就会向订阅的客户端发送变更通知
* Master选举
  * ==利用Zookeeper的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即Zookeeper将会保证客户端无法重复创建一个已经存在的数据节点==
  * 创建一个日期节点，如2018-12-20，客户端集群每天都会定时往Zookeeper上创建一个临时节点，例如/master_election/2018-12-20/binding。在这个过程中，只有一个客户端能够成功创建这个节点，那么这个客户端所在的机器就成为了Master，同时，其他客户端都会在节点/master_election/2018-12-20上注册一个子节点变更的Watcher用于监控当前的Master机器是否存活
* 分布式锁
  * 控制分布式系统之间同步访问共享资源的一个钟方式，通过互斥手段来防止彼此之间的干扰，以保证一致性
  * ==zookeeper因为临时节点的特性，如果因为其他客户端因为异常和zookeeper连接中断了，那么节点会被删除，意味着锁会被自动释放==
  * 释放锁操作，会有watch通知机制，也就是服务器端会主动发送消息给客户端这个锁已经被释放了
* 分布式队列
  * 先入先出队列FIFO
    * 所有客户端都会到/queue_fifo节点下面创建一个临时顺序节点
      1. 通过调用getChildren()获取/queue_fifo节点下的所有子节点，即获取队列中所有的元素
      2. 确定自己的节点序号在所有子节点中的顺序
      3. 如果自己不是序号最小的子节点，那么就需要进入等待，同时向比自己序号小的最后一个节点注册Watcher监听
      4. 接收到Watcher通知后，重复步骤1

## Watcher事件机制

* ==Zookeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，Zookeeper服务端会将事件通知到感兴趣的客户端上去==

* 基于 zookeeper 上创建的节点，可以对这些节点绑定监听事件，比如可以监听节点数据变更、节点删除、子节点状态变更等事件，通过这个事件机制，可以基于 zookeeper实现分布式锁、集群管理等功能

* ==Zookeeper的Watcher机制主要包括：客户端线程、客户端WatcherManager和Zookeeper服务器三部分==。

* ==客户端在向Zookeeper服务器注册Watcher同时，会将Watcher对象存储在客户端的WatcherManager中。当Zookeeper服务器端触发Watcher事件后，会向客户端发送通知，客户端线程从WatcherManager中取出对应的Watcher对象来执行回调逻辑process()==

* 事件的实现原理

  * 客户端注册Watcher
    * 构造方法参数、==getData、getChildren和exist三个接口向Zookeeper服务器注册Watcher==
  * 服务端处理 Watcher
    * 使用TCP连接来向客户端发送一个WatcherEvent事件
  * 客户端回调Watcher

* Watcher接口

  * 接口类Watcher用于表示一个标准的事件处理器，其定义了事件通知相关的逻辑，包含KeeperState和EventType两个枚举类，分别代表了通知状态和事件类型，同时定义了事件的回调方法：process(WatchedEvent event)
  * ==WatchedEvent：通知状态(keeperState)、事件类型（eventType）和节点路径（path）==
  * Zookeeper使用WatchedEvent对象来封装服务端事件并传递给Watcher，从而方便回调方法process对服务端事件进行处理
  * WatchedEvent和WatcherEvent：
    * 都是对一个服务端事件的封装
    * 不同的是：
      * WatchedEvent是一个逻辑事件，用于服务端和客户端程序执行过程中所需的逻辑对象
      * WatcherEvent因为实现了序列化接口，可以用于网络传输
  * 服务端在生成WatchedEvent事件之后，会调用getWrapper方法将自己包装成一个可序列化的WatcherEvent事件，以便通过网络传输到客户端。
  * 客户端在接收到服务端的这个事件对象之后，首先会将WatcherEvent事件还原成一个WatchedEvent事件，并传递给process方法处理，回调方法process根据入参就能够解析出完整的服务端事件了
  * 例如客户端收到如下消息：
    * KeeperState：SyncConnected
    * EventType：NodeDataChanged
    * Path：/zk-book

* watcher 特性

  * 当数据发生变化的时候， zookeeper 会产生一个 watcher 事件，并且会发送到客户端。但是客户端只会收到一次通知。如果后续这个节点再次发生变化，那么之前设置 watcher 的客户端不会再次收到消息。(watcher 是一次性的操作)。 可以通过循环监听去达到永久监听效果
    - ==一次性==
      - 无论是客户端还是服务端，一旦一个Watcher被触发，Zookeeper都会将其从相应的存储中移除
      - 需要反复注册，有效地减轻了服务端的压力，否则针对那些更新非常频繁的节点，服务端会不断地向客户端发送事件通知，这无论对于网络还是服务端性能影响都非常大
    - ==客户端串行执行==
      - 客户端Watcher回调的过程是一个串行同步的过程，保证了顺序
    - ==轻量==
      - ==WatchedEvent是最小通知单元，只包含三部分内容（通知状态、事件类型和节点路径），只会告诉客户端发生了事件，具体内容需要客户端再次去服务端重新获取==
      - 客户端向服务端注册Watcher的时候，并不会把客户端真实的Watcher对象传递到服务端，仅仅只是在客户端请求中使用boolean类型属性进行标记，同时服务端也仅仅只是保存了当前连接的ServerCnxn对象（代表一个客户端和服务器的连接）

* ==如何注册事件机制==

  * 操作：getData、Exists、getChildren

* ==如何触发事件==

  * 凡是事务类型的操作，都会触发监听事件
  * create、delete、setData

* 事件类型

  * None (-1)，客户端链接状态发生变化的时候，会收到none的事件
    * SyncConnected（3）、DisConnected（0）、Expired（-112）、AuthFailed（4）
  * NodeCreated (1)，创建节点的事件。 比如zk-persis-mic
  * NodeDeleted (2)，删除节点的事件
  * NodeDataChanged (3)，节点数据发生变更
  * NodeChildrenChanged (4)，子节点被创建、被删除、会发生事件触发

* 什么样的操作会产生什么类型的事件

  |                                   | zk-persis-mic ( 监听事件）       | zk-persis-mic/child (监听事件) |
  | --------------------------------- | -------------------------------- | ------------------------------ |
  | create(/zk-persis-mic)            | NodeCreated(exists /getData))    | 无                             |
  | delete(/zk-persis-mic)            | NodeDeleted(exists /getData)     | 无                             |
  | setData(/zk-persis-mic)           | NodeDataChanged(exists /getData) | 无                             |
  | create ( /zk-persis-mic/children) | NodeChildrenChanged(getchild)    | NodedCreated                   |
  | delete ( /zk-persis-mic/children) | NodeChildrenChanged(getchild)    | NodedDeleted                   |
  | setData(/zk-persis-mic/children)  |                                  | NodeDataChanged                |

