# MS

* ==怎么删除表的前10000行==
  * 一个连接中循环执行20次`delete from T limit 500`
  * 直接删除10000的时间太长，锁的时间也比较长
* ==join==
  * 驱动表是全表扫描，被驱动表是走树搜索
  * 两个表按照各自的条件过滤之后，计算参与`join`的各个字段的总数据量，数据量小的那个表作为驱动表
  * `（inner ）join`：两个表交集
  * `left join`：返回左表所有记录，右表没有则对应null
  * `right join`：返回右表所有记录，左表没有则对应null
  * `full join`：返回两个表的全部记录
* ==`where`和`on`条件==
  * `where`条件中对左表限制
  * `on`条件对右表限制，对左表无影响

# 基本使用

## 查询语句

* ==查出平均分高于60的学生==
  * `select name from (select avg(price) as avgSco, name from books group by name) as avgTab where avgSco > 60`
* ==查询重复记录==
  * `select cardid from t_community_accesscard_info group by cardid having count(*)>1`

## JDBC

* `prepared.Statement`：预编译的statement对象，每次只改变SQL命令的参数，避免数据库每次都要编译SQL语句。`executeQuery` `executeUpdate` `execute` `setXxx`

  ```java
  //加载数据库驱动 
  Class.forName("com.mysql.jdbc.Driver"); 
  //通过DriverManager获取数据库连接 
  DriverManager.getConnection(url, username, password); 
  //创建Statement对象
  prepareStatement(String sql)
  //使⽤用Statement执行SQL语句
  executeQuery
  executeUpdate
  //操作结果集
  next
  getXxx()
  //回收数据库资源
  ```

## SQL

* 查询语句
  * 交叉连接：⼴义笛卡尔积，n x m条记录，没有where。cross join
  * ⾃然连接：会以两个表中同名列作为连接条件，无where natural join
  * using⼦句连接：显示指定两表中同名列连接
  * on子句连接：连接条件放在on子句中指定(只指定一个连接条件)
  * 左右全外连接：right / left / full join .. on ..
  * 子查询
    * `from`后当成数据表(当成视图)
    * `where` 后，作为过滤条件的值 `> in = any all not in`
  * 并
    * `union`，数据列的数量相等，数据类型⼀一对应，`select .. union select ..`
* `DML`：数据库操作语言，`insert、update、delete`
* `DDL`：数据库定义语言，操作数据库对象，`create、alter、drop、truncate`
* `DCL`：`数据控制语言，grant、revoke`
* 事务控制语句：`commit、rollback、savepoint`
* 数据库约束
  * ==强制执行的数据校验规则，用于保证数据库里数据的完整性==
  * `NOT NULL`（列）、`UNIQUE`（表、列）、`PRIMARY KEY`（表、列）、`FOREIGN KEY`、`CHECK`（MySQL不支持）
  * ⼀对⼀ 任意表加外键；⼀对多 多的一⽅外键列，从表；多对多 额外增加⼀个连接表
* 函数
  * `group by` 后常跟一个或多个列名，结果进行分组，当一列或多列组合的值完全相同时 => 当成一组
  * 不能在`where`(仅用于过滤行)子句中过滤组，使用`having`⼦句(用于`group by`)

# 原理

## 数据库设计

* 第一范式( 1NF)：==每一列只有一个单一的值，不可再拆分==
* 第二范式( 2NF)：==每一行都有主键能进行区分==
* 第三范式( 3NF)：==必须先满足第二范式，每一个表都不包含其他表已经包含的非主键信息==

## 事务提交与回滚

* 事务只会发生在增、删、改操作，没有查
* ==操作前会建立临时表，先把操作语句产生的记录放到临时表，然后扫描原始表中要修改的记录放到临时表中，记录进行对比：如果发生约束冲突，则直接返回给程序报错；如果没有发生冲突，则将原始表中的记录替换掉，把临时表干掉，再将最终结果返回给程序==

## MySQL服务器逻辑架构

* MySQL整理分为两块：
  * Server层，主要做MySQL功能层面的事情
  * 引擎层，负责存储相关的具体事宜


![image-20190519120734490](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20190519120734490.png)

## Redo Log

- ==RedoLog实现事务的持久性，是InnoDB引擎特有的日志==
  - 物理日志，记录的是"在某个数据页上做了什么修改"
  - 循环写，空间固定会用完
- ==Redo以恢复操作为目的，重现操作;==
  - ==当有一条记录需要更新时，InnoDB引擎就会先把记录写到redo log里，并更新内存==
  - ==InnoDB引擎在空闲时将这个操作记录更新到磁盘里面==
  - ==通过redo log，InnoDB可以保证即使数据库发生异常重启，之前提交的记录都不会丢失==
- Redo log的持久：
  - ==不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo 中。具体的落盘策略可以进行配置== 
- Redo Log实现事务持久性：
  - 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的未入磁盘数据进行持久化这一特性 

## Undo Log

- ==实现事务的原子性==
  - ==指事务开始之前，在操作任何数据之前,首先将需操作的数据备份到Undo Log==
- Undo Log实现事务原子性：
  - ==事务处理过程中如果出现了错误或者用户执行了 ROLLBACK语句，Mysql可以利用Undo Log中的备份将数据恢复到事务开始之前的状态==
- ==UndoLog在Mysql innodb存储引擎中用来实现多版本并发控制：==
  - ==事务未提交之前，Undo保存了未提交之前的版本数据，Undo 中的数据可作为数据旧版本快照供其他并发事务进行快照读==
    - 快照读
      - ==SQL读取的数据是快照版本，也就是历史版本，普通的SELECT就是快照读==
      - innodb快照读，数据的读取将由 cache(原本数据) + undo(事务修改过的数据) 两部分组成
      - ==通过MVCC解决幻读==
    - 当前读
      - ==SQL读取的数据是最新版本。通过锁机制来保证读取的数据无法通过其他事务进行修改==
      - ==INSERT、UPDATE、DELETE、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE都是当前读== 
      - ==通过next-key解决幻读==

## binlog（主从复制）

* Server层日志，binlog，所有引擎都有
  * 逻辑日志，记录的是这个语句的原始逻辑，比如"给ID=2这一行的c字段加1"
  * 追加写，不会覆盖以前日志

* ==二进制的日志文件，用来记录MySQL的数据更新或潜在更新的语句==
* ==主从复制就是依靠binlog，主数据库负责写入数据，从数据库负责读取数据减轻主数据库的读取压力==

1. ==master记录二进制日志==
   * ==在每个事务更新数据完成之前，master在二进制日志记录这些改变，然后master通知存储引擎提交事务== 
   * 存储的内容格式有三种：
     1. statement：基于SQL语句
     2. row：基于行模式`update table set value = x;` 10000条 记录10000条变更数据
     3. mixed：混合模式
2. ==slave通过工作I/O 线程在master上打开一个普通的连接，异步地将master的binary log拷贝到它自己的中继日志==
3. ==Slave的SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致==

## Update流程

* 执行器先找到引擎取ID=2这一行。如果数据页存在内存中，则直接返回，否则先从磁盘读取到内存再返回
* ==执行器执行语句，得到新的一行数据，再调用引擎接口写入这行新数据==
* ==引擎将这行新数据更新到内存，同时将这个更新操作记录到redo log里，此时redo log处于prepare状态，然后告知执行器执行完成了，随时可以提交事务==
* ==执行器生成这个操作的binlog，并把binlog写入磁盘==
* ==执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成commit状态，更新完成==
  * ==redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致==

## 并发控制

* 无论何时，只要有多个查询需要在同一时刻修改数据，都会产生并发控制的问题
* 在处理并发读或者写时，可以通过两种类型的锁来解决问题
  * 共享锁（读锁）
    * 可以在同一时刻同时读取同一个资源，互不干扰
  * 排他锁（写锁）
    * 一个写锁会阻塞其他的写锁和读锁

## 事务

* ==访问并可能修改数据库中数据项的执行单元==

  * 事务启动
    * 显示启动事务语句，begin或start transaction，提交commit，回滚rollback
    * set autocommit=0，线程的自动提交关掉，只执行一个select语句，事务就开启了，并不会自动提交，持续到主动执行commit或rollback，或者断开连接
    * 建议set autocommit=1，通过显示语句的方式来启动事务

* ==ACID特性==

  * 原子性（Atomicity）
    * 整个事务中的所有操作，要么全部提交成功，要么全部失败回滚
  * 一致性（Consistency）
    * 从一个一致性的状态转换到另外一个一致性的状态
  * 隔离性（Isolation）
    * 一个事务所做的修改在最终提交前，对其他事物是不可见的
  * 持久性
    * 一旦事务提交，则其所做的修改就会永久保存在数据库中

* 隔离级别：==在数据库事务中，保证并发数据读写的正确性==

  * `show variables like 'transaction_isolation';`

* `READ UNCIMMITED`（==未提交读==）

  * 事务中的修改，即使没有提交，对其他事务也都是可见的
  * 导致==脏读==：事务A读取事务B尚未提交的数据

* `READ COMMITTED`（==提交读==）

  * 导致==不可重复读==，因为两次执行同样的查询，可能得到不一样的结果，即==允许其他事务并发修改数据==
    * 事务A第一次读取最初数据，第二次读取事务B已经提交的修改或删除数据。导致两次读取数据不一致

* `REPEATABLE READ`（==可重复读==）

  * ==保证了在同一个事务中多次读取同样记录的结果是一致的，MySQL InnoDB引擎的默认隔离级别==
  * 导致幻读
  * ==幻读==：当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行
    * ==InnoDB通过多版本并发控制（MVCC）解决了幻读问题==

* `SERIALIZABLE`（==可串行化==）

  * ==强制事务串行执行，避免幻读问题==
    * 读取需要获取共享读锁，更新需要获取排他写锁，如果SQL使用WHERE语句，还会获取区间锁
      (MySQL以GAP锁形式实现，可重复读级别中默认也会使用)
    * 会在读取的每一行数据上都加锁，可能导致大量的超时和锁争用的问题

  ![image-20190414102025282](/Users/dingyuanjie/Documents/study/github/woodyprogram/img/image-20190414102025282-6310396.png)

* 隔离级别实现：锁、MVCC

  * innodb的临键锁解决幻读，加上Next-key锁
    * 利用锁解决脏读，数据更新时加上x锁
  * 利用锁解决不可重复读，第一次查询时加上S锁

* 死锁

  * 两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象
  * ==`InnoDB`目前处理死锁的方法是，将持有最少行级排他锁的事务进行回滚==
  * 避免：
    * 类似的业务逻辑以固定的顺序访问表和行
    * 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。

* 隐式和显式锁定

  * 锁：保证多个用户对数据库进行并发操作时，数据保持一致性的机制
  * `InnoDB`采用的是两阶段锁定协议
  * ==隐式锁定：`InnoDB`会根据隔离级别在需要的时候自动加锁==
    * 在事务执行过程中，随时都可以执行锁定，锁只有在执行`COMMIT`或者`ROLLBACK`的时候才会释放，并且所有的锁是在同一时刻被释放
  * ==显示锁定：`InnoDB`通过特定语句支持==
    * 共享锁，`SELECT … LOCK IN SHARE MODE`
    * 排他锁，`SELECT … FOR UPDATE`

## 锁

* 悲观锁

  * 在操作数据时，认为每次操作会出现数据冲突，所以在进行每次操作时都要通过获取锁才能进行对相同数据的操作。

  * ==悲观锁一般利用类似`SELECT ... FOR UPDATE`这样的语句，对数据加锁，避免其他事务意外修改数据==

  * ==主要依靠数据库提供的底层锁机制来实现，如行锁、表锁、读锁、写锁等==

  * ==共享锁（读锁），非独占的数据只读操作，不能修改，释放之前不能加排他锁==

    ```mysql
    # Transaction_A
    1. mysql> set autocommit=0;
    2. mysql> select * from innodb_lock where id=4 lock in share mode;
    3. mysql> update innodb_lock set v='4002' where id=4;
    
    # Transaction_B
    1. mysql> set autocommit=0;
    2. mysql> select * from innodb_lock where id=4 lock in share mode;
    3. mysql> update innodb_lock set v='4002' where id=4;
    -- A 在进行3操纵时会等待B释放共享锁，如果此时B值性3操作，则会发生死锁
    ```

  * ==排他锁（写锁），独占的、数据读写操作，释放之前不能加其他锁，如共享锁==

    ```mysql
    Transaction-A
    1. set autocommit=0;
    2. select * from innodb_lock where id = 4 for update;
    3. update innodb_lock set v = '4001' where id = 4;
    4. commit;
    
    Transaction-B
    1. select * from innodb_lock where id = 4 for update;
    -- 在Transaction-A的 2-4期间会阻塞等待
    ```

  * 意向锁共享锁(表锁):IntentionShared Locks

    - 一个数据行加共享锁前必须先取得该表的IS锁

  * 意向锁排它锁(表锁):Intention Exclusive Locks

    - 一个数据行加排他锁前必须先取得该表的IX锁
    - ==意向锁(IS、IX)是InnoDB数据操作之前自动加的，不需要用户干预==
    - 意义：==当事务想去进行锁表时，可以先判断意向锁是否存在，存在时则可快速返回该表不能启用表锁==

  ```java
  //更新库存(使用悲观锁)
  public boolean updateStock(Long productId){
      //先锁定商品库存记录
      ProductStock product = query("SELECT * FROM tb_product_stock WHERE product_id=#{productId} FOR UPDATE", productId);
      if (product.getNumber() > 0) {
          int updateCnt = update("UPDATE tb_product_stock SET number=number-1 WHERE product_id=#{productId}", productId);
          if(updateCnt > 0){    //更新库存成功
              return true;
          }
      }
      return false;
  }
  ```

* 乐观锁

  * ==假设每次操作不会发生冲突，也就不加锁，当提交更新时需要判断数据是否已经被修改==

  * ==乐观锁利用CAS机制，并不会对数据加锁，而是通过对比数据的时间戳或者版本号，来实现乐观锁需要的版本判断==

  * 思路一般是表中增加版本字段或时间戳，更新时where语句中增加版本的判断，如果提交的数据版本号加1后大于数据库表当前版本号，则予以更新，否则认为是过期数据，算是一种CAS（Compare And Swep）操作

    ```java
    //下单减库存
    public boolean updateStock(Long productId){
        int updateCnt = 0;
        while (updateCnt == 0) {
            ProductStock product = query("SELECT * FROM tb_product_stock WHERE product_id=#{productId}", productId);
            if (product.getNumber() > 0) {
                //product.getNumber()
                updateCnt = update("UPDATE tb_product_stock SET number=number-1 WHERE product_id=#{productId} AND number=#{number}", productId, product.getNumber());
                if(updateCnt > 0){    //更新库存成功
                    return true;
                }
            } else {    //卖完啦
                return false;
            }
        }
        return false;
    }
    ```

* 乐观锁和悲观锁的区别：

  - 悲观锁使用了排他锁，当程序独占锁时，其他程序就连查询都是不允许的，导致吞吐较低。如果在查询较多的情况下，可使用乐观锁。
  - 乐观锁更新有可能会失败，甚至是更新几次都失败，这是有风险的。所以如果写入较频繁，对吞吐要求不高，可使用悲观锁。

* 行锁

  * ==InnoDB的行锁是通过给索引上的索引项加锁来实现的，只有通过索引条件进行数据检索，InnoDB才使用行级锁，否则，InnoDB将使用表锁==

    * 锁住索引的所有记录。如果不通过索引去删除数据，那么会把整个表锁住

  * 劣势：==开销大；加锁慢；会出现死锁==

  * 优势：==锁的粒度小，发生锁冲突的概率低；处理并发的能力强==

  * 加锁的方式：

    - ==自动加锁==
      - 对于UPDATE、DELETE和INSERT语句，InnoDB会自动给涉及数据集加排他锁；
      - 对于普通SELECT语句，InnoDB不会加任何锁；
    - ==显示的加锁==
      - 共享锁：`select * from tableName where ...  lock in share more`
      - 排他锁：`select * from tableName where ...  for update `

  * InnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁

    ```mysql
    mysql> update innodb_lock set v='1002' where k=1;
    mysql> commit;
    mysql> create index idx_k on innodb_lock(k);
    ```

  * ==行锁算法==

    * ==记录锁== Record Locks

      * ==锁住具体的索引项==，`select * from t_users where id = 123 for update`
      * 当sql执行按照唯一性(Primary key、Unique key)索引进行数据的检索时，查询条件等值匹配且查询的数据是存在，这时SQL语句加上的锁即为记录锁Record locks

    * ==间隙锁== GAP锁

      * ==范围条件检索数据，锁住数据不存在的区间(左开右开)==

      * 对于键值在条件范围内但并不存在的记录，叫做"间隙(GAP)"

      * ==危害：若执行的条件是范围过大，则InnoDB会将整个范围内所有的索引键值全部锁定，很容易对性能造成影响==

    * ==临键锁== Next-key Locks
      * ==锁住记录+区间(左开右闭)==

      * ==innodb默认行锁算法，防止幻读，B+Tree有顺序，从小到大排列，把相邻空间锁住，下一个相邻空间就插不进数据==

      * ==当sql执行按照索引进行数据的检索时，查询条件为范围查找(between and、<、>等)并有数据命中则此时SQL语句加上的锁为Next-key locks，锁住索引的记录+区间(左开右闭)==

  * 分析行锁定

    - 通过检查InnoDB_row_lock 状态变量分析系统上的行锁的争夺情况 
      - `show status like 'innodb_row_lock%';`
      - innodb_row_lock_current_waits: 当前正在等待锁定的数量
      - innodb_row_lock_time: 从系统启动到现在锁定总时间长度；非常重要的参数，
      - innodb_row_lock_time_avg: 每次等待所花平均时间；非常重要的参数，
      - innodb_row_lock_time_max: 从系统启动到现在等待最常的一次所花的时间；
      - innodb_row_lock_waits: 系统启动后到现在总共等待的次数；非常重要的参数。直接决定优化的方向和策略

  * ==行锁优化==

    1. 尽可能让所有数据检索都通过索引来完成，避免无索引行或索引失效导致行锁升级为表锁。
    2. 尽可能避免间隙锁带来的性能下降，减少或使用合理的检索范围。
    3. 尽可能减少事务的粒度，比如控制事务大小，而从减少锁定资源量和时间长度，从而减少锁的竞争等，提供性能。
    4. 尽可能低级别事务隔离，隔离级别越高，并发的处理能力越低

* 表锁

  * ==优势：开销小；加锁快；无死锁==

  * ==劣势：锁粒度大，发生锁冲突的概率高，并发处理能力低==

  * 加锁的方式：

    - ==自动加锁==
      - 查询操作（SELECT），会自动给涉及的所有表加读锁
      - 更新操作（UPDATE、DELETE、INSERT），会自动给涉及的表加写锁。
    - ==显示加锁：==
      - 共享读锁：lock table tableName read;
      - 独占写锁：lock table tableName write;
      - 批量解锁：unlock tables;

  * 共享读锁

    - 对MyISAM表的读操作（加读锁），不会阻塞其他进程对同一表的读操作，但会阻塞对同一表的写操作。只有当读锁释放后，才能执行其他进程的写操作。在锁释放前不能取其他表

    * 对MyISAM表的写操作（加写锁），会阻塞其他进程对同一表的读和写操作，只有当写锁释放后，才会执行其他进程的读写操作。在锁释放前不能写其他表

  * 总结：==表锁，读锁会阻塞写，不会阻塞读。而写锁则会把读写都阻塞==

  * 查看加锁情况

    * show open tables; 1表示加锁，0表示未加锁。
      - `show open tables where in_use > 0;`

  * ==分析表锁定==

    - 可以通过检查table_locks_waited 和 table_locks_immediate 状态变量分析系统上的表锁定
      - `show status like 'table_locks%';`
      - table_locks_immediate: 表示立即释放表锁数。
      - table_locks_waited: 表示需要等待的表锁数。此值越高则说明存在着越严重的表级锁争用情况。
    - 此外，MyISAM的读写锁调度是写优先，这也是MyISAM不适合做写为主表的存储引擎。因为写锁后，其他线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永久阻塞。

  * 使用表锁场景

    - InnoDB默认采用行锁，在未使用索引字段查询时升级为表锁
      - 即便在条件中使用了索引字段，MySQL会根据自身的执行计划，考虑是否使用索引(所以explain命令中会有possible_key 和 key)。如果MySQL认为全表扫描效率更高，它就不会使用索引，这种情况下InnoDB将使用表锁，而不是行锁。
    - 第一种情况：==全表更新==
      - 事务需要更新大部分或全部数据，且表又比较大。若使用行锁，会导致事务执行效率低，从而可能造成其他事务长时间锁等待和更多的锁冲突。
    - 第二种情况：==多表查询==。事务涉及多个表，比较复杂的关联查询，很可能引起死锁，造成大量事务回滚。这种情况若能一次性锁定事务涉及的表，从而可以避免死锁、减少数据库因事务回滚带来的开销

* 总结：

  * 全局锁主要用在逻辑备份过程中
  * ==InnoDB 支持表锁和行锁，使用索引作为检索条件修改数据时采用行锁，否则采用表锁。==
  * InnoDB 自动给修改操作加锁，给查询操作不自动加锁
  * 行锁可能因为未使用索引而升级为表锁，所以除了检查索引是否创建的同时，也需要通过explain执行计划查询索引是否被实际使用。
  * 行锁相对于表锁来说，优势在于高并发场景下表现更突出，毕竟锁的粒度小。
  * ==当表的大部分数据需要被修改，或者是多表复杂关联查询时，建议使用表锁优于行锁。==
  * 为了保证数据的一致完整性，任何一个数据库都存在锁定机制。锁定机制的优劣直接影响到一个数据库的并发处理能力和性能。

## 多版本并发控制（MVCC）

* ==主要用于MySQL的不可重复读和可重复读==
* 可以认为MVCC是行级锁的一个变种，MVCC在大多数情况下代替了行锁，避免了加锁操作，因此开销更低，实现了非阻塞的读操作，写操作也只是锁定必要的行
* ==并发访问(读或写)数据库时，对正在事务内处理的数据做多版本的管理。以达到用来避免写操作的堵塞，从而引发读操作的并发问题==
* 实现：
  * ==是通过保存数据在某个时间点的快照来实现的，也就是说，不管需要执行多长时间，每个事务看到的数据都是一致的。根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的==
* InnoDB的MVCC
  * ==是通过在每行记录后面保存的数据行的版本号和删除版本号这两个隐藏的列来实现的==
  * 每开始一个新的事务，系统版本号都会自动递增
  * 事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较
* REPEATABLE READ隔离级别下，MVCC具体操作：
  * ==SELECT（同时符合）==
    * InnoDB只查找版本早于等于当前事务版本的数据行
    * 行的删除版本要么未定义，要么大于当前事务版本号，可确保事务读取到的行在事务开始之前未被删除
  * ==INSERT==
    * InnoDB为新插入的每一行保存当前系统版本号作为行版本号
  * ==DELETE==
    * InnoDB为删除的每一行保存当前系统版本号作为删除标识
  * ==UPDATE==
    * InnoDB为插入一行新纪录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识
* 保存这两个额外系统版本号，使大多数读操作都可以不用加锁，读数据操作很简单，性能很好。不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作
* MVCC只在REPEATABLE READ和READ COMMITTED两个隔离级别下工作，因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的数据行。而SERIALIZABLE则会对所读取的行都加锁

## 引擎对比

* 查看当前存储引擎：`show variables like '%storage_engine%';`
* `InnoDB`
  * `Mysql5.5`及以后版本的默认存储引擎
  * ==支持事务、行级锁（锁索引）、外键，并发性能更好==
  * ==基于聚簇索引(主键索引)方式建立表进行数据存储==
    * 聚簇索引对主键查询有很高的性能，不过它的二级索引（非主键索引）中必须包含主键列
  * ==`MySQL InnoDB`引擎默认的修改数据语句，`update,delete,insert`都会自动给涉及到的数据加上排他锁，`select`语句默认不会加任何锁类型==
  * 索引
    * ==通过主键查找：==
      * `MySQL`使用聚集索引，将主键组织到一棵B+树中，而行数据就储存在叶子节点上；按条件查找主键，通过按照B+树的检索算法即可查找到对应的叶节点，之后获得行数据
    * ==非主键查找：==
      * 第一步在辅助索引`B+`树中检索，到达其叶子节点获取对应的主键；
      * 第二步使用主键在主索引`B+`树中再执行一次`B+`树检索操作，最终到达叶子节点即可获取整行数据
  * 采用`MVCC`来支持高并发，并且实现了四个标准的隔离级别，其默认级别是可重复读，并且通过间隙锁策略防止幻读的出现。间隙锁使得`InnoDB`不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，以防止幻影行的插入
* `MyISAM`
  * `Mysql5.1`及之前版本的默认存储引擎
  * ==`MyISAM`会将表存储在两个文件中：数据文件和索引文件==
  * ==表级锁、不支持事务、不支持崩溃后的安全恢复（支持全文索引、压缩）==
    * 全文索引：查找的是文本中的关键词，而不是直接比较索引中的值
  * 适用于只读的数据，或者表比较小，不需要事务并且主要是`SELECT`和`INSERT`操作
  * 加锁与并发
    * 表锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排他锁，但是在表有读取查询的同时，也可以往表中插入新的记录
  * MyISAM的索引方式也叫做“非聚集”的，MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址，然后以data域的值为地址来读取相应数据记录
    * MyISM使用的是非聚簇索引，非聚簇索引的两棵B+树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。==由于索引树是独立的，通过辅助键检索无需访问主键的索引树==

## 索引

* ==是存储引擎用于快速找到记录的一种数据结构==

* ==在MySQL中，存储引擎先在索引中找到对应的值，然后根据匹配的索引记录找到对应的数据行==

* ==索引可以包含一个或多个列的值，如果包含多个列，那么列的顺序也十分重要，因为MySQL只能高效地使用索引的最左前缀列==

  * ==索引类型分为主键索引和非主键索引==
  * InnoDB，主键索引也被称为聚簇索引，非主键索引的叶子结点内容是主键的值，在InnoDB里，非主键索引也被称为二级索引
  * 基于主键索引和普通索引的查询有什么区别？
    * 通过主键ID查询，只需搜索主键字段ID这棵B+树
    * 非主键k查询需先搜索k索引树得到主键ID值，再到ID索引树搜索一次-回表（需要多扫描一颗索引树）

* B-Tree索引

  * 使用B-Tree数据结构来存储数据

  * InnoDB使用的是B+Tree，即每一个叶子结点都包含指向下一个叶子结点的指针，从而方便叶子结点的范围遍历

  * MyISAM使用前缀压缩技术使得索引更小，但InnoDB则按照原数据格式进行存储；MyISAM索引通过数据的物理位置引用被索引的行，而InnoDB则根据主键索引被引用的行

  * ==B-Tree通常意味着所有的值都是按照顺序存储的，并且每一个叶子页到根的距离相同。B-Tree对索引列是顺序组织存储的，所以很适合查找范围数据==

    * 索引对多个值进行排序的依据是CREATE TABLE语句中定义索引时列的顺序

  * B-Tree索引能够加快访问数据的速度

    * ==因为存储引擎不再需要进行全表扫描来获取需要的数据，取而代之的是从索引的根节点开始进行搜索==
    * 根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找。
    * 通过比较节点页的值和要查找的值可以找到合适的指针进入下层子节点，这些指针实际上定义了子节点页中值的上限和下限
    * 最终存储引擎要么是找到相应的值，要么该记录不存在
    * 叶子结点比较特别，他们的指针指向的是被索引的数据，而不是其他的节点页

  * ==B-Tree索引的查询类型==（B-Tree索引适用于全键值、键值范围或键前缀查找（只适用于根据最左前缀的查找））

    * ==全值匹配：和索引中的所有列进行匹配==
    * ==匹配最左前缀：只使用索引的第一列==
    * ==匹配列前缀：只匹配某一列的值的开头部分==
    * ==匹配范围值：只使用索引的第一列的范围==
    * ==精确匹配某一列并范围匹配另外一列==
    * ==只访问索引的查询：覆盖索引==

  * 索引树中的节点是有序的，B-Tree可以按照某种方式查找到值，那么也可以按照这种方式用于排序

  * B-Tree索引的限制

    * 不是按照索引的最左列开始查找，则无法使用索引
    * 不能跳过索引中的列
    * 如果查找中有某个列的范围查询，则其右边所有列都无法使用索引优化查询

  * 索引的优点

    * 索引可以让服务器快速定位到表的指定位置
    * B-Tree索引按照顺序存储数据，所以MySQL可以用来ORDER BY和GROUP BY操作，因为数据是有序的，所以B-Tree也就会将相关的列值都存储在一起

    1. ==索引大大减少了服务器需要扫描的数据量==
    2. ==索引可以帮助服务器避免排序和临时表==
    3. ==索引可以将随机I/O变为顺序I/O==

  * 索引的缺点：

    * ==创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加==
    * ==索引占用存储空间==
    * 优质索引创建难

  * 高性能索引策略

    * ==独立的列==：
      * 索引列不能是表达式的一部分，也不能是函数的参数，如可以始终将索引列单独放在比较符号的一侧
    * ==前缀索引和索引选择性==
      * 选择足够长的前缀以保证较高的选择性（可以过滤掉更多的行），同时又不能太长（以便节约空间），前缀应该足够长，以使得前缀索引的选择性接近于索引整个列
      * 计算完整列的选择性：`select count(disinct city)/count(*) from city_demo`
      * 创建前缀索引：`alter table city_demo add key(city(7))`
      * MySQL无法使用前缀索引做ORDER BY和GROUP BY，也无法使用前缀索引做覆盖扫描
    * 多列索引
      * 在多个列上建立独立的单列索引大部分情况下并不能提高MySQL的查询性能
      * 索引合并策略，一定程度上可以使用表上的多个单列索引来定位指定的行
    * ==选择合适的索引顺序==
      * ==索引首先按照最左列进行排序，其次是第二列，等等==
      * ==将选择性最高的列放到索引最前列==
    * 聚簇索引
      * 并不是一种单独的索引类型，而是一种数据存储方式
      * InnoDB的聚簇索引实际上在同一个结构中保存了B-Tree索引和数据行
      * 当表有聚簇索引时，它的数据行实际存放在索引的叶子页中
      * 聚簇，表示数据行和相邻的键值紧凑地存储在一起
      * ==叶子页包含了行的全部数据，但是节点页只包含了索引列==
      * InnoDB将通过主键聚集数据
        * ==如果没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。==
        * InnoDB只聚集在同一个页面中的记录，包含相邻键值的页面可能可能会相距甚远
      * 优点：
        * 可以把相关数据保存在一起
        * 数据访问更快。聚簇索引将索引和数据保存在同一个B-Tree中，因此从聚簇索引中获取数据通常比在非聚簇索引中查找要快
        * 使用覆盖索引扫描的查询可以直接使用页节点中的主键值
      * 缺点：
        * 聚簇数据最大限度地提高了I/O密集型应用的性能，但如果数据全部都放在内存中，则访问的顺序就没那么重要了，聚簇索引也就没什么优势了
        * 插入速度严重依赖于插入顺序
        * 更新剧簇索引列的代价很高，因为会强制InnoDB将每个被更新的行移动到新的位置
        * 页分裂会导致表占用更多的磁盘空间
          * 页分裂：当行的主键值要求必须将这一行插入到某个已满的页中时，存储引擎会将该页分裂成两个页面来容纳该行
        * 聚簇索引可能导致全表扫描变慢，尤其是行比较稀疏，或者由于页分裂导致数据存储不连续的时候
        * 二级索引（非聚簇索引）可能比想象的要更大，因为二级索引的叶子节点包含了引用行的主键列
        * 二级索引访问需要两次索引查找，而不是一次
          * 二级索引叶子节点保存的不是指向行的物理位置的指针，而是行的主键值，根据主键值去聚簇索引中查找到对应的行。重复工作：两次B-Tree查找而不是一次
      * 聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针以及所有的剩余列。如果主键是一个列前缀索引，InnoDB也会包含完整的主键列和剩下的其他列
        * 聚簇索引：数据行的物理顺序与列值（一般是主键的那一列）的逻辑顺序相同，一个表中只能拥有一个聚集索引，索引中键值的逻辑顺序决定了表中相应行的物理顺序
      * InnoDB的B+Tree分为二级索引和聚簇索引
        * 聚簇索引的B+Tree中的叶子节点存放的是整张表的行记录数据
        * 二级索引的叶子节点存储相应行数据的聚簇索引（主键值）
          * 当通过二级索引来查询数据时，InnoDB存储引擎会遍历二级找到主键，然后再通过主键在聚簇中找到完整的行记录数据
          * 此策略减少了当出现行移动或者数据页分裂时二级索引的维护工作
          * 使用主键值当作指针会让二级索引占用更多的空间，换来的好处是，InnoDB在移动行时无需更新二级索引中的这个"指针"

    * 覆盖索引
      * ==一个索引包含所有需要查询的字段的值==
      * ==查询只需扫描索引而无需回表==
        * 减少数据访问量
        * 避免二级索引的二次查询
      * 如select name,phoneNum from users where name=?   
        - 【name,phoneNum】为联合索引，则直接返回数据（关键字就是数据），不需要做叶子节点的IO

    

    

# ==整理SQL优化思路==





* ==冗余和重复索引==

  * 重复索引：相同的列上按照相同的顺序创建的相同类型的索引，应该避免
    * MySQL的唯一限制和主键限制都是通过索引实现的
  * 如果创建了索引（A，B），再创建索引（A）就是冗余索引，因为这只是前一个索引的前缀索引，因此索引（A，B）也可以当做索引（A）来使用。如果再创建索引（B，A），则索引（A）不是冗余索引，索引（B）也不是，因为B不是索引（A，B）的最左前缀列
  * 复合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合
    - 实际开发中推荐使用==复合索引==，并且单表创建的索引个数建议不要超过五个
    - 联合索引列选择原则
      1. ==经常用的列优先 【最左匹配原则】==
      2. ==选择性(离散度)高的列优先【离散度高原则】==
      3. ==宽度小的列优先【最少空间原则】（宽度越小，路数越多）==

* 索引和锁

  * 索引可以让查询锁定更少的行
  * InnoDB只有在访问行的时候才会对其加锁，而索引能够减少InnoDB访问的行数，从而减少锁的数量
  * InnoDB可以在服务器端过滤掉行后就释放锁
  * InnoDB在二级索引上使用共享锁，但访问主键索引需要排他锁，这消除了使用覆盖索引的可能性，并且使得SELECT FOR UPDATE比LOCK IN SHARE MODE或非锁定查询要慢很多

* 总结

  * 单行访问时很慢的

    * 如果服务器从存储中读取一个数据块只是为了获取其中一行，那么就浪费了很多工作

  * 按顺序访问范围数据是很快的
    * 顺序I/O不需要多次磁盘寻道，所以比随机I/O要快很多
    * 如果服务器能够按照需要顺序读取数据，那么就不再需要额外的排序操作

  * 索引覆盖查询是很快的

    * 如果一个索引包含了查询所需的所有列，那么存储引擎就不需要再回表查找行

  * ==按照响应时间来对查询进行分析==
    * 判断是否有查询扫描了太多的行
    * 是否做了很多额外的排序或者使用了临时表
    * 是否使用随机I/O访问数据
    * 是否有太多回表查询那些不在索引中的列操作

  * ==使用B+树来构建索引，为什么不用二叉树？==

    * ==二叉查找树==

      * 若左子树不空，则左子树上所有结点的值均小于它的根结点的值
      * 若右子树不空，则右子树上所有结点的值均大于它的根结点的值
      * 左、右子树也分别为二叉排序树
      * 没有键值相等的节点
      * ==第一个插入的就是根节点，后续插入的数据都大于根节点，可能形成线性链表，数据分布不均衡，查找效率慢==

    * ==平衡二叉树：在符合二叉查找树的条件下，还满足任何节点的两个子树的高度最大差为1，对数据进行操作时，会调整数据结构以满足规则、保证相对平衡，红黑树就是一颗平衡二叉树==

    * 平衡多路查找树（B树）

      * 所有键值分布在整颗树中；任何一个关键字出现且只出现在一个结点中；

    * B+Tree

      * 是在B-Tree基础上的一种优化，InnoDB存储引擎就是用B+Tree实现其索引结构

      * ==B-Tree每个节点中不仅包含数据的key值，还有data值==

* ==而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率==

* ==在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度==

      * ==非叶子节点只存储键信息==

  * ==所有叶子节点之间都有一个链指针==

        * ==数据记录都存放在叶子节点中==

    * 红黑树在磁盘上的查询性能远不如B+树

      * ==原因是红黑树最多只有两个子节点，所以高度会非常高，导致遍历查询的次数会多==

  * 又因为红黑树在数组中存储的方式，导致逻辑上很近的父节点与子节点可能在物理上很远，导致无法使用磁盘预读的局部性原理，需要很多次IO才能找到磁盘上的数据；

    * 但B+树一个节点中可以存储很多个索引的key，且将大小设置为一个页，一次磁盘IO就能读取很多个key，且叶子节点之间还加上了下个叶子节点的指针，遍历索引也会很快

  * ==B+树的高度如何计算？==

    * 在Linux里，每个页默认4KB，假设索引的是8B的long型数据，每个key后有个页号4B，还有6B的其他数据（参考《MySQL技术内幕：InnoDB存储引擎》P193的页面数据），那么每个页的扇出系数为4KB/(8B+4B+6B)=227，即每个页可以索引245个key。在高度h=3时，227^3=1100万。通常来说，索引树的高度在2~4

* 索引创建

  ```mysql
  	-- 创建
    create [unique] index indexName on tableName (columnName...)
  	alter tableName add [unique] index [indexName] on (columnName...)
    -- 删除
    drop index [indexName] on tableName
    -- 查看
    show index from tableName
    -- 创建唯一索引
    create unique index idx_order_transaID on itdragon_order_list (transaction_id);
    explain select transaction_id from itdra_order_list where transaction_id = "..";
    -- 创建复合索引
    create index idx_order_levelDate on itdragon_order_list (order_level,input_date);
    explain select * from itdragon_order_list force index(idx_order_levelDate) order by order_level,input_date;
    -- 根据业务调整语句
    explain select * from itdragon_order_list where order_level=3 order by input_date;
  ```

## 执行计划

* ==通过explain的参数介绍，可以得知：==
  1. 表的读取顺序(id)
  2. 数据读取操作的操作类型(type)
  3. 哪些索引被实际使用(key)
  4. 表之间的引用(ref)
  5. 每张表有多少行被优化器查询(rows)
* explain输出字段的意义
  - **id**：select 查询的序列号，包含一组可以重复的数字，表示查询中执行sql语句的顺序。一般有三种情况
    - 第一种：id全部相同，sql的执行顺序是由上至下；
    - 第二种：id全部不同，sql的执行顺序是根据id大的优先执行；
    - 第三种：id既存在相同，又存在不同的。先根据id大的优先执行，再根据相同id从上至下的执行。
  - **select_type**：select 查询的类型，主要是用于区别普通查询，联合查询，嵌套的复杂查询
    - simple：简单的select 查询，查询中不包含子查询或者union
    - primary：查询中若包含任何复杂的子查询，最外层查询则被标记为primary
    - subquery：在select或where 列表中包含了子查询
    - derived：在from列表中包含的子查询被标记为derived（衍生）MySQL会递归执行这些子查询，把结果放在临时表里。
    - union：若第二个select出现在union之后，则被标记为union，若union包含在from子句的子查询中，外层select将被标记为：derived
    - union result：从union表获取结果的select
  - ==**type** : 连接类型，常见的有：all , index , range , ref , eq_ref , const , system , null 八个级别==
    - 性能从最优到最差的排序：system > const > eq_ref > ref > range > index > all
    - 对java程序员来说，若保证查询至少达到range级别或者最好能达到ref则算是一个优秀而又负责的程序员
    - all：==全表扫描==无疑是最差
    - index：==全索引文件扫描==比all好很多，毕竟从索引树中找数据，比从全表中找数据要快。
    - range：==只检索给定范围的行，使用索引来匹配行==。sql语句中一般会有between，in，>，< 等查询。
    - ref：==非唯一性索引扫描，返回所有匹配某个单独值的行==。比如查询公司所有属于研发团队的同事，匹配的结果是多个并非唯一值。
    - eq_ref：==唯一性索引扫描，对于每个索引键，表中有一条记录与之匹配==。比如查询公司的CEO，匹配的结果只可能是一条记录，
    - const：==表示通过索引一次就可找到，const用于比较primary key 或者unique索引==。因为只匹配一行数据，所以很快，若将主键至于where列表中，MySQL就能将该查询转换为一个常量
    - system：==表只有一条记录==（等于系统表），这是const类型的特列，平时不会出现，了解即可
  - **partitions**：表所使用的分区
    - 如果要统计十年公司订单的金额，可以把数据分为十个区，每一年代表一个区。这样可以大大的提高查询效率
  - **possible_keys**
    - 显示查询语句可能用到的索引(一个或多个或为null)，不一定被查询实际使用。仅供参考使用。
  - ==**key** : 实际用到索引==，如果为null，表明MySQL实际上并没有使用索引
  - **key_len**：显示索引中使用的字节数，可通过key_len计算查询中使用的索引长度。
    - 在不损失精确性的情况下索引长度越短越好。
    - key_len 显示的值为索引字段的最可能长度，并非实际使用长度，即key_len是根据表定义计算而得，并不是通过表内检索出的。
  - **ref** : 显示索引的哪一列或常量被用于查找索引列上的值
  - **rows**：根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数，值越大越不好
  - ==**Extra** :==
    - Using filesort： 说明MySQL会对数据==使用一个外部的索引排序==，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成的排序操作称为“文件排序” 。出现这个就要立刻优化sql。
      - Using filesort，使用单路排序算法，其本质就是用空间换时间，但如果数据量太大，buffer的空间不足，会导致多次I/O的情况
    - Using temporary： ==使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表==。常见于排序 order by 和 分组查询 group by。 出现这个更要立刻优化sql。
    - Using index： 表示相应的select 操作中使用了==覆盖索引==（Covering index），避免访问了表的数据行，效果不错！如果同时出现Using where，表明索引被用来执行索引键值的查找。如果没有同时出现Using where，表示索引用来读取数据而非执行查找动作。
    - Covering Index ：也叫==索引覆盖==，就是select 的数据列只用从索引中就能够取得，不必读取数据行，MySQL可以利用索引返回select 列表中的字段，而不必根据索引再次读取数据文件。
    - Using index condition： 在5.6版本后加入的新特性，优化器会在索引存在的情况下，通过符合RANGE范围的条数和总数的比例来选择是使用索引还是进行全表遍历。
    - Using where： 表明使用了where 过滤
    - Using join buffer： 表明使用了连接缓存
    - impossible where： where 语句的值总是false，不可用，不能用来获取任何元素
    - distinct： 优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作。
  - **filtered**：
    - 一个百分比的值，和rows 列的值一起使用，可以估计出查询执行计划(QEP)中的前一个表的结果集，从而确定join操作的循环次数。小表驱动大表，减轻连接的次数

## 分库分表

* Mycat 是开源的分布式数据库中间件，处于数据库服务与应用服务之间

  * ==通俗点讲，应用层可以将它看作是一个数据库的代理(或者直接看成加强版数据库)==
  * 有了数据库中间件，应用只需要集中与业务处理，大量的通用的数据聚合，事务，数据源切换都由中间件来处理，中间件的性能与处理能力将直接决定应用的读写性能
  * 原理：
    * 当 Mycat 收到一个 SQL 时，会先解析这个 SQL，查找涉及到的表，然后看此表的定义，如果有分片规则，则获取到 SQL 里分片字段的值，并匹配分片函数，得到该 SQL 对应的分片列表，然后将 SQL 发往这些分片去执行，最后收集和处理所有分片返回的结果数据，并输出到客户端

* 分区

  * 把一张表的数据分成N个区块，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的，通过将不同数据按一定规则放到不同的区块中提升表的查询效率

* 分库

  * 面对高并发的读写访问，当数据库无法承载写操作压力时，不管如何扩展slave服务器，此时都没有意义了。因此数据库进行拆分，从而提高数据库写入能力，这就是分库
  * 问题：
    1. 事务问题
       - 在执行分库之后，由于数据存储到了不同的库上，数据库事务管理出现了困难。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价；如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。
    2. 跨库跨表的join问题
       - 在执行了分库分表之后，难以避免会将原本逻辑关联性很强的数据划分到不同的表、不同的库上，无法join位于不同分库的表，也无法join分表粒度不同的表，结果原本一次查询能够完成的业务，可能需要多次查询才能完成。
    3. 额外的数据管理负担和数据运算压力。
       - 额外的数据管理负担，最显而易见的就是数据的定位问题和数据的增删改查的重复执行问题，这些都可以通过应用程序解决，但必然引起额外的逻辑运算。

* 分表

  * 水平分表：为了解决单表数据量过大（数据量达到千万级别）问题。所以将固定的ID hash之后mod，取若0~N个值，然后将数据划分到不同表中，需要在写入与查询的时候进行ID的路由与统计
  * 垂直分表：为了解决表的宽度问题，同时还能分别优化每张单表的处理能力。所以将表结构根据数据的活跃度拆分成多个表，把不常用的字段单独放到一个表、把大字段单独放到一个表、把经常使用的字段放到一个表

* 分库分表中解释一下垂直和水平2种不同的拆分？

  垂直拆分：

  1. ==垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库==
  2. ==垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联==

  优点：

  1. 可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少)
  2. 可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起
  3. 数据维护简单

  缺点：

  1. 主键出现冗余，需要管理冗余列
  2. 会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力
  3. 依然存在单表数据量过大的问题（需要水平拆分）
  4. 事务处理复杂

  水平拆分：

  ​	水平拆分是通过某种策略将数据分片来存储，分为库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量

  ​	库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决

  优点：

  1. 不存在单库大数据和高并发的性能瓶颈
  2. 应用端改造较少
  3. 提高了系统的稳定性和负载能力

  缺点：

  1. 分片事务一致性难以解决
  2. 跨节点Join性能差，逻辑复杂
  3. 数据多次扩展难度跟维护量极大

* 分库分表中垂直分库方案会带来哪些问题？

  分库的事务、系统复杂度

* 分布式数据存储中间件如mycat的核心流程是什么？

  解析SQL-数据源管理-数据源分配-请求/响应-结果整合

* 概述一下mycat？

  开源的、解决分布式存储、读写分离问题，处于业务层与数据层之间，可以看成一个加强版的数据库（数据库的代理）

* 解释一下全局表，ER表，分片表？

  全局表：具有静态的、不常变的、数据量较少的数据，会与其他表有查询关联的表

  ER表：跟分片表有ER关系的表

  分片表：大表根据拆分规则拆分成的小表

* Mycat的在分库分表之后，它是怎么支持联表查询的？

  全局表

  ER表

  注解的方式

* 进行库表拆分时，拆分规则怎么取舍？

  看数据特点

* Mycat中全局ID方案有哪些？程序自定义全局ID的方案有哪些？

  本地文件方式、数据库方式、本地时间戳、程序方式

  雪花算法

* ==简述一下一致性hash的原理？这样设计的好处是什么？==

  1. **环形Hash空间**：按照常用的hash算法来将对应的key哈希到一个具有2^32次方个桶的数字空间中，将这些数字头尾相连，想象成一个闭合的环形
  2. **把数据通过一定的hash算法处理后映射到环上**：将数据通过特定的Hash函数计算出对应的key值，然后散列到Hash环上
  3. **将机器通过hash算法映射到环上**：通过使用与对象存储一样的Hash算法将机器也映射到环中（一般情况下对机器的hash计算是采用机器的IP或者机器唯一的别名作为输入值），然后以顺时针的方向计算，将所有对象存储到离自己最近的机器中
  4. **机器的删除与添加**：如果某个机器节点出现故障被删除了，那么按照顺时针迁移的方法，故障节点附近的数据会迁移到其他节点上，其他数据没有任何改动；如果增加机器节点，通过按顺时针迁移的规则，相对靠近新节点的数据会迁移到新节点上，其它数据还保持这原有的存储位置

  一致性哈希算法在保持了单调性的同时，还是数据的迁移达到了最小，这样的算法对分布式集群来说是非常合适的，==避免了大量数据迁移==，减小了服务器的的压力

  把整个数据和节点看成一个0-2^23的环，数据通过hash计算进来后会顺序找到自己的那个服务节点，这样的好处是：节点的增减影响范围小

* 4层负载和7层负载谁性能更高？为什么？这2者区别是什么？

  四层负载均衡也称为四层交换机，它主要是通过分析IP层及TCP/UDP层的流量实现的基于IP加端口的负载均衡

  七层负载均衡器也称为七层交换机，位于OSI（ Open System Interconnection ，开放式系统互联）的最高层，即应用层，此时负载均衡器支持多种应用协议，常见的有HTTP、FTP、SMTP等

  4层负载性能更高

  7层是应用层的负载，更精准的

  四层：比较底层，性能优于七层，后端连接自己建立，直接找到目标，不需要代理

  七层：需要代理

# 优化

## 数据库性能优化

* ==MySQL性能下降的原因==
  * 自身瓶颈。磁盘空间不足，磁盘I/O在装入数据远大于内存容量的时候，服务器硬件性能低；服务器调优配置参数设置不合理
  * sql语句不好
  * 没建索引，索引建的不合理或索引失效
  * 关联查询有太多的join

1. ==优化SQL语句和索引，在where/group by/order by中用到的字段建立索引，索引字段越小越好，复合索引建立的顺序==
2. ==加缓存，Memcached, Redis==
3. ==主从复制，读写分离==
4. 垂直拆分，其实就是根据模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统
5. 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的==sharding key==,为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表

## 如何定位慢SQL

1. 慢查询日志配置

   * ==开启慢查询日志、设置慢查询日志存储位置、记录条件（如没使用索引查询的SQL、超过规定时间的查询SQL等）==

     ```sql
     --慢查询日志配置
     show variables like 'slow_query_log'
     set global slow_query_log = on
     show variables like 'slow_query%'
     set global slow_query_log_file = '/var/lib/mysql/gupaoedu-slow.log' 
     -- 记录没有通过索引查询的sql
     set global log_queries_not_using_indexes = on
     -- 执行时间超过规定的时间的sql记录下来
     set global long_query_time = 0.1 (秒)
     ```

2. ==分析慢查询日志==

   * 直接打开慢日志文件，定位到最后一行
   * `vi + /var/lib/mysql/gupaoedu-slow.log`
   * Time :日志记录的时间 、User@Host:执行的用户及主机 、==Query_time:查询耗费时间== 、==Lock_time 锁表时间==、Rows_sent 发送给请求方的记录条数 、Rows_examined ==语句扫描的记录条数== 、SET timestamp 语句执行的时间点 、select .... 执行的具体语句

3. 慢查询日志分析工具

   * `mysqldumpslow -t 10 -s at /var/lib/mysql/gupaoedu-slow.log`

## 优化细节

* 其他
  * ==必须把字段定义为NOT NULL并且提供默认值==
    * null值需要更多的存储空间
    * 对null 的处理时候，只能采用is null或is not null==，而不能采用=、in、<、<>、!=、not in这些操作符号。如:where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录
    * null 这种类型MySQL内部需要进行特殊处理增加复杂性
  * ==使用精确列名查询而不是*，特别是当数据量大的时候==
  * ==必须显示指定插入的列属性==
  * ==禁止使用OR条件，必须改为IN查询==
  * ==减少子查询，使用Join替代==
    * ==子查询慢的原因：==
      * 子查询不会先被执行，其执行效率受制于外层查询的记录数
      * 外层查询的每条数据都将会传到子查询中与其关联组成新的查询语句，如果外查询的记录数很多的话，那么性能上将会出现问题
  * ==禁止大表使用JOIN查询，禁止大表使用子查询，会产生临时表，消耗较多内存与CPU，影响数据库性能==
  * SQL 性能优化的目标：至少要达到 range 级别，要求是 ref 级别，如果可以是 consts 最好
  * ==利用覆盖索引来进行查询操作，避免回表==
  * 表必备三字段:id, gmt_create, gmt_modified
  * 小数类型为 decimal，禁止使用 float 和 double（存在精度损失的问题）
  * MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写
  * 单表行数3年后超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表
* ==索引设计规范==
  * ==表必须有主键==
  * ==单表索引建议控制在5个以内，超过的话已经起不到有效过滤数据的作用了==
  * ==对经常查询的列建立索引，不在更新频繁、区分度不高、where条件里用不到的字段上建立索引==
    * 尽量选择区分度高的列作为索引，扫描的记录数越少
  * ==建立组合索引，必须把区分度高的字段放在前面，能够更加有效的过滤数据==
    * ==最左前缀匹配原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配==
    * ==最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符==
    * ==查询条件里只有b的语句是无法使用(a,b)这个联合索引，需再维护另一个索引(b)==
    * InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录直接判断并跳过
  * ==业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引==
  * ==在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引==
  * ==order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现 file_sort 的情况，影响查询性能==
  * ==经常需要排序、分组和统计的字段需要建立索引，WHERE / GROUP BY / ORDER BY / ON 的列，加快条件的判断速度==
  * ==查询中与其他表关联的字段，外键关系建立索引==
  * ==索引字段越小越好，因为数据库按页存储的，如果每次查询IO读取的页越少查询效率越高==
* 索引失效条件
  * ==索引的作用就是避免全表扫描，索引失效，还是会进行全表扫描==
  * ==没有where子句的时候，会失效，比如直接使用group by==
  * ==在索引字段使用is null ，is not null时，该列索引无效==
  * ==负向查询（不等于操作、NOT 、NOT IN），以及%开头的模糊查询like，会导致全表扫描==
  * ==使用属性隐式转换比较不匹配的数据类型，比如数值类型和varchar类型比对，会导致全表扫描==
  * ==在WHERE条件的属性上使用函数或者表达式，会导致全表扫描==
    * 如果索引没有使用函数包装索引，那么在索引列中使用函数，也会使该索引失效。但是如果把函数作用在参数上，那么索引是有效的

## 优化案例

* 有个表特别大，字段是姓名、年龄、班级，如果调用`select * from table where name = xxx and age = xxx`该如何通过建立索引的方式优化查询速度？

  * 由于mysql查询每次只能使用一个索引，如果在name、age两列上创建复合索引的话将带来更高的效率
  * ==如果创建了(name, age)的复合索引，那么其实相当于创建了(name, age)、(name)三个索引，这被称为最佳左前缀特性。因此在创建复合索引时应该将最常用作限制条件的列放在最左边，依次递减==
  * ==其次还要考虑该列的数据离散程度，如果有很多不同的值的话建议放在左边，name的离散程度也大于age==

* max(xxx)如何用索引优化

  * 在xxx列上建立索引，因为索引是B+树顺序排列的，锁在下次查询的时候就会使用索引来查询到最大的值是哪个

* 如何对分页进行优化？

  * `SELECT * FROM big_table order by xx LIMIT 1000000,20`，这条语句会查询出1000020条的所有数据然后丢弃掉前1000000条，为了避免全表扫描的操作，在order by的列上加**索引**就能通过扫描索引来查询
  * 但是这条语句会查询还是会扫描1000020条，还能改进成==`select id from big_table where id >= 1000000 order by xx LIMIT 0,20`==，用ID作为**过滤**条件将不需要查询的数据直接去除

* SQL注入攻击

  * 是在输入的字符串之中注入SQL指令，在设计不良的程序当中忽略了字符检查

  * 通过预编译解决

    ```mysql
    strSQL = "SELECT * FROM users WHERE (name = '" + userName + "') and (pw = '"+ passWord +"');"
    -- 恶意填入 
    userName = "1' OR '1'='1";
    passWord = "1' OR '1'='1";
    ```

* SQL查询优化

  ```mysql
  SELECT * FROM XXXXX WHERE
  (convert((price_full * 100 - price * 100) , SIGNED) - convert(coupon_price*100,SIGNED)
  AND is_del = 0) ORDER BY id desc limit 100
  -- 解决 
  1. 加一列 a=convert((price_full * 100 - price * 100) , SIGNED) - convert(coupon_price*100,SIGNED)
  2. a 和 is_del建立联合索引
  ```

  